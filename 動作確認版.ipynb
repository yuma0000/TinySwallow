{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuma0000/TinySwallow/blob/main/%E5%8B%95%E4%BD%9C%E7%A2%BA%E8%AA%8D%E7%89%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA_1qOCvkv_B"
      },
      "source": [
        "# ファンクション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "WnudQPPV43xC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import inspect\n",
        "import importlib.util\n",
        "import re\n",
        "import json\n",
        "import huggingface_hub\n",
        "import numpy as np\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import functools\n",
        "import packaging\n",
        "\n",
        "from torch import nn\n",
        "from typing import Any, Union, Callable, Tuple, Optional, TypeVar, ContextManager\n",
        "from packaging import version\n",
        "from dataclasses import dataclass, fields, field, is_dataclass\n",
        "from collections.abc import Iterable, Iterator, MutableMapping\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from functools import wraps, lru_cache\n",
        "from abc import ABC, abstractmethod\n",
        "from enum import Enum\n",
        "import unicodedata\n",
        "import regex as re\n",
        "from urllib.parse import urlparse\n",
        "from contextlib import ExitStack\n",
        "\n",
        "from transformers.utils import logging\n",
        "\n",
        "from contextlib import contextmanager\n",
        "\n",
        "\n",
        "@dataclass(frozen=False, eq=True)\n",
        "class AddedToken:\n",
        "    def __init__(\n",
        "        self, content: str, single_word=False, lstrip=False, rstrip=False, special=False, normalized=None\n",
        "    ):\n",
        "        self.content = content\n",
        "        self.single_word = single_word\n",
        "        self.lstrip = lstrip\n",
        "        self.rstrip = rstrip\n",
        "        self.special = special\n",
        "        self.normalized = normalized if normalized is not None else not special\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return self.__dict__\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.content\n",
        "\n",
        "@contextmanager\n",
        "def init_on_device(device: \"torch.device\", include_buffers: bool = False):\n",
        "    if include_buffers:\n",
        "        with device:\n",
        "            yield\n",
        "        return\n",
        "\n",
        "    old_register_parameter = nn.Module.register_parameter\n",
        "    if include_buffers:\n",
        "        old_register_buffer = nn.Module.register_buffer\n",
        "\n",
        "    def register_empty_parameter(module, name, param):\n",
        "        old_register_parameter(module, name, param)\n",
        "        if param is not None:\n",
        "            param_cls = type(module._parameters[name])\n",
        "            kwargs = module._parameters[name].__dict__\n",
        "            kwargs[\"requires_grad\"] = param.requires_grad\n",
        "            module._parameters[name] = param_cls(module._parameters[name].to(device), **kwargs)\n",
        "\n",
        "    def register_empty_buffer(module, name, buffer, persistent=True):\n",
        "        old_register_buffer(module, name, buffer, persistent=persistent)\n",
        "        if buffer is not None:\n",
        "            module._buffers[name] = module._buffers[name].to(device)\n",
        "\n",
        "    # Patch tensor creation\n",
        "    if include_buffers:\n",
        "        tensor_constructors_to_patch = {\n",
        "            torch_function_name: getattr(torch, torch_function_name)\n",
        "            for torch_function_name in [\"empty\", \"zeros\", \"ones\", \"full\"]\n",
        "        }\n",
        "    else:\n",
        "        tensor_constructors_to_patch = {}\n",
        "\n",
        "    def patch_tensor_constructor(fn):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            kwargs[\"device\"] = device\n",
        "            return fn(*args, **kwargs)\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    try:\n",
        "        nn.Module.register_parameter = register_empty_parameter\n",
        "        if include_buffers:\n",
        "            nn.Module.register_buffer = register_empty_buffer\n",
        "        for torch_function_name in tensor_constructors_to_patch.keys():\n",
        "            setattr(torch, torch_function_name, patch_tensor_constructor(getattr(torch, torch_function_name)))\n",
        "        yield\n",
        "    finally:\n",
        "        nn.Module.register_parameter = old_register_parameter\n",
        "        if include_buffers:\n",
        "            nn.Module.register_buffer = old_register_buffer\n",
        "        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items():\n",
        "            setattr(torch, torch_function_name, old_torch_function)\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def init_empty_weights(include_buffers: bool = False):\n",
        "    with init_on_device(torch.device(\"meta\"), include_buffers=include_buffers) as f:\n",
        "        yield f\n",
        "\n",
        "__version__ = \"4.54.0.dev0\"\n",
        "\n",
        "def is_deepspeed_zero3_enabled():\n",
        "    if _hf_deepspeed_config_weak_ref is not None and _hf_deepspeed_config_weak_ref() is not None:\n",
        "        return _hf_deepspeed_config_weak_ref().is_zero3()\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def is_fsdp_managed_module(module: nn.Module) -> bool:\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "\n",
        "    import torch\n",
        "\n",
        "    if not torch.distributed.is_available():\n",
        "        return False\n",
        "\n",
        "    import torch.distributed.fsdp\n",
        "\n",
        "    return isinstance(module, torch.distributed.fsdp.FullyShardedDataParallel) or getattr(\n",
        "        module, \"_is_fsdp_managed_module\", False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "qG2B9hBw6dAQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LogitsProcessor:\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        raise NotImplementedError(\n",
        "            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n",
        "        )\n",
        "\n",
        "class RepetitionPenaltyLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, penalty: float, prompt_ignore_length: Optional[int] = None):\n",
        "        if not isinstance(penalty, float) or not (penalty > 0):\n",
        "            raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n",
        "\n",
        "        if prompt_ignore_length is not None and (\n",
        "            not isinstance(prompt_ignore_length, int) or prompt_ignore_length < 0\n",
        "        ):\n",
        "            raise ValueError(f\"`prompt_ignore_length` has to be a positive integer, but is {prompt_ignore_length}\")\n",
        "\n",
        "        self.penalty = penalty\n",
        "        self.prompt_ignore_length = prompt_ignore_length\n",
        "\n",
        "    #@add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        if self.prompt_ignore_length:\n",
        "            input_ids = input_ids[:, self.prompt_ignore_length :]\n",
        "\n",
        "        score = torch.gather(scores, 1, input_ids)\n",
        "\n",
        "        # if score < 0 then repetition penalty has to be multiplied to reduce the token probabilities\n",
        "        score = torch.where(score < 0, score * self.penalty, score / self.penalty)\n",
        "\n",
        "        scores_processed = scores.scatter(1, input_ids, score)\n",
        "        return scores_processed\n",
        "\n",
        "class ContextManagers:\n",
        "    def __init__(self, context_managers: list[ContextManager]):\n",
        "        self.context_managers = context_managers\n",
        "        self.stack = ExitStack()\n",
        "\n",
        "    def __enter__(self):\n",
        "        for context_manager in self.context_managers:\n",
        "            self.stack.enter_context(context_manager)\n",
        "\n",
        "    def __exit__(self, *args, **kwargs):\n",
        "        self.stack.__exit__(*args, **kwargs)\n",
        "\n",
        "def find_tied_parameters(model: \"nn.Module\", **kwargs):\n",
        "    all_named_parameters = dict(model.named_parameters(remove_duplicate=False))\n",
        "    no_duplicate_named_parameters = dict(model.named_parameters(remove_duplicate=True))\n",
        "    tied_param_names = set(all_named_parameters.keys()) - set(no_duplicate_named_parameters.keys())\n",
        "    tied_param_groups = {}\n",
        "    for tied_param_name in tied_param_names:\n",
        "        tied_param = all_named_parameters[tied_param_name]\n",
        "        for param_name, param in no_duplicate_named_parameters.items():\n",
        "            if param is tied_param:\n",
        "                if param_name not in tied_param_groups:\n",
        "                    tied_param_groups[param_name] = []\n",
        "                tied_param_groups[param_name].append(tied_param_name)\n",
        "\n",
        "    return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "tKEATT6k7DnK"
      },
      "outputs": [],
      "source": [
        "class StoppingCriteria(ABC):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        raise NotImplementedError(\"StoppingCriteria needs to be subclassed\")\n",
        "\n",
        "class MaxLengthCriteria(StoppingCriteria):\n",
        "    def __init__(self, max_length: int, max_position_embeddings: Optional[int] = None):\n",
        "        self.max_length = max_length\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        cur_len = input_ids.shape[1]\n",
        "        is_done = cur_len >= self.max_length\n",
        "        if self.max_position_embeddings is not None and not is_done and cur_len >= self.max_position_embeddings:\n",
        "            logger.warning_once(\n",
        "                \"This is a friendly reminder - the current text generation call will exceed the model's predefined \"\n",
        "                f\"maximum length ({self.max_position_embeddings}). Depending on the model, you may observe \"\n",
        "                \"exceptions, performance degradation, or nothing at all.\"\n",
        "            )\n",
        "        return torch.full((input_ids.shape[0],), is_done, device=input_ids.device, dtype=torch.bool)\n",
        "\n",
        "class TopPLogitsWarper(LogitsProcessor):\n",
        "    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
        "        top_p = float(top_p)\n",
        "        if top_p < 0 or top_p > 1.0:\n",
        "            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n",
        "        if not isinstance(min_tokens_to_keep, int) or (min_tokens_to_keep < 1):\n",
        "            raise ValueError(f\"`min_tokens_to_keep` has to be a positive integer, but is {min_tokens_to_keep}\")\n",
        "\n",
        "        self.top_p = top_p\n",
        "        self.filter_value = filter_value\n",
        "        self.min_tokens_to_keep = min_tokens_to_keep\n",
        "\n",
        "    #@add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        sorted_logits, sorted_indices = torch.sort(scores, descending=False)\n",
        "        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)\n",
        "        # Keep at least min_tokens_to_keep\n",
        "        sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
        "        return scores_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "agA2kVceADvd"
      },
      "outputs": [],
      "source": [
        "\n",
        "ENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\n",
        "_torch_xla_available = False\n",
        "_torch_distributed_available = torch.distributed.is_available ()\n",
        "\n",
        "class EosTokenCriteria(StoppingCriteria):\n",
        "    def __init__(self, eos_token_id: Union[int, list[int], torch.Tensor]):\n",
        "        if not isinstance(eos_token_id, torch.Tensor):\n",
        "            if isinstance(eos_token_id, int):\n",
        "                eos_token_id = [eos_token_id]\n",
        "            eos_token_id = torch.tensor(eos_token_id)\n",
        "        self.eos_token_id = eos_token_id\n",
        "\n",
        "    #@add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        self.eos_token_id = self.eos_token_id.to(input_ids.device)\n",
        "        is_done = isin_mps_friendly(input_ids[:, -1], self.eos_token_id)\n",
        "        return is_done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "yQSiZa52xyNo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int) -> torch.Tensor:\n",
        "    if elements.device.type == \"mps\" and not is_torch_greater_or_equal_than_2_4:\n",
        "        test_elements = torch.tensor(test_elements)\n",
        "        if test_elements.ndim == 0:\n",
        "            test_elements = test_elements.unsqueeze(0)\n",
        "        return elements.tile(test_elements.shape[0], 1).eq(test_elements.unsqueeze(1)).sum(dim=0).bool().squeeze()\n",
        "    else:\n",
        "        return torch.isin(elements, test_elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "vZM7SGAS7fHB"
      },
      "outputs": [],
      "source": [
        "def _vec_softmax(input: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None):\n",
        "    \"\"\"\n",
        "    C++ _vec_softmax を Python で再現（ベクトル化や並列化なし）\n",
        "    \"\"\"\n",
        "\n",
        "    dim = dim if dim >= 0 else input.dim() + dim\n",
        "\n",
        "    shape = input.shape\n",
        "    assert 0 <= dim < input.dim()\n",
        "\n",
        "    outer_size = 1\n",
        "    for i in range(dim):\n",
        "        outer_size *= shape[i]\n",
        "\n",
        "    inner_size = 1\n",
        "    for i in range(dim + 1, input.dim()):\n",
        "        inner_size *= shape[i]\n",
        "\n",
        "    dim_size = shape[dim]\n",
        "\n",
        "    # 軸を最後に移動 → [outer, dim, inner] とみなして処理\n",
        "    input_transposed = input.permute(\n",
        "        *[i for i in range(input.dim()) if i != dim], dim\n",
        "    ).contiguous()\n",
        "    reshaped = input_transposed.view(outer_size, dim_size, inner_size)\n",
        "\n",
        "    # --- ステップ 1: max 減算 ---\n",
        "    max_vals, _ = reshaped.max(dim=1, keepdim=True)\n",
        "\n",
        "    # --- ステップ 2: exp & sum ---\n",
        "    exps = (reshaped - max_vals).exp()\n",
        "    sum_exps = exps.sum(dim=1, keepdim=True)\n",
        "\n",
        "    # --- ステップ 3: softmax ---\n",
        "    softmaxed = exps / sum_exps\n",
        "\n",
        "    # 元の形状に戻す\n",
        "    output = softmaxed.view(input_transposed.shape).permute(\n",
        "        *[i for i in range(input.dim()) if i != dim], input.dim() - 1\n",
        "    ).contiguous()\n",
        "\n",
        "    return output\n",
        "\n",
        "def softmax(input: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    高精度で F.softmax と同等の挙動を Python で再現。\n",
        "    - 数値安定化あり（max 減算）\n",
        "    - dtype による upcast/downcast 対応\n",
        "    - Softmax に必要な処理のみ、簡潔かつ高速\n",
        "    \"\"\"\n",
        "\n",
        "    # upcast（float16 / bfloat16 の場合は float32 に）\n",
        "    orig_dtype = input.dtype\n",
        "    if dtype is not None:\n",
        "        input = input.to(dtype)\n",
        "    elif input.dtype in (torch.float16, torch.bfloat16):\n",
        "        input = input.to(torch.float32)\n",
        "\n",
        "    # 数値安定性のため max 減算\n",
        "    max_val, _ = input.max(dim=dim, keepdim=True)\n",
        "    input = input - max_val\n",
        "\n",
        "    exp_input = input.exp()\n",
        "    sum_exp = exp_input.sum(dim=dim, keepdim=True)\n",
        "    softmax_output = exp_input / sum_exp\n",
        "\n",
        "    return softmax_output.to(orig_dtype)\n",
        "\n",
        "def safe_softmax(input: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    PyTorch C++ の _safe_softmax を Python に再現。\n",
        "    入力が -inf のみの行に対して softmax 出力を 0 に置き換える。\n",
        "    \"\"\"\n",
        "    dim = dim if dim >= 0 else input.dim() + dim\n",
        "\n",
        "    # softmax を計算\n",
        "    out = softmax(input, dim=dim, dtype=dtype)\n",
        "\n",
        "    # 入力が -inf である位置を取得（bool）\n",
        "    masked = input == float(\"-inf\")\n",
        "\n",
        "    # 各行が全部 -inf かどうか（→ Trueならsoftmaxが全部ゼロになるべき）\n",
        "    masked_rows = masked.all(dim=dim, keepdim=True)\n",
        "\n",
        "    # masked_rows が True の位置を 0 に（softmaxの結果を書き換え）\n",
        "    out = torch.where(masked_rows, torch.zeros_like(out), out)\n",
        "    return out\n",
        "\n",
        "def dropout_cpu(input: torch.Tensor, p: float, train: Optional[bool] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    C++ の native_dropout_cpu の Python 再現。\n",
        "    input: 入力テンソル\n",
        "    p: dropout率\n",
        "    train: 学習モードフラグ (NoneならTrueとして扱う)\n",
        "\n",
        "    戻り値: (output, mask)\n",
        "    \"\"\"\n",
        "    if input.numel() == 0:\n",
        "        return input, torch.empty_like(input)\n",
        "\n",
        "    if train is None:\n",
        "        train = True\n",
        "\n",
        "    if train:\n",
        "        p1m = 1.0 - p\n",
        "        scale = 0.0 if p1m == 0 else 1.0 / p1m\n",
        "        # Bernoulli mask\n",
        "        mask = torch.empty_like(input, dtype=torch.bool).bernoulli_(p1m)\n",
        "        output = input * mask.to(input.dtype) * scale\n",
        "    else:\n",
        "        mask = torch.ones_like(input, dtype=torch.bool)\n",
        "        output = input.clone()\n",
        "\n",
        "    return output, mask\n",
        "\n",
        "def dropout(input: torch.Tensor, p: float = 0.5, train: bool = True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    PyTorch C++ の dropout 実装を Python で再現。\n",
        "    \"\"\"\n",
        "    if input.is_nested():\n",
        "        raise NotImplementedError(\"nested tensor の dropout は未対応です\")\n",
        "\n",
        "    # Python APIではis_fused_kernel_acceptableを無視\n",
        "    return dropout_cpu(input, p=p, train=train)\n",
        "\n",
        "def scaled_dot_product_attention(\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    attn_mask: Optional[torch.Tensor] = None,\n",
        "    dropout_p: float = 0.0,\n",
        "    is_causal: bool = False,\n",
        "    scale: Optional[float] = None,\n",
        ") -> Tuple[torch.Tensor]:\n",
        "\n",
        "    origin_dtype = query.dtype\n",
        "    def maybe_upcast(t):\n",
        "        return t.float() if t.dtype in (torch.float16, torch.bfloat16) else t\n",
        "\n",
        "    query = maybe_upcast(query)\n",
        "    key = maybe_upcast(key)\n",
        "    value = maybe_upcast(value)\n",
        "\n",
        "    # GQA: query.heads != key.heads の場合、repeat で拡張\n",
        "    if query.size(1) != key.size(1):\n",
        "        qh, kh = query.size(1), key.size(1)\n",
        "        assert qh % kh == 0, \"GQA: query_heads must be divisible by key_heads\"\n",
        "        repeat_factor = qh // kh\n",
        "        key = key.repeat_interleave(repeat_factor, dim=1)\n",
        "        value = value.repeat_interleave(repeat_factor, dim=1)\n",
        "\n",
        "    # スケーリング\n",
        "    if scale is None:\n",
        "        scale = 1.0 / (key.size(-1) ** 0.5)\n",
        "\n",
        "    # Attentionスコア\n",
        "    attn_weights = torch.matmul(query, key.transpose(-2, -1)) * scale  # [B, H, Q, K]\n",
        "\n",
        "    # causal mask\n",
        "    if is_causal:\n",
        "        q_len, k_len = attn_weights.size(-2), attn_weights.size(-1)\n",
        "        causal_mask = torch.tril(torch.ones(q_len, k_len, device=attn_weights.device, dtype=torch.bool))\n",
        "        attn_weights = attn_weights.masked_fill(~causal_mask.view(1, 1, q_len, k_len), float('-inf'))\n",
        "\n",
        "    # attention_mask の適用（通常: [B, 1, Q, K] or [B, H, Q, K]）\n",
        "    if attn_mask is not None:\n",
        "        if attn_mask.dtype == torch.bool:\n",
        "            attn_weights = attn_weights.masked_fill(~attn_mask, float('-inf'))\n",
        "        else:\n",
        "            attn_weights = attn_weights + attn_mask  # additive attention bias\n",
        "\n",
        "    # softmax\n",
        "    attn_probs = safe_softmax(attn_weights, dim=-1)\n",
        "\n",
        "    # Dropout（訓練時のみ有効）\n",
        "    if dropout_p > 0.0 and torch.is_grad_enabled():\n",
        "        attn_probs = dropout(attn_probs, p=dropout_p, training=True)\n",
        "\n",
        "    # 出力（valueとの重み付き和）\n",
        "    out = torch.matmul(attn_probs, value)\n",
        "\n",
        "    return out.to(origin_dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "W5AqaGacSvRU"
      },
      "outputs": [],
      "source": [
        "class SpecialTokensMixin:\n",
        "    SPECIAL_TOKENS_ATTRIBUTES = [\n",
        "        \"bos_token\",\n",
        "        \"eos_token\",\n",
        "        \"unk_token\",\n",
        "        \"sep_token\",\n",
        "        \"pad_token\",\n",
        "        \"cls_token\",\n",
        "        \"mask_token\",\n",
        "        \"additional_special_tokens\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, verbose=False, **kwargs):\n",
        "        self._pad_token_type_id = 0\n",
        "        self.verbose = verbose\n",
        "        self._special_tokens_map = dict.fromkeys(self.SPECIAL_TOKENS_ATTRIBUTES)\n",
        "        self._special_tokens_map[\"additional_special_tokens\"] = []  # for BC where it defaults to empty list\n",
        "\n",
        "        # We directly set the hidden value to allow initialization with special tokens\n",
        "        # which are not yet in the vocabulary. Necessary for serialization/de-serialization\n",
        "        # TODO clean this up at some point (probably by switching to fast tokenizers)\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            if value is None:\n",
        "                continue\n",
        "            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
        "                if key == \"additional_special_tokens\":\n",
        "                    assert isinstance(value, (list, tuple)), f\"Value {value} is not a list or tuple\"\n",
        "                    assert all(isinstance(t, (str, AddedToken)) for t in value), (\n",
        "                        \"One of the tokens is not a string or an AddedToken\"\n",
        "                    )\n",
        "                    setattr(self, key, value)\n",
        "                elif isinstance(value, (str, AddedToken)):\n",
        "                    setattr(self, key, value)\n",
        "                else:\n",
        "                    raise TypeError(f\"Special token {key} has to be either str or AddedToken but got: {type(value)}\")\n",
        "\n",
        "\n",
        "class PreTrainedTokenizerBase(SpecialTokensMixin): #PushToHubMixin\n",
        "    model_input_names: list[str] = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
        "    padding_side: str = \"right\"\n",
        "    truncation_side: str = \"right\"\n",
        "    slow_tokenizer_class = None\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        # inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)\n",
        "        self.init_inputs = ()\n",
        "        for key in kwargs:\n",
        "            if hasattr(self, key) and callable(getattr(self, key)):\n",
        "                raise AttributeError(f\"{key} conflicts with the method {key} in {self.__class__.__name__}\")\n",
        "\n",
        "        self.init_kwargs = copy.deepcopy(kwargs)\n",
        "        self.name_or_path = kwargs.pop(\"name_or_path\", \"\")\n",
        "        self._processor_class = kwargs.pop(\"processor_class\", None)\n",
        "\n",
        "        # For backward compatibility we fallback to set model_max_length from max_len if provided\n",
        "        model_max_length = kwargs.pop(\"model_max_length\", kwargs.pop(\"max_len\", None))\n",
        "        self.model_max_length = model_max_length if model_max_length is not None else VERY_LARGE_INTEGER\n",
        "\n",
        "        # Padding and truncation side are right by default and overridden in subclasses. If specified in the kwargs, it\n",
        "        # is changed.\n",
        "        self.padding_side = kwargs.pop(\"padding_side\", self.padding_side)\n",
        "        if self.padding_side not in [\"right\", \"left\"]:\n",
        "            raise ValueError(\n",
        "                f\"Padding side should be selected between 'right' and 'left', current value: {self.padding_side}\"\n",
        "            )\n",
        "\n",
        "        self.truncation_side = kwargs.pop(\"truncation_side\", self.truncation_side)\n",
        "        if self.truncation_side not in [\"right\", \"left\"]:\n",
        "            raise ValueError(\n",
        "                f\"Truncation side should be selected between 'right' and 'left', current value: {self.truncation_side}\"\n",
        "            )\n",
        "\n",
        "        self.model_input_names = kwargs.pop(\"model_input_names\", self.model_input_names)\n",
        "\n",
        "        # By default, cleaning tokenization spaces for both fast and slow tokenizers\n",
        "        self.clean_up_tokenization_spaces = kwargs.pop(\"clean_up_tokenization_spaces\", False)\n",
        "\n",
        "        # By default, do not split special tokens for both fast and slow tokenizers\n",
        "        self.split_special_tokens = kwargs.pop(\"split_special_tokens\", False)\n",
        "\n",
        "        self.deprecation_warnings = {}  # Use to store when we have already noticed a deprecation warning (avoid overlogging).\n",
        "        self._in_target_context_manager = False\n",
        "\n",
        "        # Stores a Jinja template that formats chat histories into tokenizable strings\n",
        "        self.chat_template = kwargs.pop(\"chat_template\", None)\n",
        "        if isinstance(self.chat_template, (list, tuple)):\n",
        "            # Chat templates are stored as lists of dicts with fixed key names,\n",
        "            # we reconstruct that into a single dict while loading them.\n",
        "            self.chat_template = {template[\"name\"]: template[\"template\"] for template in self.chat_template}\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.extra_special_tokens = kwargs.pop(\"extra_special_tokens\", {})\n",
        "        self._set_model_specific_special_tokens(special_tokens=self.extra_special_tokens)\n",
        "\n",
        "\n",
        "class PreTrainedTokenizer(PreTrainedTokenizerBase):\n",
        "    def __init__(self, **kwargs):\n",
        "        # 1. Init the parent class\n",
        "\n",
        "        self.tokens_trie = Trie()\n",
        "\n",
        "        # 2. init `_added_tokens_decoder` if child class did not\n",
        "        if not hasattr(self, \"_added_tokens_decoder\"):\n",
        "            self._added_tokens_decoder: dict[int, AddedToken] = {}\n",
        "\n",
        "        # 3. if a `added_tokens_decoder` is passed, we are loading from a saved tokenizer, we overwrite\n",
        "        self._added_tokens_decoder.update(kwargs.pop(\"added_tokens_decoder\", {}))\n",
        "        self._added_tokens_encoder: dict[str, int] = {k.content: v for v, k in self._added_tokens_decoder.items()}\n",
        "\n",
        "        # 4 init the parent class\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # 4. If some of the special tokens are not part of the vocab, we add them, at the end.\n",
        "        # the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\n",
        "        self._add_tokens(\n",
        "            [token for token in self.all_special_tokens_extended if token not in self._added_tokens_encoder],\n",
        "            special_tokens=True,\n",
        "        )\n",
        "\n",
        "        self._decode_use_source_tokenizer = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[tuple[bool, str], bool]:\n",
        "    # Check if the package spec exists and grab its version to avoid importing a local directory\n",
        "    package_exists = importlib.util.find_spec(pkg_name) is not None\n",
        "    package_version = \"N/A\"\n",
        "    if package_exists:\n",
        "        try:\n",
        "            package_version = importlib.metadata.version(pkg_name)\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            # Fallback method: Only for \"torch\" and versions containing \"dev\"\n",
        "            if pkg_name == \"torch\":\n",
        "                try:\n",
        "                    package = importlib.import_module(pkg_name)\n",
        "                    temp_version = getattr(package, \"__version__\", \"N/A\")\n",
        "                    # Check if the version contains \"dev\"\n",
        "                    if \"dev\" in temp_version:\n",
        "                        package_version = temp_version\n",
        "                        package_exists = True\n",
        "                    else:\n",
        "                        package_exists = False\n",
        "                except ImportError:\n",
        "                    # If the package can't be imported, it's not available\n",
        "                    package_exists = False\n",
        "            elif pkg_name == \"quark\":\n",
        "                # TODO: remove once `importlib.metadata.packages_distributions()` is supported.\n",
        "                try:\n",
        "                    package_version = importlib.metadata.version(\"amd-quark\")\n",
        "                except Exception:\n",
        "                    package_exists = False\n",
        "            else:\n",
        "                # For packages other than \"torch\", don't attempt the fallback and set as not available\n",
        "                package_exists = False\n",
        "    if return_version:\n",
        "        return package_exists, package_version\n",
        "    else:\n",
        "        return package_exists\n",
        "\n",
        "_peft_available = _is_package_available(\"peft\")\n",
        "def is_peft_available():\n",
        "    return _peft_available\n",
        "\n",
        "if is_peft_available():\n",
        "    from transformers.utils import find_adapter_config_file\n",
        "\n",
        "def get_torch_context_manager_or_global_device():\n",
        "    device_in_context = torch.tensor([]).device\n",
        "    # `get_default_device` was only introduced in torch>=2.3 - use cpu otherwise to align the behavior\n",
        "    default_device = torch.get_default_device() if is_torch_greater_or_equal(\"2.3\") else torch.device(\"cpu\")\n",
        "    # This case means no context manager was used -> we still check if the default that was potentially set is not cpu\n",
        "    if device_in_context == default_device:\n",
        "        if default_device != torch.device(\"cpu\"):\n",
        "            return default_device\n",
        "        return None\n",
        "    return device_in_context\n",
        "\n",
        "def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n",
        "    if not _is_package_available(\"torch\"):\n",
        "        return False\n",
        "\n",
        "    if accept_dev:\n",
        "        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n",
        "            library_version\n",
        "        )\n",
        "    else:\n",
        "        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)"
      ],
      "metadata": {
        "id": "cVbkaPbk14xl"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n",
        "    from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n",
        "\n",
        "class TensorParallelLayer:\n",
        "    use_dtensor = True\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh): ...\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh): ...\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        if self.use_dtensor:\n",
        "            distribute_module(\n",
        "                module,\n",
        "                device_mesh,\n",
        "                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n",
        "                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n",
        "            )\n",
        "\n",
        "class ColwiseParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "        use_dtensor=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Replicate(),)\n",
        "        self.output_layouts = (output_layouts or Shard(-1),)\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        # TODO: figure out dynamo support for instance method and switch this to instance method\n",
        "        # annotate module input placements/sharding with input_layouts\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        # transform the input layouts to the desired layouts of ColwiseParallel\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=False)\n",
        "        return input_tensor\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        if param_type == \"bias\":\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n",
        "            shard = [Shard(-1)]\n",
        "        else:\n",
        "            shard = [Shard(-2)]\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2)\n",
        "\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(\n",
        "                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n",
        "            )\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        if outputs.placements != output_layouts:\n",
        "            outputs = outputs.redistribute(placements=output_layouts, async_op=False)\n",
        "        return outputs.to_local() if use_local_output else outputs"
      ],
      "metadata": {
        "id": "UzV_PEH1MjmS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KAkcvf3o07Z",
        "outputId": "7d8aa6b9-0c82-4ca9-8795-cd8f5fcb9270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected tokenizers version: 0.22.0\n"
          ]
        }
      ],
      "source": [
        "def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[tuple[bool, str], bool]:\n",
        "    package_exists = importlib.util.find_spec(pkg_name) is not None\n",
        "    package_version = \"N/A\"\n",
        "    if package_exists:\n",
        "        try:\n",
        "            package_version = importlib.metadata.version(pkg_name)\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            if pkg_name == \"torch\":\n",
        "                try:\n",
        "                    package = importlib.import_module(pkg_name)\n",
        "                    temp_version = getattr(package, \"__version__\", \"N/A\")\n",
        "                    if \"dev\" in temp_version:\n",
        "                        package_version = temp_version\n",
        "                        package_exists = True\n",
        "                    else:\n",
        "                        package_exists = False\n",
        "                except ImportError:\n",
        "                    package_exists = False\n",
        "            elif pkg_name == \"quark\":\n",
        "                try:\n",
        "                    package_version = importlib.metadata.version(\"amd-quark\")\n",
        "                except Exception:\n",
        "                    package_exists = False\n",
        "            else:\n",
        "                package_exists = False\n",
        "        print(f\"Detected {pkg_name} version: {package_version}\")\n",
        "    if return_version:\n",
        "        return package_exists, package_version\n",
        "    else:\n",
        "        return package_exists\n",
        "\n",
        "_torch_available = False\n",
        "_tf_available = False\n",
        "_flax_available = False\n",
        "_mlx_available = _is_package_available(\"mlx\")\n",
        "_tokenizers_available = _is_package_available(\"tokenizers\")\n",
        "\n",
        "def is_tokenizers_available():\n",
        "    return _tokenizers_available\n",
        "\n",
        "def is_torch_available():\n",
        "    return False #_torch_available\n",
        "\n",
        "def is_torch_fx_available():\n",
        "    return False #is_torch_available()\n",
        "\n",
        "def is_torch_fx_proxy(x):\n",
        "    #if is_torch_fx_available():\n",
        "        #import torch.fx\n",
        "        #return isinstance(x, torch.fx.Proxy)\n",
        "    return False\n",
        "\n",
        "def _is_torch(x):\n",
        "    import torch\n",
        "\n",
        "    return isinstance(x, torch.Tensor)\n",
        "\n",
        "def is_torch_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_tf_available():\n",
        "    return _tf_available\n",
        "\n",
        "def is_tf_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_flax_available():\n",
        "    return False\n",
        "\n",
        "def is_mlx_available():\n",
        "    return False\n",
        "\n",
        "def is_jax_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_numpy_array(x):\n",
        "    return _is_numpy(x)\n",
        "\n",
        "def is_mlx_array(x):\n",
        "    return False\n",
        "\n",
        "def _is_numpy(x):\n",
        "    return isinstance(x, np.ndarray)\n",
        "\n",
        "def infer_framework_from_repr(x):\n",
        "    representation = str(type(x))\n",
        "    if representation.startswith(\"<class 'torch.\"):\n",
        "        return \"pt\"\n",
        "    elif representation.startswith(\"<class 'tensorflow.\"):\n",
        "        return \"tf\"\n",
        "    elif representation.startswith(\"<class 'jax\"):\n",
        "        return \"jax\"\n",
        "    elif representation.startswith(\"<class 'numpy.\"):\n",
        "        return \"np\"\n",
        "    elif representation.startswith(\"<class 'mlx.\"):\n",
        "        return \"mlx\"\n",
        "\n",
        "def get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n",
        "    try:\n",
        "        return next(parameter.parameters()).device\n",
        "    except StopIteration:\n",
        "\n",
        "        def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n",
        "            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
        "            return tuples\n",
        "\n",
        "        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n",
        "        first_tuple = next(gen)\n",
        "        return first_tuple[1].device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Mj3ZBcGc1DoS"
      },
      "outputs": [],
      "source": [
        "SpecificPretrainedConfigType = TypeVar(\"SpecificPretrainedConfigType\", bound=\"PretrainedConfig\")\n",
        "\n",
        "class PretrainedConfig():\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        # All models common arguments\n",
        "        output_hidden_states: bool = False,\n",
        "        output_attentions: bool = False,\n",
        "        return_dict: bool = True,\n",
        "        torchscript: bool = False,\n",
        "        torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n",
        "        # Common arguments\n",
        "        pruned_heads: Optional[dict[int, list[int]]] = None,\n",
        "        tie_word_embeddings: bool = True,\n",
        "        chunk_size_feed_forward: int = 0,\n",
        "        is_encoder_decoder: bool = False,\n",
        "        is_decoder: bool = False,\n",
        "        cross_attention_hidden_size: Optional[int] = None,\n",
        "        add_cross_attention: bool = False,\n",
        "        tie_encoder_decoder: bool = False,\n",
        "        # Fine-tuning task arguments\n",
        "        architectures: Optional[list[str]] = None,\n",
        "        finetuning_task: Optional[str] = None,\n",
        "        id2label: Optional[dict[int, str]] = None,\n",
        "        label2id: Optional[dict[str, int]] = None,\n",
        "        num_labels: Optional[int] = None,\n",
        "        task_specific_params: Optional[dict[str, Any]] = None,\n",
        "        problem_type: Optional[str] = None,\n",
        "        # Tokenizer kwargs\n",
        "        tokenizer_class: Optional[str] = None,\n",
        "        prefix: Optional[str] = None,\n",
        "        bos_token_id: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        sep_token_id: Optional[int] = None,\n",
        "        decoder_start_token_id: Optional[int] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # Validation for some arguments\n",
        "        if label2id is not None and not isinstance(label2id, dict):\n",
        "            raise ValueError(\"Argument label2id should be a dictionary.\")\n",
        "        if id2label is not None and not isinstance(id2label, dict):\n",
        "            raise ValueError(\"Argument id2label should be a dictionary.\")\n",
        "        #if num_labels is not None and id2label is not None and len(id2label) != num_labels:\n",
        "            #logger.warning(\n",
        "                #f\"You passed `num_labels={num_labels}` which is incompatible to \"\n",
        "                #f\"the `id2label` map of length `{len(id2label)}`.\"\n",
        "            #)\n",
        "        if problem_type is not None and problem_type not in (\n",
        "            \"regression\",\n",
        "            \"single_label_classification\",\n",
        "            \"multi_label_classification\",\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"The config parameter `problem_type` was not understood: received {problem_type} \"\n",
        "                \"but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid.\"\n",
        "            )\n",
        "        if torch_dtype is not None and isinstance(torch_dtype, str) and is_torch_available():\n",
        "            # we will start using self.torch_dtype in v5, but to be consistent with\n",
        "            # from_pretrained's torch_dtype arg convert it to an actual torch.dtype object\n",
        "            import torch\n",
        "\n",
        "            torch_dtype = getattr(torch, torch_dtype)\n",
        "\n",
        "        # Attributes common for all models\n",
        "        self.return_dict = return_dict\n",
        "        self.output_hidden_states = output_hidden_states\n",
        "        self.torchscript = torchscript\n",
        "        self.torch_dtype = torch_dtype\n",
        "        self._output_attentions = output_attentions  # has public property\n",
        "\n",
        "        # Less common kwargs, only used by some models\n",
        "        self.pruned_heads = pruned_heads if pruned_heads is not None else {}\n",
        "        self.tie_word_embeddings = tie_word_embeddings\n",
        "        self.chunk_size_feed_forward = chunk_size_feed_forward\n",
        "\n",
        "        # Encoder-decoder models attributes\n",
        "        self.is_encoder_decoder = is_encoder_decoder\n",
        "        self.is_decoder = is_decoder  # used in encoder-decoder models to differentiate encoder from decoder\n",
        "        self.cross_attention_hidden_size = cross_attention_hidden_size\n",
        "        self.add_cross_attention = add_cross_attention\n",
        "        self.tie_encoder_decoder = tie_encoder_decoder\n",
        "\n",
        "        # Fine-tuning task attributes\n",
        "        self.architectures = architectures\n",
        "        self.finetuning_task = finetuning_task\n",
        "        self.id2label = id2label\n",
        "        self.label2id = label2id\n",
        "        self.task_specific_params = task_specific_params\n",
        "        self.problem_type = problem_type\n",
        "\n",
        "        if self.id2label is None:\n",
        "            self._create_id_label_maps(num_labels if num_labels is not None else 2)\n",
        "        else:\n",
        "            # Keys are always strings in JSON so convert ids to int here.\n",
        "            self.id2label = {int(key): value for key, value in self.id2label.items()}\n",
        "\n",
        "        # Tokenizer attributes\n",
        "        self.tokenizer_class = tokenizer_class\n",
        "        self.prefix = prefix\n",
        "        self.bos_token_id = bos_token_id\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.sep_token_id = sep_token_id\n",
        "        self.decoder_start_token_id = decoder_start_token_id\n",
        "\n",
        "        # Retrocompatibility: Parameters for sequence generation. While we will keep the ability to load these\n",
        "        # parameters, saving them will be deprecated. In a distant future, we won't need to load them.\n",
        "        for parameter_name, default_value in self._get_global_generation_defaults().items():\n",
        "            setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n",
        "\n",
        "        # Name or path to the pretrained checkpoint\n",
        "        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n",
        "        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        # Attention implementation to use, if relevant.\n",
        "        self._attn_implementation_internal = kwargs.pop(\"attn_implementation\", None)\n",
        "        self._attn_implementation_autoset = False\n",
        "\n",
        "        # Drop the transformers version info\n",
        "        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n",
        "\n",
        "        # Deal with gradient checkpointing\n",
        "        if kwargs.get(\"gradient_checkpointing\", False):\n",
        "            warnings.warn(\n",
        "                \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
        "                \"Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the \"\n",
        "                \"`Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\"\n",
        "            )\n",
        "\n",
        "        # Additional attributes without default values\n",
        "        for key, value in kwargs.items():\n",
        "            try:\n",
        "                setattr(self, key, value)\n",
        "            except AttributeError as err:\n",
        "                #logger.error(f\"Can't set {key} with value {value} for {self}\")\n",
        "                raise err\n",
        "\n",
        "        # TODO: remove later, deprecated arguments for TF models\n",
        "        self.tf_legacy_loss = kwargs.pop(\"tf_legacy_loss\", False)\n",
        "        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n",
        "\n",
        "    def _create_id_label_maps(self, num_labels: int):\n",
        "        self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n",
        "        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_global_generation_defaults() -> dict[str, Any]:\n",
        "        return {\n",
        "            \"max_length\": 20,\n",
        "            \"min_length\": 0,\n",
        "            \"do_sample\": False,\n",
        "            \"early_stopping\": False,\n",
        "            \"num_beams\": 1,\n",
        "            \"num_beam_groups\": 1,\n",
        "            \"diversity_penalty\": 0.0,\n",
        "            \"temperature\": 1.0,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 1.0,\n",
        "            \"typical_p\": 1.0,\n",
        "            \"repetition_penalty\": 1.0,\n",
        "            \"length_penalty\": 1.0,\n",
        "            \"no_repeat_ngram_size\": 0,\n",
        "            \"encoder_no_repeat_ngram_size\": 0,\n",
        "            \"bad_words_ids\": None,\n",
        "            \"num_return_sequences\": 1,\n",
        "            \"output_scores\": False,\n",
        "            \"return_dict_in_generate\": False,\n",
        "            \"forced_bos_token_id\": None,\n",
        "            \"forced_eos_token_id\": None,\n",
        "            \"remove_invalid_values\": False,\n",
        "            \"exponential_decay_length_penalty\": None,\n",
        "            \"suppress_tokens\": None,\n",
        "            \"begin_suppress_tokens\": None,\n",
        "            }\n",
        "\n",
        "    def to_dict(self) -> dict[str, Any]:\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        if hasattr(self.__class__, \"model_type\"):\n",
        "            output[\"model_type\"] = self.__class__.model_type\n",
        "\n",
        "        output[\"transformers_version\"] = __version__\n",
        "\n",
        "        for key, value in output.items():\n",
        "            # Deal with nested configs like CLIP\n",
        "            if isinstance(value, PretrainedConfig):\n",
        "                value = value.to_dict()\n",
        "                del value[\"transformers_version\"]\n",
        "\n",
        "            output[key] = value\n",
        "\n",
        "        self._remove_keys_not_serialized(output)\n",
        "\n",
        "        if hasattr(self, \"quantization_config\"):\n",
        "            output[\"quantization_config\"] = (\n",
        "                self.quantization_config.to_dict()\n",
        "                if not isinstance(self.quantization_config, dict)\n",
        "                else self.quantization_config\n",
        "            )\n",
        "        self.dict_torch_dtype_to_str(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n",
        "        if hasattr(self, \"quantization_config\"):\n",
        "            _ = d.pop(\"_pre_quantization_dtype\", None)\n",
        "\n",
        "        if \"_auto_class\" in d:\n",
        "            del d[\"_auto_class\"]\n",
        "        if \"_output_attentions\" in d:\n",
        "            d[\"output_attentions\"] = d.pop(\"_output_attentions\")\n",
        "        if \"_commit_hash\" in d:\n",
        "            del d[\"_commit_hash\"]\n",
        "        if \"_attn_implementation_internal\" in d:\n",
        "            del d[\"_attn_implementation_internal\"]\n",
        "        if \"_attn_implementation_autoset\" in d:\n",
        "            del d[\"_attn_implementation_autoset\"]\n",
        "        if \"base_model_tp_plan\" in d:\n",
        "            del d[\"base_model_tp_plan\"]\n",
        "        if \"base_model_pp_plan\" in d:\n",
        "            del d[\"base_model_pp_plan\"]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self._remove_keys_not_serialized(value)\n",
        "\n",
        "    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n",
        "        if d.get(\"torch_dtype\", None) is not None:\n",
        "            if isinstance(d[\"torch_dtype\"], dict):\n",
        "                d[\"torch_dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"torch_dtype\"].items()}\n",
        "            elif not isinstance(d[\"torch_dtype\"], str):\n",
        "                d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self.dict_torch_dtype_to_str(value)\n",
        "\n",
        "    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n",
        "        decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n",
        "        encoder_possible_text_config_names = (\"text_encoder\",)\n",
        "        if decoder:\n",
        "            possible_text_config_names = decoder_possible_text_config_names\n",
        "        else:\n",
        "            possible_text_config_names = encoder_possible_text_config_names + decoder_possible_text_config_names\n",
        "\n",
        "        valid_text_config_names = []\n",
        "        for text_config_name in possible_text_config_names:\n",
        "            if hasattr(self, text_config_name):\n",
        "                text_config = getattr(self, text_config_name, None)\n",
        "                if text_config is not None:\n",
        "                    valid_text_config_names += [text_config_name]\n",
        "\n",
        "        if len(valid_text_config_names) > 1:\n",
        "            raise ValueError(\n",
        "                f\"Multiple valid text configs were found in the model config: {valid_text_config_names}. In this \"\n",
        "                \"case, using `get_text_config()` would be ambiguous. Please specify the desied text config directly.\"\n",
        "            )\n",
        "        elif len(valid_text_config_names) == 1:\n",
        "            config_to_return = getattr(self, valid_text_config_names[0])\n",
        "        else:\n",
        "            config_to_return = self\n",
        "        return config_to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "dZvtP16KEzSb"
      },
      "outputs": [],
      "source": [
        "\n",
        "TOKENIZER_MAPPING_NAMES = OrderedDict[str, tuple[Optional[str], Optional[str]]](\n",
        "[\n",
        "    (\"colqwen2\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "    (\"internvl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "    (\n",
        "        \"qwen2\",\n",
        "        (\n",
        "            \"Qwen2Tokenizer\",\n",
        "            \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "        ),\n",
        "    ),\n",
        "          (\"qwen2_5_omni\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\"qwen2_5_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\"qwen2_audio\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\n",
        "            \"qwen2_moe\",\n",
        "            (\n",
        "                \"Qwen2Tokenizer\",\n",
        "                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "            ),\n",
        "        ),\n",
        "        (\"qwen2_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\n",
        "            \"qwen3\",\n",
        "            (\n",
        "                \"Qwen2Tokenizer\",\n",
        "                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"qwen3_moe\",\n",
        "            (\n",
        "                \"Qwen2Tokenizer\",\n",
        "                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "            ),\n",
        "        ),\n",
        "])\n",
        "\n",
        "_CallableT = TypeVar(\"_CallableT\", bound=Callable[..., Any])\n",
        "\n",
        "def _compute_linear_scaling_rope_parameters(\n",
        "    config: Optional[PretrainedConfig] = None,\n",
        "    device: Optional[\"torch.device\"] = None,\n",
        "    seq_len: Optional[int] = None,\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    factor = config.rope_scaling[\"factor\"]\n",
        "\n",
        "    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n",
        "\n",
        "    inv_freq /= factor\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "\n",
        "def _compute_dynamic_ntk_parameters(\n",
        "    config: Optional[PretrainedConfig] = None,\n",
        "    device: Optional[\"torch.device\"] = None,\n",
        "    seq_len: Optional[int] = None,\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "    max_position_embeddings = config.max_position_embeddings\n",
        "    factor = config.rope_scaling[\"factor\"]\n",
        "\n",
        "    attention_factor = 1.0  # Unused in this type of RoPE\n",
        "\n",
        "    if seq_len is None:\n",
        "        seq_len = max_position_embeddings\n",
        "    elif isinstance(seq_len, torch.Tensor):\n",
        "        seq_len = torch.maximum(\n",
        "            seq_len,\n",
        "            torch.tensor(max_position_embeddings, dtype=seq_len.dtype, device=seq_len.device),\n",
        "        )\n",
        "    else:\n",
        "        seq_len = max(seq_len, max_position_embeddings)\n",
        "\n",
        "    base = base * ((factor * seq_len / max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))\n",
        "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "\n",
        "def _compute_yarn_parameters(\n",
        "    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "    factor = config.rope_scaling[\"factor\"]\n",
        "    attention_factor = config.rope_scaling.get(\"attention_factor\")\n",
        "    mscale = config.rope_scaling.get(\"mscale\")\n",
        "    mscale_all_dim = config.rope_scaling.get(\"mscale_all_dim\")\n",
        "\n",
        "    if \"original_max_position_embeddings\" in config.rope_scaling:\n",
        "        original_max_position_embeddings = config.rope_scaling[\"original_max_position_embeddings\"]\n",
        "        factor = config.max_position_embeddings / original_max_position_embeddings\n",
        "    else:\n",
        "        original_max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "    def get_mscale(scale, mscale=1):\n",
        "        if scale <= 1:\n",
        "            return 1.0\n",
        "        return 0.1 * mscale * math.log(scale) + 1.0\n",
        "\n",
        "    if attention_factor is None:\n",
        "        if mscale and mscale_all_dim:\n",
        "            attention_factor = float(get_mscale(factor, mscale) / get_mscale(factor, mscale_all_dim))\n",
        "        else:\n",
        "            attention_factor = get_mscale(factor)\n",
        "\n",
        "    beta_fast = config.rope_scaling.get(\"beta_fast\") or 32\n",
        "    beta_slow = config.rope_scaling.get(\"beta_slow\") or 1\n",
        "\n",
        "    def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n",
        "        return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (2 * math.log(base))\n",
        "\n",
        "    def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings):\n",
        "        low = math.floor(find_correction_dim(low_rot, dim, base, max_position_embeddings))\n",
        "        high = math.ceil(find_correction_dim(high_rot, dim, base, max_position_embeddings))\n",
        "        return max(low, 0), min(high, dim - 1)\n",
        "\n",
        "    def linear_ramp_factor(min, max, dim):\n",
        "        if min == max:\n",
        "            max += 0.001  # Prevent singularity\n",
        "\n",
        "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
        "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
        "        return ramp_func\n",
        "\n",
        "    pos_freqs = base ** (torch.arange(0, dim, 2).to(device=device, dtype=torch.float) / dim)\n",
        "    inv_freq_extrapolation = 1.0 / pos_freqs\n",
        "    inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n",
        "\n",
        "    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings)\n",
        "\n",
        "    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(device=device, dtype=torch.float)\n",
        "    inv_freq = (\n",
        "        inv_freq_interpolation * (1 - inv_freq_extrapolation_factor)\n",
        "        + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
        "    )\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "def _compute_longrope_parameters(\n",
        "    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "    long_factor = config.rope_scaling[\"long_factor\"]\n",
        "    short_factor = config.rope_scaling[\"short_factor\"]\n",
        "    factor = config.rope_scaling.get(\"factor\")\n",
        "    attention_factor = config.rope_scaling.get(\"attention_factor\")\n",
        "\n",
        "    if hasattr(config, \"original_max_position_embeddings\"):\n",
        "        original_max_position_embeddings = config.original_max_position_embeddings\n",
        "        factor = config.max_position_embeddings / config.original_max_position_embeddings\n",
        "    else:\n",
        "        original_max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "    if attention_factor is None:\n",
        "        if factor <= 1.0:\n",
        "            attention_factor = 1.0\n",
        "        else:\n",
        "            attention_factor = math.sqrt(1 + math.log(factor) / math.log(original_max_position_embeddings))\n",
        "\n",
        "    if seq_len and seq_len > original_max_position_embeddings:\n",
        "        ext_factors = torch.tensor(long_factor, dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        ext_factors = torch.tensor(short_factor, dtype=torch.float32, device=device)\n",
        "    inv_freq_shape = torch.arange(0, dim, 2, dtype=torch.int64, device=device).float() / dim\n",
        "    inv_freq = 1.0 / (ext_factors * base**inv_freq_shape)\n",
        "\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "def _compute_llama3_parameters(\n",
        "    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n",
        "\n",
        "    factor = config.rope_scaling[\"factor\"]  # `8` in the original implementation\n",
        "    low_freq_factor = config.rope_scaling[\"low_freq_factor\"]  # `1` in the original implementation\n",
        "    high_freq_factor = config.rope_scaling[\"high_freq_factor\"]  # `4` in the original implementation\n",
        "    old_context_len = config.rope_scaling[\"original_max_position_embeddings\"]  # `8192` in the original implementation\n",
        "\n",
        "    low_freq_wavelen = old_context_len / low_freq_factor\n",
        "    high_freq_wavelen = old_context_len / high_freq_factor\n",
        "\n",
        "    wavelen = 2 * math.pi / inv_freq\n",
        "\n",
        "    inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)\n",
        "\n",
        "    smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n",
        "    smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama\n",
        "    is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)\n",
        "    inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
        "\n",
        "    return inv_freq_llama, attention_factor\n",
        "\n",
        "\n",
        "def _compute_default_rope_parameters(\n",
        "    config: Optional[PretrainedConfig] = None,\n",
        "    device: Optional[\"torch.device\"] = None,\n",
        "    seq_len: Optional[int] = None,\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "\n",
        "    attention_factor = 1.0\n",
        "\n",
        "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "def rope_config_validation(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n",
        "    rope_scaling = getattr(config, \"rope_scaling\", None)  # not a default parameter in `PretrainedConfig`\n",
        "    if rope_scaling is None:\n",
        "        return\n",
        "\n",
        "    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", \"default\"))\n",
        "    validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n",
        "    if validation_fn is not None:\n",
        "        validation_fn(config, ignore_keys=ignore_keys)\n",
        "    #else:\n",
        "        #logger.warning(\n",
        "            #f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\"\n",
        "        #)\n",
        "\n",
        "ROPE_INIT_FUNCTIONS = {\n",
        "    \"default\": _compute_default_rope_parameters,\n",
        "    \"linear\": _compute_linear_scaling_rope_parameters,\n",
        "    \"dynamic\": _compute_dynamic_ntk_parameters,\n",
        "    \"yarn\": _compute_yarn_parameters,\n",
        "    \"longrope\": _compute_longrope_parameters,\n",
        "    \"llama3\": _compute_llama3_parameters,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "ImrU5Z6qpOJy"
      },
      "outputs": [],
      "source": [
        "TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n",
        "\n",
        "_T = TypeVar(\"_T\")\n",
        "_LazyAutoMappingValue = tuple[Union[type[Any], None], Union[type[Any], None]]\n",
        "\n",
        "class _LazyAutoMapping(OrderedDict[type[PretrainedConfig], _LazyAutoMappingValue]):\n",
        "    def __init__(self, config_mapping, model_mapping) -> None:\n",
        "        self._config_mapping = config_mapping\n",
        "        self._reverse_config_mapping = {v: k for k, v in config_mapping.items()}\n",
        "        self._model_mapping = model_mapping\n",
        "        self._model_mapping._model_mapping = self\n",
        "        self._extra_content = {}\n",
        "        self._modules = {}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        common_keys = set(self._config_mapping.keys()).intersection(self._model_mapping.keys())\n",
        "        return len(common_keys) + len(self._extra_content)\n",
        "\n",
        "    def __getitem__(self, key: type[PretrainedConfig]) -> _LazyAutoMappingValue:\n",
        "        if key in self._extra_content:\n",
        "            return self._extra_content[key]\n",
        "        model_type = self._reverse_config_mapping[key.__name__]\n",
        "        if model_type in self._model_mapping:\n",
        "            model_name = self._model_mapping[model_type]\n",
        "            return self._load_attr_from_module(model_type, model_name)\n",
        "\n",
        "        # Maybe there was several model types associated with this config.\n",
        "        model_types = [k for k, v in self._config_mapping.items() if v == key.__name__]\n",
        "        for mtype in model_types:\n",
        "            if mtype in self._model_mapping:\n",
        "                model_name = self._model_mapping[mtype]\n",
        "                return self._load_attr_from_module(mtype, model_name)\n",
        "        raise KeyError(key)\n",
        "\n",
        "    def _load_attr_from_module(self, model_type, attr):\n",
        "        module_name = model_type_to_module_name(model_type)\n",
        "        if module_name not in self._modules:\n",
        "            self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n",
        "        return getattribute_from_module(self._modules[module_name], attr)\n",
        "\n",
        "    def keys(self) -> list[type[PretrainedConfig]]:\n",
        "        mapping_keys = [\n",
        "            self._load_attr_from_module(key, name)\n",
        "            for key, name in self._config_mapping.items()\n",
        "            if key in self._model_mapping.keys()\n",
        "        ]\n",
        "        return mapping_keys + list(self._extra_content.keys())\n",
        "\n",
        "    def get(self, key: type[PretrainedConfig], default: _T) -> Union[_LazyAutoMappingValue, _T]:\n",
        "        try:\n",
        "            return self.__getitem__(key)\n",
        "        except KeyError:\n",
        "            return default\n",
        "\n",
        "    def __bool__(self) -> bool:\n",
        "        return bool(self.keys())\n",
        "\n",
        "    def values(self) -> list[_LazyAutoMappingValue]:\n",
        "        mapping_values = [\n",
        "            self._load_attr_from_module(key, name)\n",
        "            for key, name in self._model_mapping.items()\n",
        "            if key in self._config_mapping.keys()\n",
        "        ]\n",
        "        return mapping_values + list(self._extra_content.values())\n",
        "\n",
        "    def items(self) -> list[tuple[type[PretrainedConfig], _LazyAutoMappingValue]]:\n",
        "        mapping_items = [\n",
        "            (\n",
        "                self._load_attr_from_module(key, self._config_mapping[key]),\n",
        "                self._load_attr_from_module(key, self._model_mapping[key]),\n",
        "            )\n",
        "            for key in self._model_mapping.keys()\n",
        "            if key in self._config_mapping.keys()\n",
        "        ]\n",
        "        return mapping_items + list(self._extra_content.items())\n",
        "\n",
        "    def __iter__(self) -> Iterator[type[PretrainedConfig]]:\n",
        "        return iter(self.keys())\n",
        "\n",
        "    def __contains__(self, item: type) -> bool:\n",
        "        if item in self._extra_content:\n",
        "            return True\n",
        "        if not hasattr(item, \"__name__\") or item.__name__ not in self._reverse_config_mapping:\n",
        "            return False\n",
        "        model_type = self._reverse_config_mapping[item.__name__]\n",
        "        return model_type in self._model_mapping\n",
        "\n",
        "    def register(self, key: type[PretrainedConfig], value: _LazyAutoMappingValue, exist_ok=False) -> None:\n",
        "        if hasattr(key, \"__name__\") and key.__name__ in self._reverse_config_mapping:\n",
        "            model_type = self._reverse_config_mapping[key.__name__]\n",
        "            if model_type in self._model_mapping.keys() and not exist_ok:\n",
        "                raise ValueError(f\"'{key}' is already used by a Transformers model.\")\n",
        "\n",
        "        self._extra_content[key] = value\n",
        "\n",
        "CONFIG_MAPPING_NAMES = OrderedDict[str, str]([])\n",
        "\n",
        "TOKENIZER_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TOKENIZER_MAPPING_NAMES)\n",
        "\n",
        "ALLOWED_LAYER_TYPES = (\n",
        "    \"full_attention\",\n",
        "    \"sliding_attention\",\n",
        "    \"chunked_attention\",\n",
        "    \"linear_attention\",\n",
        ")\n",
        "\n",
        "def layer_type_validation(layer_types: list[str]):\n",
        "    if not all(layer_type in ALLOWED_LAYER_TYPES for layer_type in layer_types):\n",
        "        raise ValueError(f\"The `layer_types` entries must be in {ALLOWED_LAYER_TYPES}\")\n",
        "\n",
        "class ClassInstantier(OrderedDict):\n",
        "    def __getitem__(self, key):\n",
        "        content = super().__getitem__(key)\n",
        "        cls, kwargs = content if isinstance(content, tuple) else (content, {})\n",
        "        return cls(**kwargs)\n",
        "\n",
        "class ClippedGELUActivation(nn.Module):\n",
        "    def __init__(self, min: float, max: float):\n",
        "        if min > max:\n",
        "            raise ValueError(f\"min should be < max (got min: {min}, max: {max})\")\n",
        "\n",
        "        super().__init__()\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.clip(gelu(x), self.min, self.max)\n",
        "\n",
        "class FastGELUActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * input * (1.0 + torch.tanh(input * 0.7978845608 * (1.0 + 0.044715 * input * input)))\n",
        "\n",
        "class NewGELUActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
        "\n",
        "class PytorchGELUTanh(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return nn.functional.gelu(input, approximate=\"tanh\")\n",
        "\n",
        "class AccurateGELUActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.precomputed_constant = math.sqrt(2 / math.pi)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * input * (1 + torch.tanh(self.precomputed_constant * (input + 0.044715 * torch.pow(input, 3))))\n",
        "\n",
        "class LaplaceActivation(nn.Module):\n",
        "    def forward(self, input, mu=0.707107, sigma=0.282095):\n",
        "        input = (input - mu).div(sigma * math.sqrt(2.0))\n",
        "        return 0.5 * (1.0 + torch.erf(input))\n",
        "\n",
        "class LinearActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input\n",
        "\n",
        "class MishActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.act = nn.functional.mish\n",
        "\n",
        "    def _mish_python(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input * torch.tanh(nn.functional.softplus(input))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(input)\n",
        "\n",
        "class QuickGELUActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input * torch.sigmoid(1.702 * input)\n",
        "\n",
        "class ReLUSquaredActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        relu_applied = nn.functional.relu(input)\n",
        "        squared = torch.square(relu_applied)\n",
        "        return squared\n",
        "\n",
        "\n",
        "def replace_list_option_in_docstrings(\n",
        "    config_to_class=None, use_model_types: bool = True\n",
        ") -> Callable[[_CallableT], _CallableT]:\n",
        "    def docstring_decorator(fn):\n",
        "        docstrings = fn.__doc__\n",
        "        if docstrings is None:\n",
        "            return fn\n",
        "        lines = docstrings.split(\"\\n\")\n",
        "        i = 0\n",
        "        while i < len(lines) and re.search(r\"^(\\s*)List options\\s*$\", lines[i]) is None:\n",
        "            i += 1\n",
        "        if i < len(lines):\n",
        "            indent = re.search(r\"^(\\s*)List options\\s*$\", lines[i]).groups()[0]\n",
        "            if use_model_types:\n",
        "                indent = f\"{indent}    \"\n",
        "            lines[i] = _list_model_options(indent, config_to_class=config_to_class, use_model_types=use_model_types)\n",
        "            docstrings = \"\\n\".join(lines)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"The function {fn} should have an empty 'List options' in its docstring as placeholder, current\"\n",
        "                f\" docstring is:\\n{docstrings}\"\n",
        "            )\n",
        "        fn.__doc__ = docstrings\n",
        "        return fn\n",
        "\n",
        "    return docstring_decorator\n",
        "\n",
        "def get_tokenizer_config(\n",
        "    pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n",
        "    cache_dir: Optional[Union[str, os.PathLike[str]]] = None,\n",
        "    force_download: bool = False,\n",
        "    resume_download: Optional[bool] = None,\n",
        "    proxies: Optional[dict[str, str]] = None,\n",
        "    token: Optional[Union[bool, str]] = None,\n",
        "    revision: Optional[str] = None,\n",
        "    local_files_only: bool = False,\n",
        "    subfolder: str = \"\",\n",
        "    **kwargs,\n",
        ") -> dict[str, Any]:\n",
        "    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "    if use_auth_token is not None:\n",
        "        warnings.warn(\n",
        "            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
        "            FutureWarning,\n",
        "        )\n",
        "        if token is not None:\n",
        "            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n",
        "        token = use_auth_token\n",
        "\n",
        "    commit_hash = kwargs.get(\"_commit_hash\", None)\n",
        "    resolved_config_file = cached_file(\n",
        "        pretrained_model_name_or_path,\n",
        "        TOKENIZER_CONFIG_FILE,\n",
        "        cache_dir=cache_dir,\n",
        "        force_download=force_download,\n",
        "        resume_download=resume_download,\n",
        "        proxies=proxies,\n",
        "        token=token,\n",
        "        revision=revision,\n",
        "        local_files_only=local_files_only,\n",
        "        subfolder=subfolder,\n",
        "        _raise_exceptions_for_gated_repo=False,\n",
        "        _raise_exceptions_for_missing_entries=False,\n",
        "        _raise_exceptions_for_connection_errors=False,\n",
        "        _commit_hash=commit_hash,\n",
        "    )\n",
        "    if resolved_config_file is None:\n",
        "        #logger.info(\"Could not locate the tokenizer configuration file, will try to use the model config instead.\")\n",
        "        return {}\n",
        "    commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "\n",
        "    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n",
        "        result = json.load(reader)\n",
        "    result[\"_commit_hash\"] = commit_hash\n",
        "    return result\n",
        "\n",
        "def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n",
        "    if class_name == \"PreTrainedTokenizerFast\":\n",
        "        return PreTrainedTokenizerFast\n",
        "\n",
        "    for module_name, tokenizers in TOKENIZER_MAPPING_NAMES.items():\n",
        "        if class_name in tokenizers:\n",
        "            module_name = model_type_to_module_name(module_name)\n",
        "            if module_name in [\"mistral\", \"mixtral\"] and class_name == \"MistralCommonTokenizer\":\n",
        "                module = importlib.import_module(\".tokenization_mistral_common\", \"transformers\")\n",
        "            else:\n",
        "                module = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n",
        "            try:\n",
        "                return getattr(module, class_name)\n",
        "            except AttributeError:\n",
        "                continue\n",
        "\n",
        "    for config, tokenizers in TOKENIZER_MAPPING._extra_content.items():\n",
        "        for tokenizer in tokenizers:\n",
        "            if getattr(tokenizer, \"__name__\", None) == class_name:\n",
        "                return tokenizer\n",
        "\n",
        "    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n",
        "    # init and we return the proper dummy to get an appropriate error message.\n",
        "    main_module = importlib.import_module(\"transformers\")\n",
        "    if hasattr(main_module, class_name):\n",
        "        return getattr(main_module, class_name)\n",
        "\n",
        "    return None\n",
        "\n",
        "class AutoTokenizer_no:\n",
        "    def __init__(self):\n",
        "        raise OSError(\n",
        "            \"AutoTokenizer is designed to be instantiated \"\n",
        "            \"using the `AutoTokenizer.from_pretrained(pretrained_model_name_or_path)` method.\"\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    @replace_list_option_in_docstrings(TOKENIZER_MAPPING_NAMES)\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "        if use_auth_token is not None:\n",
        "            warnings.warn(\n",
        "                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            if kwargs.get(\"token\", None) is not None:\n",
        "                raise ValueError(\n",
        "                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n",
        "                )\n",
        "            kwargs[\"token\"] = use_auth_token\n",
        "\n",
        "        config = kwargs.pop(\"config\", None)\n",
        "        kwargs[\"_from_auto\"] = True\n",
        "\n",
        "        use_fast = kwargs.pop(\"use_fast\", True)\n",
        "        tokenizer_type = kwargs.pop(\"tokenizer_type\", None)\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        gguf_file = kwargs.get(\"gguf_file\", None)\n",
        "\n",
        "        if tokenizer_type is not None:\n",
        "            tokenizer_class = None\n",
        "            tokenizer_class_tuple = TOKENIZER_MAPPING_NAMES.get(tokenizer_type, None)\n",
        "\n",
        "            if tokenizer_class_tuple is None:\n",
        "                raise ValueError(\n",
        "                    f\"Passed `tokenizer_type` {tokenizer_type} does not exist. `tokenizer_type` should be one of \"\n",
        "                    f\"{', '.join(c for c in TOKENIZER_MAPPING_NAMES.keys())}.\"\n",
        "                )\n",
        "\n",
        "            tokenizer_class_name, tokenizer_fast_class_name = tokenizer_class_tuple\n",
        "\n",
        "            if use_fast:\n",
        "                if tokenizer_fast_class_name is not None:\n",
        "                    tokenizer_class = tokenizer_class_from_name(tokenizer_fast_class_name)\n",
        "                #else:\n",
        "                    #logger.warning(\n",
        "                        #\"`use_fast` is set to `True` but the tokenizer class does not have a fast version. \"\n",
        "                        #\" Falling back to the slow version.\"\n",
        "                    #)\n",
        "            if tokenizer_class is None:\n",
        "                tokenizer_class = tokenizer_class_from_name(tokenizer_class_name)\n",
        "\n",
        "            if tokenizer_class is None:\n",
        "                raise ValueError(f\"Tokenizer class {tokenizer_class_name} is not currently imported.\")\n",
        "\n",
        "            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "\n",
        "        tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
        "        if \"_commit_hash\" in tokenizer_config:\n",
        "            kwargs[\"_commit_hash\"] = tokenizer_config[\"_commit_hash\"]\n",
        "        config_tokenizer_class = tokenizer_config.get(\"tokenizer_class\")\n",
        "        tokenizer_auto_map = None\n",
        "        if \"auto_map\" in tokenizer_config:\n",
        "            if isinstance(tokenizer_config[\"auto_map\"], (tuple, list)):\n",
        "                # Legacy format for dynamic tokenizers\n",
        "                tokenizer_auto_map = tokenizer_config[\"auto_map\"]\n",
        "            else:\n",
        "                tokenizer_auto_map = tokenizer_config[\"auto_map\"].get(\"AutoTokenizer\", None)\n",
        "\n",
        "        if config_tokenizer_class is None:\n",
        "            if not isinstance(config, PretrainedConfig):\n",
        "                if gguf_file:\n",
        "                    gguf_path = cached_file(pretrained_model_name_or_path, gguf_file, **kwargs)\n",
        "                    config_dict = load_gguf_checkpoint(gguf_path, return_tensors=False)[\"config\"]\n",
        "                    config = AutoConfig.for_model(**config_dict)\n",
        "                else:\n",
        "                    config = AutoConfig.from_pretrained(\n",
        "                        pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n",
        "                    )\n",
        "            config_tokenizer_class = config.tokenizer_class\n",
        "            if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\n",
        "                tokenizer_auto_map = config.auto_map[\"AutoTokenizer\"]\n",
        "\n",
        "        has_remote_code = tokenizer_auto_map is not None\n",
        "        has_local_code = type(config) in TOKENIZER_MAPPING or (\n",
        "            config_tokenizer_class is not None\n",
        "            and (\n",
        "                tokenizer_class_from_name(config_tokenizer_class) is not None\n",
        "                or tokenizer_class_from_name(config_tokenizer_class + \"Fast\") is not None\n",
        "            )\n",
        "        )\n",
        "        if has_remote_code:\n",
        "            if use_fast and tokenizer_auto_map[1] is not None:\n",
        "                class_ref = tokenizer_auto_map[1]\n",
        "            else:\n",
        "                class_ref = tokenizer_auto_map[0]\n",
        "            if \"--\" in class_ref:\n",
        "                upstream_repo = class_ref.split(\"--\")[0]\n",
        "            else:\n",
        "                upstream_repo = None\n",
        "            trust_remote_code = resolve_trust_remote_code(\n",
        "                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n",
        "            )\n",
        "\n",
        "        if has_remote_code and trust_remote_code:\n",
        "            tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\n",
        "            _ = kwargs.pop(\"code_revision\", None)\n",
        "            tokenizer_class.register_for_auto_class()\n",
        "            return tokenizer_class.from_pretrained(\n",
        "                pretrained_model_name_or_path, *inputs, trust_remote_code=trust_remote_code, **kwargs\n",
        "            )\n",
        "        elif config_tokenizer_class is not None:\n",
        "            tokenizer_class = None\n",
        "            if use_fast and not config_tokenizer_class.endswith(\"Fast\"):\n",
        "                tokenizer_class_candidate = f\"{config_tokenizer_class}Fast\"\n",
        "                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n",
        "            if tokenizer_class is None:\n",
        "                tokenizer_class_candidate = config_tokenizer_class\n",
        "                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n",
        "            if tokenizer_class is None:\n",
        "                raise ValueError(\n",
        "                    f\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\n",
        "                )\n",
        "            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "\n",
        "        if isinstance(config, EncoderDecoderConfig):\n",
        "            #if type(config.decoder) is not type(config.encoder):  # noqa: E721\n",
        "                #logger.warning(\n",
        "                    #f\"The encoder model config class: {config.encoder.__class__} is different from the decoder model \"\n",
        "                    #f\"config class: {config.decoder.__class__}. It is not recommended to use the \"\n",
        "                    #\"`AutoTokenizer.from_pretrained()` method in this case. Please use the encoder and decoder \"\n",
        "                    #\"specific tokenizer classes.\"\n",
        "                #)\n",
        "            config = config.encoder\n",
        "\n",
        "        model_type = config_class_to_model_type(type(config).__name__)\n",
        "        if model_type is not None:\n",
        "            tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\n",
        "\n",
        "            if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):\n",
        "                return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "            else:\n",
        "                if tokenizer_class_py is not None:\n",
        "                    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        \"This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \"\n",
        "                        \"in order to use this tokenizer.\"\n",
        "                    )\n",
        "\n",
        "        raise ValueError(\n",
        "            f\"Unrecognized configuration class {config.__class__} to build an AutoTokenizer.\\n\"\n",
        "            f\"Model type should be one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING.keys())}.\"\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False):\n",
        "        if slow_tokenizer_class is None and fast_tokenizer_class is None:\n",
        "            raise ValueError(\"You need to pass either a `slow_tokenizer_class` or a `fast_tokenizer_class\")\n",
        "        if slow_tokenizer_class is not None and issubclass(slow_tokenizer_class, PreTrainedTokenizerFast):\n",
        "            raise ValueError(\"You passed a fast tokenizer in the `slow_tokenizer_class`.\")\n",
        "        if fast_tokenizer_class is not None and issubclass(fast_tokenizer_class, PreTrainedTokenizer):\n",
        "            raise ValueError(\"You passed a slow tokenizer in the `fast_tokenizer_class`.\")\n",
        "\n",
        "        if (\n",
        "            slow_tokenizer_class is not None\n",
        "            and fast_tokenizer_class is not None\n",
        "            and issubclass(fast_tokenizer_class, PreTrainedTokenizerFast)\n",
        "            and fast_tokenizer_class.slow_tokenizer_class != slow_tokenizer_class\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"The fast tokenizer class you are passing has a `slow_tokenizer_class` attribute that is not \"\n",
        "                \"consistent with the slow tokenizer class you passed (fast tokenizer has \"\n",
        "                f\"{fast_tokenizer_class.slow_tokenizer_class} and you passed {slow_tokenizer_class}. Fix one of those \"\n",
        "                \"so they match!\"\n",
        "            )\n",
        "\n",
        "        if config_class in TOKENIZER_MAPPING._extra_content:\n",
        "            existing_slow, existing_fast = TOKENIZER_MAPPING[config_class]\n",
        "            if slow_tokenizer_class is None:\n",
        "                slow_tokenizer_class = existing_slow\n",
        "            if fast_tokenizer_class is None:\n",
        "                fast_tokenizer_class = existing_fast\n",
        "\n",
        "        TOKENIZER_MAPPING.register(config_class, (slow_tokenizer_class, fast_tokenizer_class), exist_ok=exist_ok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "9Edw1G61aw--"
      },
      "outputs": [],
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\n",
        "    \"vocab_file\": \"vocab.json\",\n",
        "    \"merges_file\": \"merges.txt\",\n",
        "}\n",
        "\n",
        "\n",
        "MAX_MODEL_INPUT_SIZES = {\"qwen/qwen-tokenizer\": 32768}\n",
        "\n",
        "PRETOKENIZE_REGEX = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "# Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n",
        "def bytes_to_unicode():\n",
        "    bs = (\n",
        "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    )\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "# Copied from transformers.models.gpt2.tokenization_gpt2.get_pairs\n",
        "def get_pairs(word):\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Qwen2Tokenizer_no(PreTrainedTokenizer):\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        merges_file,\n",
        "        errors=\"replace\",\n",
        "        unk_token=\"<|endoftext|>\",\n",
        "        bos_token=None,\n",
        "        eos_token=\"<|endoftext|>\",\n",
        "        pad_token=\"<|endoftext|>\",\n",
        "        clean_up_tokenization_spaces=False,\n",
        "        split_special_tokens=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # Qwen vocab does not contain control tokens; added tokens need to be special\n",
        "        bos_token = (\n",
        "            AddedToken(bos_token, lstrip=False, rstrip=False, special=True, normalized=False)\n",
        "            if isinstance(bos_token, str)\n",
        "            else bos_token\n",
        "        )\n",
        "        eos_token = (\n",
        "            AddedToken(eos_token, lstrip=False, rstrip=False, special=True, normalized=False)\n",
        "            if isinstance(eos_token, str)\n",
        "            else eos_token\n",
        "        )\n",
        "        unk_token = (\n",
        "            AddedToken(unk_token, lstrip=False, rstrip=False, special=True, normalized=False)\n",
        "            if isinstance(unk_token, str)\n",
        "            else unk_token\n",
        "        )\n",
        "        pad_token = (\n",
        "            AddedToken(pad_token, lstrip=False, rstrip=False, special=True, normalized=False)\n",
        "            if isinstance(pad_token, str)\n",
        "            else pad_token\n",
        "        )\n",
        "\n",
        "        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
        "            self.encoder = json.load(vocab_handle)\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors  # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        bpe_merges = []\n",
        "        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n",
        "            for i, line in enumerate(merges_handle):\n",
        "                line = line.strip()\n",
        "                if (i == 0 and line.startswith(\"#version:\")) or not line:\n",
        "                    continue\n",
        "                bpe_merges.append(tuple(line.split()))\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        # NOTE: the cache can grow without bound and will get really large for long running processes\n",
        "        # (esp. for texts of language that do not use space between word, e.g. Chinese); technically\n",
        "        # not a memory leak but appears as one.\n",
        "        # GPT2Tokenizer has the same problem, so let's be consistent.\n",
        "        self.cache = {}\n",
        "\n",
        "        self.pat = re.compile(PRETOKENIZE_REGEX)\n",
        "\n",
        "        #if kwargs.get(\"add_prefix_space\", False):\n",
        "            #logger.warning_once(\n",
        "                #f\"{self.__class__.__name} does not support `add_prefix_space`, setting it to True has no effect.\"\n",
        "            #)\n",
        "\n",
        "        super().__init__(\n",
        "            errors=errors,\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            pad_token=pad_token,\n",
        "            unk_token=unk_token,\n",
        "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
        "            split_special_tokens=split_special_tokens,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.encoder)\n",
        "\n",
        "    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.get_vocab\n",
        "    def get_vocab(self):\n",
        "        return dict(self.encoder, **self.added_tokens_encoder)\n",
        "\n",
        "    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.bpe\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                except ValueError:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "                else:\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"Tokenize a string.\"\"\"\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = \"\".join(\n",
        "                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
        "            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
        "            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
        "        return bpe_tokens\n",
        "\n",
        "    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
        "        return self.encoder.get(token, self.encoder.get(self.unk_token))\n",
        "\n",
        "    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        return self.decoder.get(index)\n",
        "\n",
        "    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
        "        text = \"\".join(tokens)\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
        "        return text\n",
        "\n",
        "    def decode(\n",
        "        self,\n",
        "        token_ids,\n",
        "        skip_special_tokens: bool = False,\n",
        "        clean_up_tokenization_spaces: Optional[bool] = False,\n",
        "        spaces_between_special_tokens: bool = False,\n",
        "        **kwargs,\n",
        "    ) -> str:\n",
        "        # `spaces_between_special_tokens` defaults to True for _decode in slow tokenizers\n",
        "        # and cannot be configured elsewhere, but it should default to False for Qwen2Tokenizer\n",
        "        return super().decode(\n",
        "            token_ids,\n",
        "            skip_special_tokens=skip_special_tokens,\n",
        "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
        "            spaces_between_special_tokens=spaces_between_special_tokens,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.save_vocabulary\n",
        "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n",
        "        if not os.path.isdir(save_directory):\n",
        "            #logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n",
        "            return\n",
        "        vocab_file = os.path.join(\n",
        "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
        "        )\n",
        "        merge_file = os.path.join(\n",
        "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n",
        "        )\n",
        "\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        index = 0\n",
        "        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            writer.write(\"#version: 0.2\\n\")\n",
        "            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    #logger.warning(\n",
        "                        #f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n",
        "                        #\" Please check that the tokenizer is not corrupted!\"\n",
        "                    #)\n",
        "                    index = token_index\n",
        "                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return vocab_file, merge_file\n",
        "\n",
        "    def prepare_for_tokenization(self, text, **kwargs):\n",
        "        text = unicodedata.normalize(\"NFC\", text)\n",
        "        return (text, kwargs)\n",
        "\n",
        "class Qwen2Tokenizer(PreTrainedTokenizer):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "OujfLwNeDzD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "oBE3rOCFvo_V"
      },
      "outputs": [],
      "source": [
        "DEPRECATED_MODELS = []\n",
        "SPECIAL_MODEL_TYPE_TO_MODULE_NAME = []\n",
        "\"\"\"\n",
        "DEPRECATED_MODELS = [\n",
        "    \"bort\",\n",
        "    \"deta\",\n",
        "    \"efficientformer\",\n",
        "    \"ernie_m\",\n",
        "    \"gptsan_japanese\",\n",
        "    \"graphormer\",\n",
        "    \"jukebox\",\n",
        "    \"mctct\",\n",
        "    \"mega\",\n",
        "    \"mmbt\",\n",
        "    \"nat\",\n",
        "    \"nezha\",\n",
        "    \"open_llama\",\n",
        "    \"qdqbert\",\n",
        "    \"realm\",\n",
        "    \"retribert\",\n",
        "    \"speech_to_text_2\",\n",
        "    \"tapex\",\n",
        "    \"trajectory_transformer\",\n",
        "    \"transfo_xl\",\n",
        "    \"tvlt\",\n",
        "    \"van\",\n",
        "    \"vit_hybrid\",\n",
        "    \"xlm_prophetnet\",\n",
        "]\n",
        "\n",
        "SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict[str, str](\n",
        "    [\n",
        "        (\"openai-gpt\", \"openai\"),\n",
        "        (\"data2vec-audio\", \"data2vec\"),\n",
        "        (\"data2vec-text\", \"data2vec\"),\n",
        "        (\"data2vec-vision\", \"data2vec\"),\n",
        "        (\"donut-swin\", \"donut\"),\n",
        "        (\"kosmos-2\", \"kosmos2\"),\n",
        "        (\"maskformer-swin\", \"maskformer\"),\n",
        "        (\"xclip\", \"x_clip\"),\n",
        "        (\"clip_vision_model\", \"clip\"),\n",
        "        (\"qwen2_audio_encoder\", \"qwen2_audio\"),\n",
        "        (\"clip_text_model\", \"clip\"),\n",
        "        (\"aria_text\", \"aria\"),\n",
        "        (\"gemma3_text\", \"gemma3\"),\n",
        "        (\"gemma3n_audio\", \"gemma3n\"),\n",
        "        (\"gemma3n_text\", \"gemma3n\"),\n",
        "        (\"gemma3n_vision\", \"gemma3n\"),\n",
        "        (\"glm4v_text\", \"glm4v\"),\n",
        "        (\"idefics3_vision\", \"idefics3\"),\n",
        "        (\"siglip_vision_model\", \"siglip\"),\n",
        "        (\"aimv2_vision_model\", \"aimv2\"),\n",
        "        (\"smolvlm_vision\", \"smolvlm\"),\n",
        "        (\"chinese_clip_vision_model\", \"chinese_clip\"),\n",
        "        (\"rt_detr_resnet\", \"rt_detr\"),\n",
        "        (\"granitevision\", \"llava_next\"),\n",
        "        (\"internvl_vision\", \"internvl\"),\n",
        "        (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n",
        "        (\"qwen2_vl_text\", \"qwen2_vl\"),\n",
        "        (\"sam_vision_model\", \"sam\"),\n",
        "        (\"sam_hq_vision_model\", \"sam_hq\"),\n",
        "        (\"llama4_text\", \"llama4\"),\n",
        "        (\"blip_2_qformer\", \"blip_2\"),\n",
        "        (\"fastspeech2_conformer_with_hifigan\", \"fastspeech2_conformer\"),\n",
        "        (\"perception_encoder\", \"perception_lm\"),\n",
        "    ]\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "def model_type_to_module_name(key) -> str:\n",
        "    if key in SPECIAL_MODEL_TYPE_TO_MODULE_NAME:\n",
        "        key = SPECIAL_MODEL_TYPE_TO_MODULE_NAME[key]\n",
        "\n",
        "        if key in DEPRECATED_MODELS:\n",
        "            key = f\"deprecated.{key}\"\n",
        "        return key\n",
        "\n",
        "    key = key.replace(\"-\", \"_\")\n",
        "    if key in DEPRECATED_MODELS:\n",
        "        key = f\"deprecated.{key}\"\n",
        "\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers.utils.quantization_config import QuantizationConfigMixin\n",
        "class HfQuantizer(ABC):\n",
        "    requires_calibration = False\n",
        "    required_packages = None\n",
        "    requires_parameters_quantization = False\n",
        "\n",
        "    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n",
        "        self.quantization_config = quantization_config\n",
        "\n",
        "        # -- Handle extra kwargs below --\n",
        "        self.modules_to_not_convert = kwargs.pop(\"modules_to_not_convert\", [])\n",
        "        self.pre_quantized = kwargs.pop(\"pre_quantized\", True)\n",
        "\n",
        "        if not self.pre_quantized and self.requires_calibration:\n",
        "            raise ValueError(\n",
        "                f\"The quantization method {quantization_config.quant_method} does require the model to be pre-quantized.\"\n",
        "                f\" You explicitly passed `pre_quantized=False` meaning your model weights are not quantized. Make sure to \"\n",
        "                f\"pass `pre_quantized=True` while knowing what you are doing.\"\n",
        "            )\n",
        "\n",
        "    def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n",
        "        return torch_dtype\n",
        "\n",
        "    def update_device_map(self, device_map: Optional[dict[str, Any]]) -> Optional[dict[str, Any]]:\n",
        "        return device_map\n",
        "\n",
        "    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n",
        "        return missing_keys\n",
        "\n",
        "    def update_unexpected_keys(self, model, unexpected_keys: list[str], prefix: str) -> list[str]:\n",
        "        return unexpected_keys\n",
        "\n",
        "    def update_missing_keys_after_loading(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n",
        "        return missing_keys\n",
        "\n",
        "    def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: list[str]) -> list[str]:\n",
        "        return expected_keys\n",
        "\n",
        "    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n",
        "        return max_memory\n",
        "\n",
        "    def check_quantized_param(\n",
        "        self,\n",
        "        model: \"PreTrainedModel\",\n",
        "        param_value: \"torch.Tensor\",\n",
        "        param_name: str,\n",
        "        state_dict: dict[str, Any],\n",
        "        **kwargs,\n",
        "    ) -> bool:\n",
        "        return False\n",
        "\n",
        "    def create_quantized_param(self, *args, **kwargs) -> \"torch.nn.Parameter\":\n",
        "        if not self.requires_parameters_quantization:\n",
        "            raise AttributeError(\n",
        "                f\"`.create_quantized_param()` method is not supported by quantizer class {self.__class__.__name__}.\"\n",
        "            )\n",
        "\n",
        "    def validate_environment(self, *args, **kwargs):\n",
        "        return\n",
        "\n",
        "    def update_tp_plan(self, config):\n",
        "        return config\n",
        "\n",
        "    def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n",
        "        model.is_quantized = True\n",
        "        model.quantization_method = self.quantization_config.quant_method\n",
        "        if self.pre_quantized:\n",
        "            self._convert_model_for_quantization(model)\n",
        "        return self._process_model_before_weight_loading(model, **kwargs)\n",
        "\n",
        "    def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n",
        "        return self._process_model_after_weight_loading(model, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_modules_to_not_convert(\n",
        "        model: \"PreTrainedModel\",\n",
        "        skip_modules: Optional[list[str]] = None,\n",
        "        keep_in_fp32_modules: Optional[list[str]] = None,\n",
        "        add_default_skips: bool = False,\n",
        "    ):\n",
        "        from ..integrations import get_keys_to_not_convert\n",
        "\n",
        "        if skip_modules is None or add_default_skips:\n",
        "            modules_to_not_convert = get_keys_to_not_convert(model)\n",
        "        else:\n",
        "            modules_to_not_convert = []\n",
        "\n",
        "        if skip_modules is not None:\n",
        "            modules_to_not_convert.extend(skip_modules)\n",
        "\n",
        "        if keep_in_fp32_modules is not None:\n",
        "            modules_to_not_convert.extend(keep_in_fp32_modules)\n",
        "\n",
        "        return modules_to_not_convert\n",
        "\n",
        "\n",
        "    @property\n",
        "    def is_compileable(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @abstractmethod\n",
        "    def _process_model_before_weight_loading(self, model, **kwargs): ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def _process_model_after_weight_loading(self, model, **kwargs): ...\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def is_trainable(self): ...\n",
        "\n",
        "    def _convert_model_for_quantization(self, model):\n",
        "        from accelerate import init_empty_weights\n",
        "\n",
        "        for name, module in model.named_modules():\n",
        "            module_class_name = module.__class__.__name__\n",
        "            if module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION.keys() and (\n",
        "                self.quantization_config.quant_method\n",
        "                in MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"quantization_methods\"]\n",
        "            ):\n",
        "                with init_empty_weights():\n",
        "                    parent_module, name = get_module_from_name(model, name)\n",
        "                    parent_module._modules[name] = MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"module_name\"](\n",
        "                        model.config.get_text_config()\n",
        "                    )"
      ],
      "metadata": {
        "id": "MGHBSGwJsghA"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_module_from_name(module, tensor_name: str) -> tuple[Any, str]:\n",
        "    if \".\" in tensor_name:\n",
        "        module_name, tensor_name = tensor_name.rsplit(\".\", 1)\n",
        "        module = module.get_submodule(module_name)\n",
        "    return module, tensor_name\n",
        "\n",
        "def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor: torch.Tensor):\n",
        "    module, param_type = get_module_from_name(model, param_name)\n",
        "    module.load_state_dict({param_type: tensor}, strict=False, assign=True)\n",
        "\n",
        "def _infer_parameter_dtype(\n",
        "    model: \"PreTrainedModel\",\n",
        "    param_name: str,\n",
        "    empty_param: torch.Tensor,\n",
        "    keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "    hf_quantizer: Optional[HfQuantizer] = None,\n",
        ") -> Union[bool, Optional[torch.dtype]]:\n",
        "    try:\n",
        "        old_param = model.get_parameter_or_buffer(param_name)\n",
        "    except Exception as e:\n",
        "        if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method in {\n",
        "            QuantizationMethod.HQQ,\n",
        "            QuantizationMethod.QUARK,\n",
        "        }:\n",
        "            return True, None\n",
        "        else:\n",
        "            raise e\n",
        "    is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n",
        "    casting_dtype = None\n",
        "    is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n",
        "    if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n",
        "        if keep_in_fp32_regex is not None and keep_in_fp32_regex.search(param_name):\n",
        "            casting_dtype = torch.float32\n",
        "        elif hf_quantizer is not None:\n",
        "            casting_dtype = model.config._pre_quantization_dtype\n",
        "        else:\n",
        "            casting_dtype = old_param.dtype\n",
        "    return old_param is not None and old_param.is_contiguous(), casting_dtype\n",
        "\n",
        "@torch.no_grad()\n",
        "def _load_state_dict_into_meta_model(\n",
        "    model: \"PreTrainedModel\",\n",
        "    state_dict: dict,\n",
        "    shard_file: str,\n",
        "    expected_keys: list[str],\n",
        "    reverse_renaming_mapping: dict[str, str],\n",
        "    device_map: Optional[dict] = None,\n",
        "    disk_offload_folder: Optional[str] = None,\n",
        "    disk_offload_index: Optional[dict] = None,\n",
        "    cpu_offload_folder: Optional[str] = None,\n",
        "    cpu_offload_index: Optional[dict] = None,\n",
        "    hf_quantizer: Optional[HfQuantizer] = None,\n",
        "    is_safetensors: bool = False,\n",
        "    keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "    unexpected_keys: Optional[list[str]] = None,  # passing `unexpected` for cleanup from quantization items\n",
        "    device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n",
        ") -> tuple[Optional[dict], Optional[dict]]:\n",
        "    tensor_device = \"cpu\"\n",
        "    if device_map is not None and device_map.get(\"\", None) is not None:\n",
        "        if device_map[\"\"] not in (\"cpu\", torch.device(\"cpu\")):\n",
        "            tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n",
        "    if device_map is not None:\n",
        "        device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n",
        "\n",
        "    is_quantized = hf_quantizer is not None\n",
        "    is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "        QuantizationMethod.HQQ,\n",
        "        QuantizationMethod.BITS_AND_BYTES,\n",
        "    }\n",
        "    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb\n",
        "    file_pointer = None\n",
        "    if is_meta_state_dict:\n",
        "        file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n",
        "\n",
        "    for param_name, empty_param in state_dict.items():\n",
        "        if param_name not in expected_keys:\n",
        "            continue\n",
        "\n",
        "        # we need to use serialized_param_name as file pointer is untouched\n",
        "        if is_meta_state_dict:\n",
        "            # This is the name of the parameter as it appears on disk file\n",
        "            serialized_param_name = reverse_renaming_mapping[param_name]\n",
        "            param = file_pointer.get_slice(serialized_param_name)\n",
        "        else:\n",
        "            param = empty_param.to(tensor_device)  # It is actually not empty!\n",
        "\n",
        "        to_contiguous, casting_dtype = _infer_parameter_dtype(\n",
        "            model,\n",
        "            param_name,\n",
        "            empty_param,\n",
        "            keep_in_fp32_regex,\n",
        "            hf_quantizer,\n",
        "        )\n",
        "\n",
        "        if device_mesh is not None:  # In this case, the param is already on the correct device!\n",
        "            shard_and_distribute_module(\n",
        "                model,\n",
        "                param,\n",
        "                empty_param,\n",
        "                param_name,\n",
        "                casting_dtype,\n",
        "                to_contiguous,\n",
        "                device_mesh.get_local_rank(),\n",
        "                device_mesh,\n",
        "            )\n",
        "        else:\n",
        "            param = param[...]\n",
        "            if casting_dtype is not None:\n",
        "                param = param.to(casting_dtype)\n",
        "            if to_contiguous:\n",
        "                param = param.contiguous()\n",
        "\n",
        "            if device_map is None:\n",
        "                param_device = \"cpu\"\n",
        "            else:\n",
        "                module_layer = re.search(device_map_regex, param_name)\n",
        "                if not module_layer:\n",
        "                    raise ValueError(f\"{param_name} doesn't have any device set.\")\n",
        "                else:\n",
        "                    param_device = device_map[module_layer.group()]\n",
        "\n",
        "            if param_device == \"disk\":\n",
        "                if not is_safetensors:\n",
        "                    disk_offload_index = offload_weight(param, param_name, disk_offload_folder, disk_offload_index)\n",
        "            elif param_device == \"cpu\" and cpu_offload_index is not None:\n",
        "                cpu_offload_index = offload_weight(param, param_name, cpu_offload_folder, cpu_offload_index)\n",
        "            elif (\n",
        "                not is_quantized\n",
        "                or (not hf_quantizer.requires_parameters_quantization)\n",
        "                or (\n",
        "                    not hf_quantizer.check_quantized_param(\n",
        "                        model,\n",
        "                        param,\n",
        "                        param_name,\n",
        "                        state_dict,\n",
        "                        param_device=param_device,\n",
        "                        device_map=device_map,\n",
        "                    )\n",
        "                )\n",
        "            ):\n",
        "                if is_fsdp_enabled():\n",
        "                    param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n",
        "\n",
        "                _load_parameter_into_model(model, param_name, param.to(param_device))\n",
        "\n",
        "            else:\n",
        "                hf_quantizer.create_quantized_param(\n",
        "                    model, param, param_name, param_device, state_dict, unexpected_keys\n",
        "                )\n",
        "                if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n",
        "                    module, param_type = get_module_from_name(model, param_name)\n",
        "                    value = getattr(module, param_type)\n",
        "                    param_to = \"cpu\"\n",
        "                    if is_fsdp_enabled() and not is_local_dist_rank_0():\n",
        "                        param_to = \"meta\"\n",
        "                    val_kwargs = {}\n",
        "                    if hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\":\n",
        "                        val_kwargs[\"requires_grad\"] = False\n",
        "                    value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n",
        "                    setattr(module, param_type, value)\n",
        "\n",
        "    if file_pointer is not None:\n",
        "        file_pointer.__exit__(None, None, None)\n",
        "\n",
        "    return disk_offload_index, cpu_offload_index"
      ],
      "metadata": {
        "id": "cweMN5RLEz8n"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "_iD0At3RRAJ_"
      },
      "outputs": [],
      "source": [
        "class Cache:\n",
        "    is_compileable = False\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        key_states: torch.Tensor,\n",
        "        value_states: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "        cache_kwargs: Optional[dict[str, Any]] = None,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        raise NotImplementedError(\"Make sure to implement `update` in a subclass.\")\n",
        "\n",
        "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
        "        raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n",
        "\n",
        "    def get_max_cache_shape(self) -> Optional[int]:\n",
        "        raise NotImplementedError(\"Make sure to implement `get_max_cache_shape` in a subclass.\")\n",
        "\n",
        "    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n",
        "        max_length = self.get_max_cache_shape()\n",
        "        previous_seq_length = self.get_seq_length(layer_idx)\n",
        "        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n",
        "            return max_length - new_seq_length\n",
        "        return previous_seq_length\n",
        "\n",
        "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
        "        for layer_idx in range(len(self.key_cache)):\n",
        "            if self.key_cache[layer_idx].numel():\n",
        "                device = self.key_cache[layer_idx].device\n",
        "                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
        "            if self.value_cache[layer_idx].numel():\n",
        "                device = self.value_cache[layer_idx].device\n",
        "                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
        "\n",
        "    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n",
        "        query_length = cache_position.shape[0]\n",
        "        past_seen_tokens = self.get_seq_length()\n",
        "        kv_length = query_length + past_seen_tokens\n",
        "        return kv_length, 0\n",
        "\n",
        "class DynamicCache(Cache):\n",
        "    def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n",
        "        super().__init__()\n",
        "        self.key_cache: list[torch.Tensor] = []\n",
        "        self.value_cache: list[torch.Tensor] = []\n",
        "\n",
        "        if _distributed_cache_data is not None:\n",
        "            for key_states, value_states in _distributed_cache_data:\n",
        "                self.key_cache.append(key_states)\n",
        "                self.value_cache.append(value_states)\n",
        "\n",
        "    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        if layer_idx < len(self):\n",
        "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
        "        else:\n",
        "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        for layer_idx in range(len(self)):\n",
        "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.key_cache)\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        key_states: torch.Tensor,\n",
        "        value_states: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "        cache_kwargs: Optional[dict[str, Any]] = None,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        if key_states is not None:\n",
        "            if len(self.key_cache) <= layer_idx:\n",
        "                for _ in range(len(self.key_cache), layer_idx):\n",
        "                    self.key_cache.append(torch.tensor([]))\n",
        "                    self.value_cache.append(torch.tensor([]))\n",
        "                self.key_cache.append(key_states)\n",
        "                self.value_cache.append(value_states)\n",
        "            elif (\n",
        "                not self.key_cache[layer_idx].numel()\n",
        "            ):\n",
        "                self.key_cache[layer_idx] = key_states\n",
        "                self.value_cache[layer_idx] = value_states\n",
        "            else:\n",
        "                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
        "                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
        "\n",
        "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
        "\n",
        "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
        "        is_empty_layer = (\n",
        "            len(self.key_cache) == 0  # no cache in any layer\n",
        "            or len(self.key_cache) <= layer_idx  # skipped `layer_idx` and hasn't run a layer with cache after it\n",
        "            or not self.key_cache[layer_idx].numel()  # the layer has no cache\n",
        "        )\n",
        "        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n",
        "        return layer_seq_length\n",
        "\n",
        "    def get_max_cache_shape(self) -> Optional[int]:\n",
        "        return None\n",
        "\n",
        "    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n",
        "        legacy_cache = ()\n",
        "        for layer_idx in range(len(self)):\n",
        "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
        "        return legacy_cache\n",
        "\n",
        "    @classmethod\n",
        "    def from_legacy_cache(\n",
        "        cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor, torch.FloatTensor]]] = None\n",
        "    ) -> \"DynamicCache\":\n",
        "        cache = cls()\n",
        "        if past_key_values is not None:\n",
        "            for layer_idx in range(len(past_key_values)):\n",
        "                key_states, value_states = past_key_values[layer_idx]\n",
        "                cache.update(key_states, value_states, layer_idx)\n",
        "        return cache\n",
        "\n",
        "    def crop(self, max_length: int):\n",
        "        if max_length < 0:\n",
        "            max_length = self.get_seq_length() - abs(max_length)\n",
        "\n",
        "        if self.get_seq_length() <= max_length:\n",
        "            return\n",
        "\n",
        "        for idx in range(len(self.key_cache)):\n",
        "            if self.key_cache[idx].numel():\n",
        "                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n",
        "                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n",
        "\n",
        "    def batch_split(self, full_batch_size: int, split_size: int) -> list[\"DynamicCache\"]:\n",
        "        out = []\n",
        "        for i in range(0, full_batch_size, split_size):\n",
        "            current_split = DynamicCache()\n",
        "            current_split.key_cache = [tensor[i : i + split_size] for tensor in self.key_cache]\n",
        "            current_split.value_cache = [tensor[i : i + split_size] for tensor in self.value_cache]\n",
        "            out.append(current_split)\n",
        "        return out\n",
        "\n",
        "    @classmethod\n",
        "    def from_batch_splits(cls, splits: list[\"DynamicCache\"]) -> \"DynamicCache\":\n",
        "        cache = cls()\n",
        "        for idx in range(len(splits[0])):\n",
        "            key_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx].numel()]\n",
        "            value_cache = [current.value_cache[idx] for current in splits if current.value_cache[idx].numel()]\n",
        "            if key_cache != []:\n",
        "                layer_keys = torch.cat(key_cache, dim=0)\n",
        "                layer_values = torch.cat(value_cache, dim=0)\n",
        "                cache.update(layer_keys, layer_values, idx)\n",
        "        return cache\n",
        "\n",
        "    def batch_repeat_interleave(self, repeats: int):\n",
        "        for layer_idx in range(len(self)):\n",
        "            self.key_cache[layer_idx] = self.key_cache[layer_idx].repeat_interleave(repeats, dim=0)\n",
        "            self.value_cache[layer_idx] = self.value_cache[layer_idx].repeat_interleave(repeats, dim=0)\n",
        "\n",
        "    def batch_select_indices(self, indices: torch.Tensor):\n",
        "        for layer_idx in range(len(self)):\n",
        "            self.key_cache[layer_idx] = self.key_cache[layer_idx][indices, ...]\n",
        "            self.value_cache[layer_idx] = self.value_cache[layer_idx][indices, ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "JyA21IWgn2gJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd19d32-dda6-46fb-9eff-bd178a7ffb1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected torch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class GeneralInterface(MutableMapping):\n",
        "    _global_mapping = {}\n",
        "\n",
        "    def __init__(self):\n",
        "        self._local_mapping = {}\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if key in self._local_mapping:\n",
        "            return self._local_mapping[key]\n",
        "        return self._global_mapping[key]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self._local_mapping.update({key: value})\n",
        "\n",
        "    def __delitem__(self, key):\n",
        "        del self._local_mapping[key]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter({**self._global_mapping, **self._local_mapping})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._global_mapping.keys() | self._local_mapping.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def register(cls, key: str, value: Callable):\n",
        "        cls._global_mapping.update({key: value})\n",
        "\n",
        "    @classmethod\n",
        "    def valid_keys(cls) -> list[str]:\n",
        "        return list(cls._global_mapping.keys())\n",
        "\n",
        "def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n",
        "    if not _is_package_available(\"torch\"):\n",
        "        return False\n",
        "\n",
        "    if accept_dev:\n",
        "        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n",
        "            library_version\n",
        "        )\n",
        "    else:\n",
        "        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n",
        "\n",
        "_is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)\n",
        "\n",
        "def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n",
        "    return kv_idx <= q_idx\n",
        "\n",
        "def prepare_padding_mask(\n",
        "    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int, _slice: bool = True\n",
        ") -> Optional[torch.Tensor]:\n",
        "    local_padding_mask = attention_mask\n",
        "    if attention_mask is not None:\n",
        "        if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
        "            local_padding_mask = torch.nn.functional.pad(attention_mask, (0, padding_length))\n",
        "        if _slice:\n",
        "            mask_indices = torch.arange(kv_length, device=local_padding_mask.device)\n",
        "            mask_indices += kv_offset\n",
        "            local_padding_mask = local_padding_mask[:, mask_indices]\n",
        "    return local_padding_mask\n",
        "\n",
        "def is_torchdynamo_compiling():\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "\n",
        "        return torch.compiler.is_compiling()\n",
        "    except Exception:\n",
        "        try:\n",
        "            import torch._dynamo as dynamo  # noqa: F401\n",
        "\n",
        "            return dynamo.is_compiling()\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "\n",
        "def _ignore_causal_mask_sdpa(\n",
        "    padding_mask: Optional[torch.Tensor],\n",
        "    query_length: int,\n",
        "    kv_length: int,\n",
        "    kv_offset: int,\n",
        "    local_attention_size: Optional[int] = None,\n",
        ") -> bool:\n",
        "    is_tracing = torch.jit.is_tracing() or isinstance(padding_mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n",
        "    if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n",
        "        mask_indices = torch.arange(kv_length, device=padding_mask.device)\n",
        "        mask_indices += kv_offset\n",
        "        padding_mask = padding_mask[:, mask_indices]\n",
        "\n",
        "    if (\n",
        "        not is_tracing\n",
        "        and (query_length == 1 or kv_length == query_length)\n",
        "        and (local_attention_size is None or kv_length < local_attention_size)\n",
        "        and (padding_mask is None or padding_mask.all())\n",
        "    ):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def sdpa_mask_recent_torch(\n",
        "    batch_size: int,\n",
        "    cache_position: torch.Tensor,\n",
        "    kv_length: int,\n",
        "    kv_offset: int = 0,\n",
        "    mask_function: Callable = causal_mask_function,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    local_size: Optional[int] = None,\n",
        "    allow_is_causal_skip: bool = True,\n",
        "    **kwargs,\n",
        ") -> Optional[torch.Tensor]:\n",
        "    q_length = cache_position.shape[0]\n",
        "    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n",
        "\n",
        "    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n",
        "        return None\n",
        "\n",
        "    kv_arange = torch.arange(kv_length, device=cache_position.device)\n",
        "    kv_arange += kv_offset\n",
        "\n",
        "    if padding_mask is not None:\n",
        "        mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n",
        "\n",
        "    batch_arange = torch.arange(batch_size, device=cache_position.device)\n",
        "    head_arange = torch.arange(1, device=cache_position.device)\n",
        "    with TransformGetItemToIndex():\n",
        "        causal_mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n",
        "\n",
        "    return causal_mask\n",
        "\n",
        "sdpa_mask = sdpa_mask_recent_torch if _is_torch_greater_or_equal_than_2_6 else sdpa_mask_older_torch\n",
        "\n",
        "class AttentionMaskInterface(GeneralInterface):\n",
        "    _global_mapping = {\n",
        "        \"sdpa\": sdpa_mask,\n",
        "    }\n",
        "\n",
        "def _preprocess_mask_arguments(\n",
        "    config: PretrainedConfig,\n",
        "    input_embeds: torch.Tensor,\n",
        "    attention_mask: Optional[Union[torch.Tensor, torch.Tensor]],\n",
        "    cache_position: torch.Tensor,\n",
        "    past_key_values: Optional[Cache],\n",
        "    position_ids: Optional[torch.Tensor],\n",
        "    layer_idx: Optional[int],\n",
        ") -> tuple[bool, Optional[Union[torch.Tensor, torch.Tensor]], int, int]:\n",
        "\n",
        "    if isinstance(attention_mask, (torch.Tensor, torch.Tensor)) and len(attention_mask.shape) == 4:\n",
        "        return True, attention_mask, None, None, None\n",
        "\n",
        "    if config._attn_implementation not in AttentionMaskInterface._global_mapping:\n",
        "        return True, None, None, None, None\n",
        "\n",
        "    if attention_mask is not None and attention_mask.ndim == 2:\n",
        "        attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n",
        "\n",
        "    if past_key_values is not None:\n",
        "        kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n",
        "    else:\n",
        "        kv_length, kv_offset = input_embeds.shape[1], 0\n",
        "\n",
        "    packed_sequence_mask = None\n",
        "    if position_ids is not None and attention_mask is None and past_key_values is None:\n",
        "        batch_size = input_embeds.shape[0]\n",
        "        if batch_size != position_ids.shape[0]:\n",
        "            position_ids = position_ids.expand(batch_size, -1)\n",
        "        packed_sequence_mask = find_packed_sequence_indices(position_ids)\n",
        "\n",
        "    return False, attention_mask, packed_sequence_mask, kv_length, kv_offset\n",
        "\n",
        "def create_causal_mask(\n",
        "    config: PretrainedConfig,\n",
        "    input_embeds: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    cache_position: torch.Tensor,\n",
        "    past_key_values: Optional[Cache],\n",
        "    position_ids: Optional[torch.Tensor] = None,\n",
        "    or_mask_function: Optional[Callable] = None,\n",
        "    and_mask_function: Optional[Callable] = None,\n",
        ") -> Optional[Union[torch.Tensor, torch.Tensor]]:\n",
        "    if hasattr(past_key_values, \"is_sliding\") and False in past_key_values.is_sliding:\n",
        "        layer_idx = past_key_values.is_sliding.index(False)\n",
        "    else:\n",
        "        layer_idx = 0\n",
        "\n",
        "    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n",
        "        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n",
        "    )\n",
        "    if early_exit:\n",
        "        return attention_mask\n",
        "\n",
        "    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n",
        "    mask_factory_function = causal_mask_function\n",
        "    mask_interface = sdpa_mask\n",
        "\n",
        "    allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n",
        "\n",
        "    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n",
        "        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n",
        "        allow_is_causal_skip = False\n",
        "\n",
        "    if or_mask_function is not None:\n",
        "        if not _is_torch_greater_or_equal_than_2_6:\n",
        "            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n",
        "        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n",
        "        allow_is_causal_skip = False\n",
        "    if and_mask_function is not None:\n",
        "        if not _is_torch_greater_or_equal_than_2_6:\n",
        "            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n",
        "        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n",
        "        allow_is_causal_skip = False\n",
        "\n",
        "    causal_mask = mask_interface(\n",
        "        batch_size=batch_size,\n",
        "        cache_position=cache_position,\n",
        "        kv_length=kv_length,\n",
        "        kv_offset=kv_offset,\n",
        "        mask_function=mask_factory_function,\n",
        "        attention_mask=attention_mask,\n",
        "        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n",
        "        dtype=dtype,  # Additional kwarg for eager\n",
        "        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n",
        "    )\n",
        "    return causal_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "G-3ixmlD9DDz"
      },
      "outputs": [],
      "source": [
        "class RowwiseParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "        use_dtensor=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Shard(-1),)\n",
        "        self.output_layouts = (output_layouts or Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        if param_type != \"bias\":\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n",
        "            shard = [Shard(-1)]\n",
        "        else:\n",
        "            shard = [Replicate()]\n",
        "            parameter = param[:]\n",
        "\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(\n",
        "                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n",
        "            )\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
        "            mod._bias = mod.bias\n",
        "            mod.bias = None\n",
        "\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        if outputs.placements != output_layouts:\n",
        "            outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n",
        "        if hasattr(mod, \"_bias\"):\n",
        "            outputs += mod._bias\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        module._distribute_module_applied = True\n",
        "        if self.use_dtensor:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                self.desired_input_layouts: tuple[Placement, ...] = (Shard(-1),)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                self.desired_input_layouts = (Replicate(),)\n",
        "            elif isinstance(module, nn.Parameter):\n",
        "                self.desired_input_layouts = (Shard(-1),)\n",
        "            else:\n",
        "                raise NotImplementedError(\"RowwiseParallel currently only support nn.Linear and nn.Embedding!\")\n",
        "\n",
        "            distribute_module(\n",
        "                module,\n",
        "                device_mesh,\n",
        "                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n",
        "                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n",
        "            )\n",
        "\n",
        "class IsolatedParallel(TensorParallelLayer):\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh=None):\n",
        "        input_tensor = inputs[0]\n",
        "        if isinstance(input_tensor, DTensor):\n",
        "            input_tensor = input_tensor.to_local()\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh=None):\n",
        "        return outputs\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        distribute_module(\n",
        "            module,\n",
        "            device_mesh,\n",
        "            partial(self._prepare_input_fn, None, None),\n",
        "            partial(self._prepare_output_fn, None, None),\n",
        "        )\n",
        "\n",
        "class GatherParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Replicate(),)\n",
        "        self.output_layouts = output_layouts\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        if inputs and isinstance(inputs[0], DTensor):\n",
        "            inputs = inputs[0].to_local()\n",
        "        return inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        torch.distributed.all_reduce(outputs[0], op=torch.distributed.ReduceOp.SUM, async_op=False)\n",
        "        return outputs\n",
        "\n",
        "class PackedRowwiseParallel(RowwiseParallel):\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -1)\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-1)], run_check=False)\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "class SequenceParallel(TensorParallelLayer):\n",
        "    def __init__(self, *, sequence_dim: int = 1, use_local_output: bool = False, use_dtensor=False):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (Replicate(),)\n",
        "        self.desired_input_layouts = (Shard(1),)\n",
        "        self.output_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = True\n",
        "        self.sequence_sharding = (Shard(sequence_dim),)\n",
        "        self.use_local_output = use_local_output\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        outputs = outputs.redistribute(\n",
        "            placements=(Replicate(),), async_op=True\n",
        "        )  # maybe we have to replicate ? because next layer is not sharded\n",
        "        return outputs.to_local()  # if use_local_output else outputs\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        parameter = param[...]\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(parameter, device_mesh, [Replicate()], run_check=False)\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "class ReplicateParallel(TensorParallelLayer):\n",
        "    def __init__(self, *, use_dtensor=True, use_local_output=True):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (Replicate(),)\n",
        "        self.output_layouts = (Replicate(),)\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        param = param[...].to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            param = param.contiguous()\n",
        "        param = DTensor.from_local(param, device_mesh, [Replicate()], run_check=False)\n",
        "        return param"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_torch_distributed_available = torch.distributed.is_available()\n",
        "\n",
        "str_to_torch_dtype = {\n",
        "    \"BOOL\": torch.bool,\n",
        "    \"U8\": torch.uint8,\n",
        "    \"I8\": torch.int8,\n",
        "    \"I16\": torch.int16,\n",
        "    \"F16\": torch.float16,\n",
        "    \"BF16\": torch.bfloat16,\n",
        "    \"I32\": torch.int32,\n",
        "    \"F32\": torch.float32,\n",
        "    \"F64\": torch.float64,\n",
        "    \"I64\": torch.int64,\n",
        "    \"F8_E4M3\": torch.float8_e4m3fn,\n",
        "    \"F8_E5M2\": torch.float8_e5m2,\n",
        "}\n",
        "\n",
        "def _find_missing_and_unexpected_keys(\n",
        "    cls,\n",
        "    model: \"PreTrainedModel\",\n",
        "    original_checkpoint_keys: list[str],\n",
        "    checkpoint_keys: list[str],\n",
        "    loading_base_model_from_task_state_dict: bool,\n",
        "    hf_quantizer: Optional[HfQuantizer],\n",
        "    device_map: dict,\n",
        ") -> tuple[list[str], list[str]]:\n",
        "    prefix = model.base_model_prefix\n",
        "\n",
        "    # Compute expected keys, i.e. keys that the FULL model (not model_to_load) expects\n",
        "    expected_keys = list(model.state_dict().keys())\n",
        "    if hf_quantizer is not None:\n",
        "        expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n",
        "\n",
        "    missing_keys = sorted(set(expected_keys) - set(checkpoint_keys))\n",
        "    unexpected_keys = set(checkpoint_keys) - set(expected_keys)\n",
        "\n",
        "    if loading_base_model_from_task_state_dict:\n",
        "        task_specific_keys = [k for k in original_checkpoint_keys if not k.startswith(f\"{prefix}.\")]\n",
        "        unexpected_keys.update(task_specific_keys)\n",
        "\n",
        "    model_buffers = {n for n, _ in model.named_buffers()}\n",
        "    unexpected_keys = sorted(unexpected_keys - model_buffers)\n",
        "    has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer in model_buffers)\n",
        "    if has_inv_freq_buffers:\n",
        "        unexpected_keys = [k for k in unexpected_keys if \"rotary_emb.inv_freq\" not in k]\n",
        "\n",
        "    tied_params = find_tied_parameters(model)\n",
        "    for group in tied_params:\n",
        "        missing_in_group = [k for k in missing_keys if k in group]\n",
        "        if len(missing_in_group) > 0 and len(missing_in_group) < len(group):\n",
        "            missing_keys = [k for k in missing_keys if k not in missing_in_group]\n",
        "\n",
        "    if hf_quantizer is not None:\n",
        "        missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n",
        "        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n",
        "    if cls._keys_to_ignore_on_load_missing is not None:\n",
        "        for pattern in cls._keys_to_ignore_on_load_missing:\n",
        "            missing_keys = [k for k in missing_keys if re.search(pattern, k) is None]\n",
        "\n",
        "    if cls._keys_to_ignore_on_load_unexpected is not None:\n",
        "        for pattern in cls._keys_to_ignore_on_load_unexpected:\n",
        "            unexpected_keys = [k for k in unexpected_keys if re.search(pattern, k) is None]\n",
        "\n",
        "    return missing_keys, unexpected_keys\n",
        "\n",
        "def _find_mismatched_keys(\n",
        "    model: \"PreTrainedModel\",\n",
        "    state_dict: Optional[dict],\n",
        "    checkpoint_files: Optional[list[str]],\n",
        "    ignore_mismatched_sizes: bool,\n",
        "    keys_to_rename_mapping: dict[str, str],\n",
        "    is_quantized: bool,\n",
        "    weights_only: bool,\n",
        ") -> tuple[list[str], list[tuple[int, int]]]:\n",
        "    if not ignore_mismatched_sizes:\n",
        "        return [], []\n",
        "\n",
        "    if state_dict is not None:\n",
        "        checkpoint_files = [\"\"]\n",
        "\n",
        "    model_state_dict = model.state_dict()\n",
        "    mismatched_keys = []\n",
        "    mismatched_shapes = []\n",
        "    for shard_file in checkpoint_files:\n",
        "        # If shard_file is \"\", we use the existing state_dict instead of loading it\n",
        "        if shard_file != \"\":\n",
        "            state_dict = load_state_dict(\n",
        "                shard_file, is_quantized=is_quantized, map_location=\"meta\", weights_only=weights_only\n",
        "            )\n",
        "\n",
        "        # Fix the key names\n",
        "        new_state_dict = {keys_to_rename_mapping[k]: v for k, v in state_dict.items() if k in keys_to_rename_mapping}\n",
        "\n",
        "        for key in new_state_dict.keys():\n",
        "            if key in model_state_dict and new_state_dict[key].shape != model_state_dict[key].shape:\n",
        "                # This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.\n",
        "                # Without matching with module type or parameter type it seems like a practical way to detect valid 4bit weights.\n",
        "                if not (\n",
        "                    is_quantized\n",
        "                    and new_state_dict[key].shape[-1] == 1\n",
        "                    and new_state_dict[key].numel() * 2 == model_state_dict[key].numel()\n",
        "                ):\n",
        "                    mismatched_keys.append(key)\n",
        "                    mismatched_shapes.append((new_state_dict[key].shape, model_state_dict[key].shape))\n",
        "\n",
        "    return mismatched_keys, mismatched_shapes\n",
        "\n",
        "def is_fsdp_enabled():\n",
        "    return (\n",
        "        torch.distributed.is_available()\n",
        "        and torch.distributed.is_initialized()\n",
        "        and strtobool(os.environ.get(\"ACCELERATE_USE_FSDP\", \"False\")) == 1\n",
        "        and strtobool(os.environ.get(\"FSDP_CPU_RAM_EFFICIENT_LOADING\", \"False\")) == 1\n",
        "    )\n",
        "\n",
        "def set_initialized_submodules(model, state_dict_keys):\n",
        "    state_dict_keys = set(state_dict_keys)\n",
        "    not_initialized_submodules = {}\n",
        "    for module_name, module in model.named_modules():\n",
        "        if module_name == \"\":\n",
        "            module_keys = set(module.state_dict())\n",
        "        else:\n",
        "            module_keys = {f\"{module_name}.{k}\" for k in module.state_dict()}\n",
        "        if module_keys.issubset(state_dict_keys):\n",
        "            module._is_hf_initialized = True\n",
        "        else:\n",
        "            not_initialized_submodules[module_name] = module\n",
        "    return not_initialized_submodules\n",
        "\n",
        "\n",
        "def load_state_dict(\n",
        "    checkpoint_file: Union[str, os.PathLike],\n",
        "    is_quantized: bool = False,\n",
        "    map_location: Optional[Union[str, torch.device]] = \"cpu\",\n",
        "    weights_only: bool = True,\n",
        "):\n",
        "    # Use safetensors if possible\n",
        "    if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n",
        "        with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
        "            metadata = f.metadata()\n",
        "\n",
        "            if metadata is not None and metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\", \"mlx\"]:\n",
        "                raise OSError(\n",
        "                    f\"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure \"\n",
        "                    \"you save your model with the `save_pretrained` method.\"\n",
        "                )\n",
        "            state_dict = {}\n",
        "            for k in f.keys():\n",
        "                if map_location == \"meta\":\n",
        "                    _slice = f.get_slice(k)\n",
        "                    k_dtype = _slice.get_dtype()\n",
        "                    if k_dtype in str_to_torch_dtype:\n",
        "                        dtype = str_to_torch_dtype[k_dtype]\n",
        "                    else:\n",
        "                        raise ValueError(f\"Cannot load safetensors of unknown dtype {k_dtype}\")\n",
        "                    state_dict[k] = torch.empty(size=_slice.get_shape(), dtype=dtype, device=\"meta\")\n",
        "                else:\n",
        "                    state_dict[k] = f.get_tensor(k)\n",
        "            return state_dict\n",
        "\n",
        "    # Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\n",
        "    if weights_only:\n",
        "        check_torch_load_is_safe()\n",
        "    try:\n",
        "        if map_location is None:\n",
        "            if (\n",
        "                (\n",
        "                    is_deepspeed_zero3_enabled()\n",
        "                    and torch.distributed.is_initialized()\n",
        "                    and torch.distributed.get_rank() > 0\n",
        "                )\n",
        "                or (is_fsdp_enabled() and not is_local_dist_rank_0())\n",
        "            ) and not is_quantized:\n",
        "                map_location = \"meta\"\n",
        "            else:\n",
        "                map_location = \"cpu\"\n",
        "        extra_args = {}\n",
        "        # mmap can only be used with files serialized with zipfile-based format.\n",
        "        if isinstance(checkpoint_file, str) and map_location != \"meta\" and is_zipfile(checkpoint_file):\n",
        "            extra_args = {\"mmap\": True}\n",
        "        return torch.load(\n",
        "            checkpoint_file,\n",
        "            map_location=map_location,\n",
        "            weights_only=weights_only,\n",
        "            **extra_args,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            with open(checkpoint_file) as f:\n",
        "                if f.read(7) == \"version\":\n",
        "                    raise OSError(\n",
        "                        \"You seem to have cloned a repository without having git-lfs installed. Please install \"\n",
        "                        \"git-lfs and run `git lfs install` followed by `git lfs pull` in the folder \"\n",
        "                        \"you cloned.\"\n",
        "                    )\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        f\"Unable to locate the file {checkpoint_file} which is necessary to load this pretrained \"\n",
        "                        \"model. Make sure you have saved the model properly.\"\n",
        "                    ) from e\n",
        "        except (UnicodeDecodeError, ValueError):\n",
        "            raise OSError(\n",
        "                f\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\n",
        "                f\"at '{checkpoint_file}'. \"\n",
        "                \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\n",
        "            )\n",
        "\n",
        "def load_shard_file(args):\n",
        "    (\n",
        "        shard_file,\n",
        "        state_dict,\n",
        "        disk_only_shard_files,\n",
        "        is_hqq_or_bnb,\n",
        "        is_quantized,\n",
        "        device_map,\n",
        "        hf_quantizer,\n",
        "        key_renaming_mapping,\n",
        "        weights_only,\n",
        "        model_to_load,\n",
        "        expected_keys,\n",
        "        reverse_key_renaming_mapping,\n",
        "        disk_offload_folder,\n",
        "        disk_offload_index,\n",
        "        cpu_offload_folder,\n",
        "        cpu_offload_index,\n",
        "        is_offloaded_safetensors,\n",
        "        keep_in_fp32_regex,\n",
        "        unexpected_keys,\n",
        "        device_mesh,\n",
        "    ) = args\n",
        "\n",
        "    # Skip the load for shards that only contain disk-offloaded weights\n",
        "    if shard_file in disk_only_shard_files:\n",
        "        return [], disk_offload_index, cpu_offload_index\n",
        "\n",
        "    map_location = \"cpu\"\n",
        "    if (\n",
        "        shard_file.endswith(\".safetensors\")\n",
        "        and not is_hqq_or_bnb\n",
        "        and not (is_deepspeed_zero3_enabled() and not is_quantized)\n",
        "    ):\n",
        "        map_location = \"meta\"\n",
        "    elif (\n",
        "        device_map is not None\n",
        "        and hf_quantizer is not None\n",
        "        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n",
        "        and (\n",
        "            hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n",
        "            or isinstance(hf_quantizer.quantization_config.quant_type, Int4WeightOnlyConfig)\n",
        "        )\n",
        "    ):\n",
        "        map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n",
        "\n",
        "    # If shard_file is \"\", we use the existing state_dict instead of loading it\n",
        "    if shard_file != \"\":\n",
        "        state_dict = load_state_dict(\n",
        "            shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only\n",
        "        )\n",
        "\n",
        "    # Fix the key names\n",
        "    state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n",
        "\n",
        "    error_msgs = []\n",
        "\n",
        "    if is_deepspeed_zero3_enabled() and not is_quantized:\n",
        "        error_msgs += _load_state_dict_into_zero3_model(model_to_load, state_dict)\n",
        "    # Skip it with fsdp on ranks other than 0\n",
        "    elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n",
        "        disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n",
        "            model_to_load,\n",
        "            state_dict,\n",
        "            shard_file,\n",
        "            expected_keys,\n",
        "            reverse_key_renaming_mapping,\n",
        "            device_map=device_map,\n",
        "            disk_offload_folder=disk_offload_folder,\n",
        "            disk_offload_index=disk_offload_index,\n",
        "            cpu_offload_folder=cpu_offload_folder,\n",
        "            cpu_offload_index=cpu_offload_index,\n",
        "            hf_quantizer=hf_quantizer,\n",
        "            is_safetensors=is_offloaded_safetensors,\n",
        "            keep_in_fp32_regex=keep_in_fp32_regex,\n",
        "            unexpected_keys=unexpected_keys,\n",
        "            device_mesh=device_mesh,\n",
        "        )\n",
        "\n",
        "    return error_msgs, disk_offload_index, cpu_offload_index\n",
        "\n",
        "\n",
        "class GELUActivation(nn.Module):\n",
        "    def __init__(self, use_gelu_python: bool = False):\n",
        "        super().__init__()\n",
        "        if use_gelu_python:\n",
        "            self.act = self._gelu_python\n",
        "        else:\n",
        "            self.act = nn.functional.gelu\n",
        "\n",
        "    def _gelu_python(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input * 0.5 * (1.0 + torch.erf(input / math.sqrt(2.0)))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(input)\n",
        "\n",
        "class ParallelInterface(GeneralInterface):\n",
        "    _global_mapping = (\n",
        "        {\n",
        "            \"colwise\": ColwiseParallel(),\n",
        "            \"rowwise\": RowwiseParallel(),\n",
        "            \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n",
        "            \"rowwise_rep\": RowwiseParallel(input_layouts=Replicate()),\n",
        "            \"local_colwise\": ColwiseParallel(use_dtensor=False),\n",
        "            \"local_rowwise\": RowwiseParallel(use_dtensor=False),\n",
        "            \"local\": IsolatedParallel(),\n",
        "            \"gather\": GatherParallel(),\n",
        "            \"local_packed_rowwise\": PackedRowwiseParallel(use_dtensor=False),\n",
        "            \"sequence_parallel\": SequenceParallel(),\n",
        "            \"replicate\": ReplicateParallel(),\n",
        "        }\n",
        "        if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available\n",
        "        else {}\n",
        "    )\n",
        "\n",
        "\n",
        "ALL_PARALLEL_STYLES: ParallelInterface = ParallelInterface()"
      ],
      "metadata": {
        "id": "GFx4Q4Sc1CFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d79b54-92b7-44b6-9289-65d1e0bc4fd7"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected torch version: 2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqrV4C7BkgxJ"
      },
      "source": [
        "# コード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "aY9BCGp2zz_R"
      },
      "outputs": [],
      "source": [
        "\n",
        "ACT2CLS = {\n",
        "    \"gelu\": GELUActivation,\n",
        "    \"gelu_10\": (ClippedGELUActivation, {\"min\": -10, \"max\": 10}),\n",
        "    \"gelu_fast\": FastGELUActivation,\n",
        "    \"gelu_new\": NewGELUActivation,\n",
        "    \"gelu_python\": (GELUActivation, {\"use_gelu_python\": True}),\n",
        "    \"gelu_pytorch_tanh\": PytorchGELUTanh,\n",
        "    \"gelu_accurate\": AccurateGELUActivation,\n",
        "    \"laplace\": LaplaceActivation,\n",
        "    \"leaky_relu\": nn.LeakyReLU,\n",
        "    \"linear\": LinearActivation,\n",
        "    \"mish\": MishActivation,\n",
        "    \"quick_gelu\": QuickGELUActivation,\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"relu2\": ReLUSquaredActivation,\n",
        "    \"relu6\": nn.ReLU6,\n",
        "    \"sigmoid\": nn.Sigmoid,\n",
        "    \"silu\": nn.SiLU,\n",
        "    \"swish\": nn.SiLU,\n",
        "    \"tanh\": nn.Tanh,\n",
        "    \"prelu\": nn.PReLU,\n",
        "}\n",
        "ACT2FN = ClassInstantier(ACT2CLS)\n",
        "\n",
        "def ForCausalLMLoss(\n",
        "    logits,\n",
        "    labels,\n",
        "    vocab_size: int,\n",
        "    num_items_in_batch: Optional[torch.Tensor] = None,\n",
        "    ignore_index: int = -100,\n",
        "    shift_labels: Optional[torch.Tensor] = None,\n",
        "    **kwargs,\n",
        ") -> torch.Tensor:\n",
        "    # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
        "    logits = logits.float()\n",
        "\n",
        "    if shift_labels is None:\n",
        "        # Shift so that tokens < n predict n\n",
        "        labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "    # Flatten the tokens\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    shift_labels = shift_labels.view(-1)\n",
        "    # Enable model parallelism\n",
        "    shift_labels = shift_labels.to(logits.device)\n",
        "    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n",
        "    return loss\n",
        "\n",
        "def ForMaskedLMLoss(\n",
        "    logits: torch.Tensor,\n",
        "    labels: torch.Tensor,\n",
        "    vocab_size: int,\n",
        "    num_items_in_batch: Optional[torch.Tensor] = None,\n",
        "    ignore_index: int = -100,\n",
        "    **kwargs,\n",
        "):\n",
        "    # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
        "    logits = logits.float()\n",
        "\n",
        "    # Flatten the tokens\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    labels = labels.view(-1)\n",
        "    # Enable model parallelism\n",
        "\n",
        "    labels = labels.to(logits.device)\n",
        "    loss = fixed_cross_entropy(logits, labels, num_items_in_batch, ignore_index, **kwargs)\n",
        "    return loss\n",
        "\n",
        "LOSS_MAPPING = {\n",
        "    \"ForCausalLM\": ForCausalLMLoss,\n",
        "    \"ForMaskedLM\": ForMaskedLMLoss,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "zOzAD0NfozUr"
      },
      "outputs": [],
      "source": [
        "class Qwen2Config(PretrainedConfig):\n",
        "    model_type = \"qwen2\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "\n",
        "    model_type: str = \"\"\n",
        "    base_config_key: str = \"\"\n",
        "    sub_configs: dict[str, type[\"PretrainedConfig\"]] = {}\n",
        "    has_no_defaults_at_init: bool = False\n",
        "    attribute_map: dict[str, str] = {}\n",
        "    base_model_tp_plan: Optional[dict[str, Any]] = None\n",
        "    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n",
        "    _auto_class: Optional[str] = None\n",
        "\n",
        "\n",
        "    base_model_tp_plan = {\n",
        "        \"layers.*.self_attn.q_proj\": \"colwise\",\n",
        "        \"layers.*.self_attn.k_proj\": \"colwise\",\n",
        "        \"layers.*.self_attn.v_proj\": \"colwise\",\n",
        "        \"layers.*.self_attn.o_proj\": \"rowwise\",\n",
        "        \"layers.*.mlp.gate_proj\": \"colwise\",\n",
        "        \"layers.*.mlp.up_proj\": \"colwise\",\n",
        "        \"layers.*.mlp.down_proj\": \"rowwise\",\n",
        "    }\n",
        "    base_model_pp_plan = {\n",
        "        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n",
        "        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n",
        "        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_token_in_kwargs(kwargs, token=None):\n",
        "        if token is None:\n",
        "            token = kwargs.pop(\"token\", None)\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "\n",
        "    @classmethod\n",
        "    def get_config_dict(\n",
        "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
        "    ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "        cls._set_token_in_kwargs(kwargs)\n",
        "\n",
        "        original_kwargs = copy.deepcopy(kwargs)\n",
        "        config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "        if \"_commit_hash\" in config_dict:\n",
        "            original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        return config_dict, kwargs\n",
        "\n",
        "    @classmethod\n",
        "    def _get_config_dict(\n",
        "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
        "    ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
        "        force_download = kwargs.pop(\"force_download\", False)\n",
        "        resume_download = kwargs.pop(\"resume_download\", None)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        token = kwargs.pop(\"token\", None)\n",
        "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
        "        revision = kwargs.pop(\"revision\", None)\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        gguf_file = kwargs.get(\"gguf_file\", None)\n",
        "\n",
        "        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n",
        "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
        "\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "\n",
        "        configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME) if gguf_file is None else gguf_file\n",
        "\n",
        "        resolved_config_file = cached_file(\n",
        "            pretrained_model_name_or_path,\n",
        "            configuration_file,\n",
        "            cache_dir=cache_dir,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            resume_download=resume_download,\n",
        "            local_files_only=local_files_only,\n",
        "            token=token,\n",
        "            user_agent=user_agent,\n",
        "            revision=revision,\n",
        "            subfolder=subfolder,\n",
        "            _commit_hash=commit_hash,\n",
        "        )\n",
        "        if resolved_config_file is None:\n",
        "            raise FileNotFoundError(f\"Config file not found at {pretrained_model_name_or_path}\")\n",
        "        commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "\n",
        "        config_dict = cls._dict_from_json_file(resolved_config_file)\n",
        "\n",
        "        config_dict[\"_commit_hash\"] = commit_hash\n",
        "\n",
        "        #if is_local:\n",
        "            #logger.info(f\"loading configuration file {resolved_config_file}\")\n",
        "        return config_dict, kwargs\n",
        "\n",
        "    @classmethod\n",
        "    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
        "        with open(json_file, encoding=\"utf-8\") as reader:\n",
        "            text = reader.read()\n",
        "        return json.loads(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(\n",
        "        cls: type[SpecificPretrainedConfigType], config_dict: dict[str, Any], **kwargs\n",
        "    ) -> SpecificPretrainedConfigType:\n",
        "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
        "        kwargs.pop(\"_from_auto\", None)\n",
        "        kwargs.pop(\"_from_pipeline\", None)\n",
        "        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n",
        "            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n",
        "\n",
        "        config = cls(**config_dict)\n",
        "\n",
        "        if hasattr(config, \"pruned_heads\"):\n",
        "            config.pruned_heads = {int(key): value for key, value in config.pruned_heads.items()}\n",
        "\n",
        "        to_remove = []\n",
        "        #logger.info(f\"Model config {config}\")\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls: type[SpecificPretrainedConfigType],\n",
        "        pretrained_model_name_or_path: Union[str, os.PathLike],\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        **kwargs,\n",
        "    ) -> SpecificPretrainedConfigType:\n",
        "        kwargs[\"cache_dir\"] = cache_dir\n",
        "        kwargs[\"force_download\"] = force_download\n",
        "        kwargs[\"local_files_only\"] = local_files_only\n",
        "        kwargs[\"revision\"] = revision\n",
        "\n",
        "        cls._set_token_in_kwargs(kwargs, token)\n",
        "\n",
        "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
        "            for v in config_dict.values():\n",
        "                if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n",
        "                    config_dict = v\n",
        "\n",
        "        return cls.from_dict(config_dict, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "18p38Q9meo6q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _get_frameworks_and_test_func(x):\n",
        "    framework_to_test = {\n",
        "        \"pt\": is_torch_tensor,\n",
        "        \"tf\": is_tf_tensor,\n",
        "        \"jax\": is_jax_tensor,\n",
        "        \"np\": is_numpy_array,\n",
        "        \"mlx\": is_mlx_array,\n",
        "    }\n",
        "    preferred_framework = infer_framework_from_repr(x)\n",
        "    frameworks = [] if preferred_framework is None else [preferred_framework]\n",
        "    if preferred_framework != \"np\":\n",
        "        frameworks.append(\"np\")\n",
        "    frameworks.extend([f for f in framework_to_test if f not in [preferred_framework, \"np\"]])\n",
        "    return {f: framework_to_test[f] for f in frameworks}\n",
        "\n",
        "\n",
        "\n",
        "def is_tensor(x):\n",
        "    framework_to_test_func = _get_frameworks_and_test_func(x)\n",
        "    for test_func in framework_to_test_func.values():\n",
        "        if test_func(x):\n",
        "            return True\n",
        "\n",
        "    if is_torch_fx_proxy(x):\n",
        "        return True\n",
        "\n",
        "    if is_flax_available():\n",
        "        from jax.core import Tracer\n",
        "\n",
        "        if isinstance(x, Tracer):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "class ModelOutput(OrderedDict):\n",
        "    def __init_subclass__(cls) -> None:\n",
        "        if is_torch_available():\n",
        "            if version.parse(get_torch_version()) >= version.parse(\"2.2\"):\n",
        "                from torch.utils._pytree import register_pytree_node\n",
        "\n",
        "                register_pytree_node(\n",
        "                    cls,\n",
        "                    _model_output_flatten,\n",
        "                    partial(_model_output_unflatten, output_type=cls),\n",
        "                    serialized_type_name=f\"{cls.__module__}.{cls.__name__}\",\n",
        "                )\n",
        "            else:\n",
        "                from torch.utils._pytree import _register_pytree_node\n",
        "\n",
        "                _register_pytree_node(\n",
        "                    cls,\n",
        "                    _model_output_flatten,\n",
        "                    partial(_model_output_unflatten, output_type=cls),\n",
        "                )\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        is_modeloutput_subclass = self.__class__ != ModelOutput\n",
        "\n",
        "        if is_modeloutput_subclass and not is_dataclass(self):\n",
        "            raise TypeError(\n",
        "                f\"{self.__module__}.{self.__class__.__name__} is not a dataclass.\"\n",
        "                \" This is a subclass of ModelOutput and so must use the @dataclass decorator.\"\n",
        "            )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        class_fields = fields(self)\n",
        "\n",
        "        if not len(class_fields):\n",
        "            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n",
        "        if not all(field.default is None for field in class_fields[1:]):\n",
        "            raise ValueError(f\"{self.__class__.__name__} should not have more than one required field.\")\n",
        "\n",
        "        first_field = getattr(self, class_fields[0].name)\n",
        "        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n",
        "\n",
        "        if other_fields_are_none and not is_tensor(first_field):\n",
        "            if isinstance(first_field, dict):\n",
        "                iterator = first_field.items()\n",
        "                first_field_iterator = True\n",
        "            else:\n",
        "                try:\n",
        "                    iterator = iter(first_field)\n",
        "                    first_field_iterator = True\n",
        "                except TypeError:\n",
        "                    first_field_iterator = False\n",
        "\n",
        "            if first_field_iterator:\n",
        "                for idx, element in enumerate(iterator):\n",
        "                    if (\n",
        "                        not isinstance(element, (list, tuple))\n",
        "                        or not len(element) == 2\n",
        "                        or not isinstance(element[0], str)\n",
        "                    ):\n",
        "                        if idx == 0:\n",
        "                            self[class_fields[0].name] = first_field\n",
        "                        else:\n",
        "                            raise ValueError(\n",
        "                                f\"Cannot set key/value for {element}. It needs to be a tuple (key, value).\"\n",
        "                            )\n",
        "                        break\n",
        "                    setattr(self, element[0], element[1])\n",
        "                    if element[1] is not None:\n",
        "                        self[element[0]] = element[1]\n",
        "            elif first_field is not None:\n",
        "                self[class_fields[0].name] = first_field\n",
        "        else:\n",
        "            for field in class_fields:\n",
        "                v = getattr(self, field.name)\n",
        "                if v is not None:\n",
        "                    self[field.name] = v\n",
        "\n",
        "    def __delitem__(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def setdefault(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def pop(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def update(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def __getitem__(self, k):\n",
        "        if isinstance(k, str):\n",
        "            inner_dict = dict(self.items())\n",
        "            return inner_dict[k]\n",
        "        else:\n",
        "            return self.to_tuple()[k]\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if name in self.keys() and value is not None:\n",
        "            # Don't call self.__setitem__ to avoid recursion errors\n",
        "            super().__setitem__(name, value)\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        super().__setitem__(key, value)\n",
        "        super().__setattr__(key, value)\n",
        "\n",
        "    def __reduce__(self):\n",
        "        if not is_dataclass(self):\n",
        "            return super().__reduce__()\n",
        "        callable, _args, *remaining = super().__reduce__()\n",
        "        args = tuple(getattr(self, field.name) for field in fields(self))\n",
        "        return callable, args, *remaining\n",
        "\n",
        "    def to_tuple(self) -> tuple[Any]:\n",
        "        return tuple(self[k] for k in self.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class BaseModelOutputWithPast(ModelOutput):\n",
        "    last_hidden_state: Optional[torch.FloatTensor] = None\n",
        "    past_key_values: Optional[Cache] = None\n",
        "    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "\n",
        "class GradientCheckpointingLayer(nn.Module):\n",
        "    gradient_checkpointing = False\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            do_warn = False\n",
        "            layer_name = self.__class__.__name__\n",
        "            message = f\"Caching is incompatible with gradient checkpointing in {layer_name}. Setting\"\n",
        "\n",
        "            if \"use_cache\" in kwargs and kwargs[\"use_cache\"]:\n",
        "                kwargs[\"use_cache\"] = False\n",
        "                message += \" `use_cache=False`,\"\n",
        "                do_warn = True\n",
        "\n",
        "            if \"past_key_value\" in kwargs and kwargs[\"past_key_value\"] is not None:\n",
        "                kwargs[\"past_key_value\"] = None\n",
        "                message += \" `past_key_value=None`,\"\n",
        "                do_warn = True\n",
        "\n",
        "            if \"past_key_values\" in kwargs and kwargs[\"past_key_values\"] is not None:\n",
        "                kwargs[\"past_key_values\"] = None\n",
        "                message += \" `past_key_values=None`,\"\n",
        "                do_warn = True\n",
        "\n",
        "            if \"layer_past\" in kwargs and kwargs[\"layer_past\"] is not None:\n",
        "                kwargs[\"layer_past\"] = None\n",
        "                message += \" `layer_past=None`,\"\n",
        "                do_warn = True\n",
        "\n",
        "            if do_warn:\n",
        "                message = message.rstrip(\",\") + \".\"\n",
        "                #logger.warning(message)\n",
        "\n",
        "            return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)\n",
        "        return super().__call__(*args, **kwargs)"
      ],
      "metadata": {
        "id": "Fw0lM5mPvvWV"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "P5vFFBGbV7wp"
      },
      "outputs": [],
      "source": [
        "class ModuleUtilsMixin:\n",
        "    @staticmethod\n",
        "    def _hook_rss_memory_pre_forward(module, *args, **kwargs):\n",
        "        try:\n",
        "            import psutil\n",
        "        except ImportError:\n",
        "            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n",
        "\n",
        "        process = psutil.Process(os.getpid())\n",
        "        mem = process.memory_info()\n",
        "        module.mem_rss_pre_forward = mem.rss\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _hook_rss_memory_post_forward(module, *args, **kwargs):\n",
        "        try:\n",
        "            import psutil\n",
        "        except ImportError:\n",
        "            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n",
        "\n",
        "        process = psutil.Process(os.getpid())\n",
        "        mem = process.memory_info()\n",
        "        module.mem_rss_post_forward = mem.rss\n",
        "        mem_rss_diff = module.mem_rss_post_forward - module.mem_rss_pre_forward\n",
        "        module.mem_rss_diff = mem_rss_diff + (module.mem_rss_diff if hasattr(module, \"mem_rss_diff\") else 0)\n",
        "        return None\n",
        "\n",
        "    def add_memory_hooks(self):\n",
        "        for module in self.modules():\n",
        "            module.register_forward_pre_hook(self._hook_rss_memory_pre_forward)\n",
        "            module.register_forward_hook(self._hook_rss_memory_post_forward)\n",
        "        self.reset_memory_hooks_state()\n",
        "\n",
        "    def reset_memory_hooks_state(self):\n",
        "        for module in self.modules():\n",
        "            module.mem_rss_diff = 0\n",
        "            module.mem_rss_post_forward = 0\n",
        "            module.mem_rss_pre_forward = 0\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return get_parameter_device(self)\n",
        "\n",
        "    @property\n",
        "    def dtype(self) -> torch.dtype:\n",
        "        return get_parameter_dtype(self)\n",
        "\n",
        "    def invert_attention_mask(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        if encoder_attention_mask.dim() == 3:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
        "        if encoder_attention_mask.dim() == 2:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
        "        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
        "        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * torch.finfo(self.dtype).min\n",
        "\n",
        "        return encoder_extended_attention_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None):\n",
        "        if device is not None:\n",
        "            warnings.warn(\n",
        "                \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n",
        "            )\n",
        "        else:\n",
        "            device = attention_mask.device\n",
        "        batch_size, seq_length = input_shape\n",
        "        seq_ids = torch.arange(seq_length, device=device)\n",
        "        causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
        "        # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
        "        causal_mask = causal_mask.to(attention_mask.dtype)\n",
        "\n",
        "        if causal_mask.shape[1] < attention_mask.shape[1]:\n",
        "            prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
        "            causal_mask = torch.cat(\n",
        "                [\n",
        "                    torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype),\n",
        "                    causal_mask,\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "\n",
        "        extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
        "        return extended_attention_mask\n",
        "\n",
        "    def get_extended_attention_mask(\n",
        "        self, attention_mask: torch.Tensor, input_shape: tuple[int], device: torch.device = None, dtype: torch.float = None\n",
        "    ) -> torch.Tensor:\n",
        "        if dtype is None:\n",
        "            dtype = self.dtype\n",
        "\n",
        "        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n",
        "            if device is not None:\n",
        "                warnings.warn(\n",
        "                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n",
        "                )\n",
        "        if attention_mask.dim() == 3:\n",
        "            extended_attention_mask = attention_mask[:, None, :, :]\n",
        "        elif attention_mask.dim() == 2:\n",
        "            if self.config.is_decoder:\n",
        "                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n",
        "                    input_shape, attention_mask, device\n",
        "                )\n",
        "            else:\n",
        "                extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n",
        "            )\n",
        "\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n",
        "        return extended_attention_mask\n",
        "\n",
        "    def _sdpa_can_dispatch(self, hard_check_only: bool = False) -> bool:\n",
        "        if hard_check_only:\n",
        "            if not self._supports_sdpa:\n",
        "                raise ValueError(\n",
        "                    f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n",
        "                    \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n",
        "                    ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n",
        "                )\n",
        "            if not is_torch_sdpa_available():\n",
        "                raise ImportError(\n",
        "                    \"PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.1.1.\"\n",
        "                )\n",
        "\n",
        "        if (\n",
        "            torch.version.hip is not None\n",
        "            and torch.cuda.device_count() > 1\n",
        "            and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n",
        "        ):\n",
        "            #logger.warning_once(\n",
        "                #\"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n",
        "            #)\n",
        "            torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "        # This means we have `hard_check_only=False` and fallback to eager if SDPA isn't supported\n",
        "        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n",
        "        if not is_torch_sdpa_available() or not self._supports_sdpa or _is_bettertransformer:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_head_mask(\n",
        "        self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False\n",
        "    ) -> torch.Tensor:\n",
        "        if head_mask is not None:\n",
        "            head_mask = self._convert_head_mask_to_5d(head_mask, num_hidden_layers)\n",
        "            if is_attention_chunked is True:\n",
        "                head_mask = head_mask.unsqueeze(-1)\n",
        "        else:\n",
        "            head_mask = [None] * num_hidden_layers\n",
        "\n",
        "        return head_mask\n",
        "\n",
        "    def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):\n",
        "        if head_mask.dim() == 1:\n",
        "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "            head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)\n",
        "        elif head_mask.dim() == 2:\n",
        "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
        "        assert head_mask.dim() == 5, f\"head_mask.dim != 5, instead {head_mask.dim()}\"\n",
        "        head_mask = head_mask.to(dtype=self.dtype)  # switch to float if need + fp16 compatibility\n",
        "        return head_mask\n",
        "\n",
        "    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n",
        "        if exclude_embeddings:\n",
        "            embedding_param_names = [\n",
        "                f\"{name}.weight\" for name, module_type in self.named_modules() if isinstance(module_type, nn.Embedding)\n",
        "            ]\n",
        "            total_parameters = [\n",
        "                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n",
        "            ]\n",
        "        else:\n",
        "            total_parameters = list(self.parameters())\n",
        "\n",
        "        total_numel = []\n",
        "        is_loaded_in_4bit = getattr(self, \"is_loaded_in_4bit\", False)\n",
        "\n",
        "        if is_loaded_in_4bit:\n",
        "            if is_bitsandbytes_available():\n",
        "                import bitsandbytes as bnb\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"bitsandbytes is not installed but it seems that the model has been loaded in 4bit precision, something went wrong\"\n",
        "                    \" make sure to install bitsandbytes with `pip install bitsandbytes`. You also need a GPU. \"\n",
        "                )\n",
        "\n",
        "        for param in total_parameters:\n",
        "            if param.requires_grad or not only_trainable:\n",
        "                if is_loaded_in_4bit and isinstance(param, bnb.nn.Params4bit):\n",
        "                    if hasattr(param, \"element_size\"):\n",
        "                        num_bytes = param.element_size()\n",
        "                    elif hasattr(param, \"quant_storage\"):\n",
        "                        num_bytes = param.quant_storage.itemsize\n",
        "                    else:\n",
        "                        num_bytes = 1\n",
        "                    total_numel.append(param.numel() * 2 * num_bytes)\n",
        "                else:\n",
        "                    total_numel.append(param.numel())\n",
        "\n",
        "        return sum(total_numel)\n",
        "\n",
        "    def estimate_tokens(self, input_dict: dict[str, Union[torch.Tensor, Any]]) -> int:\n",
        "        if not hasattr(self, \"warnings_issued\"):\n",
        "            self.warnings_issued = {}\n",
        "        if self.main_input_name in input_dict:\n",
        "            return input_dict[self.main_input_name].numel()\n",
        "        elif \"estimate_tokens\" not in self.warnings_issued:\n",
        "            #logger.warning(\n",
        "                #\"Could not estimate the number of tokens of the input, floating-point operations will not be computed\"\n",
        "            #)\n",
        "            self.warnings_issued[\"estimate_tokens\"] = True\n",
        "        return 0\n",
        "\n",
        "    def floating_point_ops(\n",
        "        self, input_dict: dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n",
        "    ) -> int:\n",
        "        return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n",
        "\n",
        "    def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n",
        "        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n",
        "\n",
        "        for key in self.config.sub_configs.keys():\n",
        "            sub_config = getattr(self.config, key)\n",
        "            curr_attn_implementation = (\n",
        "                requested_attn_implementation\n",
        "                if not isinstance(requested_attn_implementation, dict)\n",
        "                else requested_attn_implementation.get(key, None)\n",
        "            )\n",
        "            if (\n",
        "                sub_config is not None\n",
        "                and sub_config._attn_implementation_internal is None\n",
        "                and curr_attn_implementation is not None\n",
        "            ):\n",
        "                sub_config._attn_implementation_internal = curr_attn_implementation\n",
        "\n",
        "        if requested_attn_implementation == \"flash_attention_3\" and self._flash_attn_3_can_dispatch():\n",
        "            self.config._attn_implementation = \"flash_attention_3\"\n",
        "        if requested_attn_implementation == \"flash_attention_2\" and self._flash_attn_2_can_dispatch():\n",
        "            self.config._attn_implementation = \"flash_attention_2\"\n",
        "        elif requested_attn_implementation == \"flex_attention\" and self._flex_attn_can_dispatch():\n",
        "            self.config._attn_implementation = \"flex_attention\"\n",
        "        elif (\n",
        "            requested_attn_implementation in [None, \"sdpa\"]\n",
        "            and not is_torch_xla_available()\n",
        "            and self._sdpa_can_dispatch(hard_check_only=requested_attn_implementation is not None)\n",
        "        ):\n",
        "            self.config._attn_implementation = \"sdpa\"\n",
        "        elif requested_attn_implementation in AttentionInterface.valid_keys():\n",
        "            self.config._attn_implementation = requested_attn_implementation\n",
        "        elif isinstance(requested_attn_implementation, dict):\n",
        "            self.config._attn_implementation = requested_attn_implementation.get(\"\", None)\n",
        "        else:\n",
        "            self.config._attn_implementation = \"eager\"\n",
        "\n",
        "        self.config._attn_implementation_autoset = True\n",
        "\n",
        "    @classmethod\n",
        "    def _check_attn_implementation(cls, attn_implementation: Union[dict, str]) -> Union[dict, str]:\n",
        "        \"\"\"\n",
        "        Checks that the requested attention implementation exists and tries to get the kernel from hub\n",
        "        if `attn_implementation` matches hf kernels pattern.\n",
        "        \"\"\"\n",
        "        if isinstance(attn_implementation, str) and re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", attn_implementation):\n",
        "            if not is_kernels_available():\n",
        "                raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n",
        "\n",
        "            # Extract repo_id and kernel_name from the string\n",
        "            repo_id, kernel_name = attn_implementation.split(\":\")\n",
        "            kernel_name = kernel_name.strip()\n",
        "            repo_id = repo_id.strip()\n",
        "\n",
        "            try:\n",
        "                kernel = get_kernel(repo_id)\n",
        "                AttentionInterface.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n",
        "                attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n",
        "            except FileNotFoundError as e:\n",
        "                #logger.warning(\n",
        "                    #f\"Could not find a kernel repository '{repo_id}' compatible with your devicein the hub: {e}. Using eager attention implementation instead.\"\n",
        "                #)\n",
        "                attn_implementation = None  # try to dispatch SDPA and fallback eager if not available\n",
        "            except AttributeError:\n",
        "                raise ValueError(\n",
        "                    \"the kernel function name or class specified in the attn_implementation argument is not valid. \\\n",
        "                                 Please check the documentation for the correct format, \\\n",
        "                                 and check that the kernel exports the class and the function correctly.\"\n",
        "                )\n",
        "        if (\n",
        "            not isinstance(attn_implementation, dict)\n",
        "            and attn_implementation not in [\"eager\", None] + AttentionInterface.valid_keys()\n",
        "        ):\n",
        "            message = f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n",
        "            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n",
        "            if cls._supports_flash_attn or getattr(cls, \"_supports_flash_attn_2\", False):\n",
        "                message += (\n",
        "                    ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n",
        "                    ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n",
        "                )\n",
        "            if cls._supports_sdpa:\n",
        "                message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n",
        "            if cls._supports_flex_attn:\n",
        "                message += ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n",
        "            raise ValueError(message + \".\")\n",
        "\n",
        "        return attn_implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "piiRhR_EA21r"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PeftAdapterMixin:\n",
        "    _hf_peft_config_loaded = False\n",
        "\n",
        "    def load_adapter(\n",
        "        self,\n",
        "        peft_model_id: Optional[str] = None,\n",
        "        adapter_name: Optional[str] = None,\n",
        "        revision: Optional[str] = None,\n",
        "        token: Optional[str] = None,\n",
        "        device_map: Optional[str] = \"auto\",\n",
        "        max_memory: Optional[str] = None,\n",
        "        offload_folder: Optional[str] = None,\n",
        "        offload_index: Optional[int] = None,\n",
        "        peft_config: Optional[dict[str, Any]] = None,\n",
        "        adapter_state_dict: Optional[dict[str, \"torch.Tensor\"]] = None,\n",
        "        low_cpu_mem_usage: bool = False,\n",
        "        is_trainable: bool = False,\n",
        "        adapter_kwargs: Optional[dict[str, Any]] = None,\n",
        "    ) -> None:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "\n",
        "        # peft only supports low_cpu_mem_usage starting from v0.13.0\n",
        "        peft_load_kwargs = {}\n",
        "        key_mapping = adapter_kwargs.pop(\"key_mapping\", None) if adapter_kwargs is not None else None\n",
        "        if key_mapping is None and any(allowed_name in self.__class__.__name__.lower() for allowed_name in VLMS):\n",
        "            key_mapping = self._checkpoint_conversion_mapping\n",
        "        if low_cpu_mem_usage:\n",
        "            min_version_lcmu = \"0.13.0\"\n",
        "            if version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version_lcmu):\n",
        "                peft_load_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"The version of PEFT you are using does not support `low_cpu_mem_usage` yet, \"\n",
        "                    f\"please install PEFT >= {min_version_lcmu}.\"\n",
        "                )\n",
        "\n",
        "        adapter_name = adapter_name if adapter_name is not None else \"default\"\n",
        "        if adapter_kwargs is None:\n",
        "            adapter_kwargs = {}\n",
        "\n",
        "        from peft import PeftConfig, inject_adapter_in_model, load_peft_weights\n",
        "        from peft.utils import set_peft_model_state_dict\n",
        "\n",
        "        if self._hf_peft_config_loaded and adapter_name in self.peft_config:\n",
        "            raise ValueError(f\"Adapter with name {adapter_name} already exists. Please use a different name.\")\n",
        "\n",
        "        if peft_model_id is None and (adapter_state_dict is None and peft_config is None):\n",
        "            raise ValueError(\n",
        "                \"You should either pass a `peft_model_id` or a `peft_config` and `adapter_state_dict` to load an adapter.\"\n",
        "            )\n",
        "\n",
        "        if \"device\" not in adapter_kwargs:\n",
        "            device = self.device if not hasattr(self, \"hf_device_map\") else list(self.hf_device_map.values())[0]\n",
        "        else:\n",
        "            device = adapter_kwargs.pop(\"device\")\n",
        "\n",
        "        # To avoid PEFT errors later on with safetensors.\n",
        "        if isinstance(device, torch.device):\n",
        "            device = str(device)\n",
        "\n",
        "        # We keep `revision` in the signature for backward compatibility\n",
        "        if revision is not None and \"revision\" not in adapter_kwargs:\n",
        "            adapter_kwargs[\"revision\"] = revision\n",
        "        #elif revision is not None and \"revision\" in adapter_kwargs and revision != adapter_kwargs[\"revision\"]:\n",
        "            #logger.error(\n",
        "                #\"You passed a `revision` argument both in `adapter_kwargs` and as a standalone argument. \"\n",
        "                #\"The one in `adapter_kwargs` will be used.\"\n",
        "            #)\n",
        "\n",
        "        # Override token with adapter_kwargs' token\n",
        "        if \"token\" in adapter_kwargs:\n",
        "            token = adapter_kwargs.pop(\"token\")\n",
        "\n",
        "        if peft_config is None:\n",
        "            adapter_config_file = find_adapter_config_file(\n",
        "                peft_model_id,\n",
        "                token=token,\n",
        "                **adapter_kwargs,\n",
        "            )\n",
        "\n",
        "            if adapter_config_file is None:\n",
        "                raise ValueError(\n",
        "                    f\"adapter model file not found in {peft_model_id}. Make sure you are passing the correct path to the \"\n",
        "                    \"adapter model.\"\n",
        "                )\n",
        "\n",
        "            peft_config = PeftConfig.from_pretrained(\n",
        "                peft_model_id,\n",
        "                token=token,\n",
        "                **adapter_kwargs,\n",
        "            )\n",
        "            peft_config.inference_mode = not is_trainable\n",
        "\n",
        "        # Create and add fresh new adapters into the model.\n",
        "        inject_adapter_in_model(peft_config, self, adapter_name, **peft_load_kwargs)\n",
        "\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            self._hf_peft_config_loaded = True\n",
        "\n",
        "        if peft_model_id is not None:\n",
        "            adapter_state_dict = load_peft_weights(peft_model_id, token=token, device=device, **adapter_kwargs)\n",
        "\n",
        "        # We need to pre-process the state dict to remove unneeded prefixes - for backward compatibility\n",
        "        processed_adapter_state_dict = {}\n",
        "        prefix = \"base_model.model.\"\n",
        "        for key, value in adapter_state_dict.items():\n",
        "            if key.startswith(prefix):\n",
        "                new_key = key[len(prefix) :]\n",
        "            else:\n",
        "                new_key = key\n",
        "\n",
        "            if key_mapping:\n",
        "                for pattern, replacement in key_mapping.items():\n",
        "                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n",
        "                    # Early exit of the loop\n",
        "                    if n_replace > 0:\n",
        "                        break\n",
        "            processed_adapter_state_dict[new_key] = value\n",
        "\n",
        "        # Load state dict\n",
        "        incompatible_keys = set_peft_model_state_dict(\n",
        "            self, processed_adapter_state_dict, adapter_name, **peft_load_kwargs\n",
        "        )\n",
        "\n",
        "        if incompatible_keys is not None:\n",
        "            err_msg = \"\"\n",
        "            origin_name = peft_model_id if peft_model_id is not None else \"state_dict\"\n",
        "            # Check for unexpected keys.\n",
        "            if hasattr(incompatible_keys, \"unexpected_keys\") and len(incompatible_keys.unexpected_keys) > 0:\n",
        "                err_msg = (\n",
        "                    f\"Loading adapter weights from {origin_name} led to unexpected keys not found in the model: \"\n",
        "                    f\"{', '.join(incompatible_keys.unexpected_keys)}. \"\n",
        "                )\n",
        "\n",
        "            # Check for missing keys.\n",
        "            missing_keys = getattr(incompatible_keys, \"missing_keys\", None)\n",
        "            if missing_keys:\n",
        "                # Filter missing keys specific to the current adapter, as missing base model keys are expected.\n",
        "                lora_missing_keys = [k for k in missing_keys if \"lora_\" in k and adapter_name in k]\n",
        "                if lora_missing_keys:\n",
        "                    err_msg += (\n",
        "                        f\"Loading adapter weights from {origin_name} led to missing keys in the model: \"\n",
        "                        f\"{', '.join(lora_missing_keys)}\"\n",
        "                    )\n",
        "\n",
        "            #if err_msg:\n",
        "                #logger.warning(err_msg)\n",
        "\n",
        "        if peft_config.inference_mode:\n",
        "            self.eval()\n",
        "\n",
        "        # Re-dispatch model and hooks in case the model is offloaded to CPU / Disk.\n",
        "        if (\n",
        "            (getattr(self, \"hf_device_map\", None) is not None)\n",
        "            and (len(set(self.hf_device_map.values()).intersection({\"cpu\", \"disk\"})) > 0)\n",
        "            and len(self.peft_config) == 1\n",
        "        ):\n",
        "            self._dispatch_accelerate_model(\n",
        "                device_map=device_map,\n",
        "                max_memory=max_memory,\n",
        "                offload_folder=offload_folder,\n",
        "                offload_index=offload_index,\n",
        "            )\n",
        "\n",
        "    def add_adapter(self, adapter_config, adapter_name: Optional[str] = None) -> None:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "\n",
        "        from peft import PeftConfig, inject_adapter_in_model\n",
        "\n",
        "        adapter_name = adapter_name or \"default\"\n",
        "\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            self._hf_peft_config_loaded = True\n",
        "        elif adapter_name in self.peft_config:\n",
        "            raise ValueError(f\"Adapter with name {adapter_name} already exists. Please use a different name.\")\n",
        "\n",
        "        if not isinstance(adapter_config, PeftConfig):\n",
        "            raise TypeError(f\"adapter_config should be an instance of PeftConfig. Got {type(adapter_config)} instead.\")\n",
        "\n",
        "        # Retrieve the name or path of the model, one could also use self.config._name_or_path\n",
        "        # but to be consistent with what we do in PEFT: https://github.com/huggingface/peft/blob/6e783780ca9df3a623992cc4d1d665001232eae0/src/peft/mapping.py#L100\n",
        "        adapter_config.base_model_name_or_path = self.__dict__.get(\"name_or_path\", None)\n",
        "        inject_adapter_in_model(adapter_config, self, adapter_name)\n",
        "\n",
        "        self.set_adapter(adapter_name)\n",
        "\n",
        "    def set_adapter(self, adapter_name: Union[list[str], str]) -> None:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n",
        "        elif isinstance(adapter_name, list):\n",
        "            missing = set(adapter_name) - set(self.peft_config)\n",
        "            if len(missing) > 0:\n",
        "                raise ValueError(\n",
        "                    f\"Following adapter(s) could not be found: {', '.join(missing)}. Make sure you are passing the correct adapter name(s).\"\n",
        "                    f\" current loaded adapters are: {list(self.peft_config.keys())}\"\n",
        "                )\n",
        "        elif adapter_name not in self.peft_config:\n",
        "            raise ValueError(\n",
        "                f\"Adapter with name {adapter_name} not found. Please pass the correct adapter name among {list(self.peft_config.keys())}\"\n",
        "            )\n",
        "\n",
        "        from peft.tuners.tuners_utils import BaseTunerLayer\n",
        "        from peft.utils import ModulesToSaveWrapper\n",
        "\n",
        "        _adapters_has_been_set = False\n",
        "\n",
        "        for _, module in self.named_modules():\n",
        "            if isinstance(module, (BaseTunerLayer, ModulesToSaveWrapper)):\n",
        "                # For backward compatibility with previous PEFT versions\n",
        "                if hasattr(module, \"set_adapter\"):\n",
        "                    module.set_adapter(adapter_name)\n",
        "                else:\n",
        "                    module.active_adapter = adapter_name\n",
        "                _adapters_has_been_set = True\n",
        "\n",
        "        if not _adapters_has_been_set:\n",
        "            raise ValueError(\n",
        "                \"Did not succeeded in setting the adapter. Please make sure you are using a model that supports adapters.\"\n",
        "            )\n",
        "\n",
        "    def disable_adapters(self) -> None:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n",
        "\n",
        "        from peft.tuners.tuners_utils import BaseTunerLayer\n",
        "        from peft.utils import ModulesToSaveWrapper\n",
        "\n",
        "        for _, module in self.named_modules():\n",
        "            if isinstance(module, (BaseTunerLayer, ModulesToSaveWrapper)):\n",
        "                # The recent version of PEFT need to call `enable_adapters` instead\n",
        "                if hasattr(module, \"enable_adapters\"):\n",
        "                    module.enable_adapters(enabled=False)\n",
        "                else:\n",
        "                    module.disable_adapters = True\n",
        "\n",
        "    def enable_adapters(self) -> None:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n",
        "\n",
        "        from peft.tuners.tuners_utils import BaseTunerLayer\n",
        "\n",
        "        for _, module in self.named_modules():\n",
        "            if isinstance(module, BaseTunerLayer):\n",
        "                # The recent version of PEFT need to call `enable_adapters` instead\n",
        "                if hasattr(module, \"enable_adapters\"):\n",
        "                    module.enable_adapters(enabled=True)\n",
        "                else:\n",
        "                    module.disable_adapters = False\n",
        "\n",
        "    def active_adapters(self) -> list[str]:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "\n",
        "        if not is_peft_available():\n",
        "            raise ImportError(\"PEFT is not available. Please install PEFT to use this function: `pip install peft`.\")\n",
        "\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n",
        "\n",
        "        from peft.tuners.tuners_utils import BaseTunerLayer\n",
        "\n",
        "        for _, module in self.named_modules():\n",
        "            if isinstance(module, BaseTunerLayer):\n",
        "                active_adapters = module.active_adapter\n",
        "                break\n",
        "\n",
        "        # For previous PEFT versions\n",
        "        if isinstance(active_adapters, str):\n",
        "            active_adapters = [active_adapters]\n",
        "\n",
        "        return active_adapters\n",
        "\n",
        "    def active_adapter(self) -> str:\n",
        "        warnings.warn(\n",
        "            \"The `active_adapter` method is deprecated and will be removed in a future version.\", FutureWarning\n",
        "        )\n",
        "\n",
        "        return self.active_adapters()[0]\n",
        "\n",
        "    def get_adapter_state_dict(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n",
        "\n",
        "        from peft import get_peft_model_state_dict\n",
        "\n",
        "        if adapter_name is None:\n",
        "            adapter_name = self.active_adapters()[0]\n",
        "\n",
        "        adapter_state_dict = get_peft_model_state_dict(self, state_dict=state_dict, adapter_name=adapter_name)\n",
        "        return adapter_state_dict\n",
        "\n",
        "    def _dispatch_accelerate_model(\n",
        "        self,\n",
        "        device_map: str,\n",
        "        max_memory: Optional[int] = None,\n",
        "        offload_folder: Optional[str] = None,\n",
        "        offload_index: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        dispatch_model_kwargs = {}\n",
        "        # Safety checker for previous `accelerate` versions\n",
        "        # `offload_index` was introduced in https://github.com/huggingface/accelerate/pull/873/\n",
        "        if \"offload_index\" in inspect.signature(dispatch_model).parameters:\n",
        "            dispatch_model_kwargs[\"offload_index\"] = offload_index\n",
        "\n",
        "        no_split_module_classes = self._no_split_modules\n",
        "\n",
        "        if device_map != \"sequential\":\n",
        "            max_memory = get_balanced_memory(\n",
        "                self,\n",
        "                max_memory=max_memory,\n",
        "                no_split_module_classes=no_split_module_classes,\n",
        "                low_zero=(device_map == \"balanced_low_0\"),\n",
        "            )\n",
        "        if isinstance(device_map, str):\n",
        "            device_map = infer_auto_device_map(\n",
        "                self, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n",
        "            )\n",
        "        dispatch_model(\n",
        "            self,\n",
        "            device_map=device_map,\n",
        "            offload_dir=offload_folder,\n",
        "            **dispatch_model_kwargs,\n",
        "        )\n",
        "\n",
        "    def delete_adapter(self, adapter_names: Union[list[str], str]) -> None:\n",
        "        check_peft_version(min_version=MIN_PEFT_VERSION)\n",
        "\n",
        "        if not self._hf_peft_config_loaded:\n",
        "            raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n",
        "\n",
        "        from peft.tuners.tuners_utils import BaseTunerLayer\n",
        "\n",
        "        if isinstance(adapter_names, str):\n",
        "            adapter_names = [adapter_names]\n",
        "\n",
        "        # Check that all adapter names are present in the config\n",
        "        missing_adapters = [name for name in adapter_names if name not in self.peft_config]\n",
        "        if missing_adapters:\n",
        "            raise ValueError(\n",
        "                f\"The following adapter(s) are not present and cannot be deleted: {', '.join(missing_adapters)}\"\n",
        "            )\n",
        "\n",
        "        for adapter_name in adapter_names:\n",
        "            for module in self.modules():\n",
        "                if isinstance(module, BaseTunerLayer):\n",
        "                    if hasattr(module, \"delete_adapter\"):\n",
        "                        module.delete_adapter(adapter_name)\n",
        "                    else:\n",
        "                        raise ValueError(\n",
        "                            \"The version of PEFT you are using is not compatible, please use a version that is greater than 0.6.1\"\n",
        "                        )\n",
        "\n",
        "            # For transformers integration - we need to pop the adapter from the config\n",
        "            if getattr(self, \"_hf_peft_config_loaded\", False) and hasattr(self, \"peft_config\"):\n",
        "                self.peft_config.pop(adapter_name, None)\n",
        "\n",
        "        # In case all adapters are deleted, we need to delete the config\n",
        "        # and make sure to set the flag to False\n",
        "        if len(self.peft_config) == 0:\n",
        "            del self.peft_config\n",
        "            self._hf_peft_config_loaded = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_NAME = \"config.json\"\n",
        "SAFE_WEIGHTS_NAME = \"model.safetensors\"\n",
        "GENERATION_CONFIG_NAME = \"generation_config.json\"\n",
        "\n",
        "_safetensors_available = _is_package_available(\"safetensors\")\n",
        "_is_offline_mode = huggingface_hub.constants.HF_HUB_OFFLINE\n",
        "SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n",
        "_hf_deepspeed_config_weak_ref = None\n",
        "\n",
        "VLMS = [\n",
        "    \"aria\",\n",
        "    \"ayavision\",\n",
        "    \"colpali\",\n",
        "    \"emu3\",\n",
        "    \"fuyu\",\n",
        "    \"gotocr2\",\n",
        "    \"gemma3\",\n",
        "    \"internvl\",\n",
        "    \"llava\",  # all llava prefixed models fall under this check\n",
        "    \"mistral3\",\n",
        "    \"mllama\",\n",
        "    \"paligemma\",\n",
        "    \"shieldgemma2\",\n",
        "    \"qwen2vl\",\n",
        "    \"qwen2_5_vl\",\n",
        "    \"videollava\",\n",
        "    \"vipllava\",\n",
        "]\n",
        "\n",
        "def is_offline_mode():\n",
        "    return _is_offline_mode\n",
        "\n",
        "def extract_commit_hash(resolved_file: Optional[str], commit_hash: Optional[str]) -> Optional[str]:\n",
        "    if resolved_file is None or commit_hash is not None:\n",
        "        return commit_hash\n",
        "    resolved_file = str(Path(resolved_file).as_posix())\n",
        "    search = re.search(r\"snapshots/([^/]+)/\", resolved_file)\n",
        "    if search is None:\n",
        "        return None\n",
        "    commit_hash = search.groups()[0]\n",
        "    return commit_hash if REGEX_COMMIT_HASH.match(commit_hash) else None\n",
        "\n",
        "_torch_available = False\n",
        "\n",
        "def is_torch_available():\n",
        "    return _torch_available\n",
        "\n",
        "def restore_default_torch_dtype(func):\n",
        "    @wraps(func)\n",
        "    def _wrapper(*args, **kwargs):\n",
        "        old_dtype = torch.get_default_dtype()\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        finally:\n",
        "            torch.set_default_dtype(old_dtype)\n",
        "\n",
        "    return _wrapper\n",
        "\n",
        "def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n",
        "    if variant is not None:\n",
        "        path, name = weights_name.rsplit(\".\", 1)\n",
        "        weights_name = f\"{path}.{variant}.{name}\"\n",
        "    return weights_name"
      ],
      "metadata": {
        "id": "R2lCj0Qbhw85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522ba5de-3f5e-42a4-a38b-023c788cce85"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected safetensors version: 0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "C8J6-Q65yDkx"
      },
      "outputs": [],
      "source": [
        "class PreTrainedModel(nn.Module, ModuleUtilsMixin, PeftAdapterMixin): #PushToHubMixin\n",
        "    _auto_class = None\n",
        "    _keep_in_fp32_modules = None\n",
        "    _keep_in_fp32_modules_strict = None\n",
        "    _keys_to_ignore_on_load_missing = None\n",
        "    _keys_to_ignore_on_load_unexpected = None\n",
        "    main_input_name = \"input_ids\"\n",
        "\n",
        "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
        "        super().__init__()\n",
        "        if not isinstance(config, PretrainedConfig):\n",
        "            raise ValueError(\n",
        "                f\"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class \"\n",
        "                \"`PretrainedConfig`. To create a model from a pretrained model use \"\n",
        "                f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n",
        "            )\n",
        "        self.config = config\n",
        "\n",
        "        if hasattr(config, \"_attn_implementation_internal\") and not getattr(\n",
        "            config, \"_attn_implementation_autoset\", False\n",
        "        ):\n",
        "            self.set_attention_implementation(self.config._attn_implementation_internal)\n",
        "\n",
        "        loss_type = self.__class__.__name__\n",
        "        if loss_type not in LOSS_MAPPING:\n",
        "            loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n",
        "            loss_type = re.findall(loss_groups, self.__class__.__name__)\n",
        "            if len(loss_type) > 0:\n",
        "                loss_type = loss_type[0]\n",
        "            else:\n",
        "                loss_type = None\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        self.name_or_path = config.name_or_path\n",
        "        self.warnings_issued = {}\n",
        "        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n",
        "        self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n",
        "        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n",
        "\n",
        "        self._no_split_modules = self._no_split_modules or []\n",
        "        _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs  # added for executorch support only\n",
        "\n",
        "    @classmethod\n",
        "    @restore_default_torch_dtype\n",
        "    def from_pretrained(\n",
        "        cls: type[SpecificPreTrainedModelType],\n",
        "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
        "        *model_args,\n",
        "        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        ignore_mismatched_sizes: bool = False,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        use_safetensors: Optional[bool] = None,\n",
        "        weights_only: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> SpecificPreTrainedModelType:\n",
        "        state_dict = kwargs.pop(\"state_dict\", None)\n",
        "        from_tf = kwargs.pop(\"from_tf\", False)\n",
        "        from_flax = kwargs.pop(\"from_flax\", False)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
        "        device_map = kwargs.pop(\"device_map\", None)\n",
        "        max_memory = kwargs.pop(\"max_memory\", None)\n",
        "        offload_folder = kwargs.pop(\"offload_folder\", None)\n",
        "        offload_state_dict = kwargs.pop(\"offload_state_dict\", False)\n",
        "        offload_buffers = kwargs.pop(\"offload_buffers\", False)\n",
        "        load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n",
        "        load_in_4bit = kwargs.pop(\"load_in_4bit\", False)\n",
        "        quantization_config = kwargs.pop(\"quantization_config\", None)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "        variant = kwargs.pop(\"variant\", None)\n",
        "        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n",
        "        adapter_name = kwargs.pop(\"adapter_name\", \"default\")\n",
        "        generation_config = kwargs.pop(\"generation_config\", None)\n",
        "        gguf_file = kwargs.pop(\"gguf_file\", None)\n",
        "        tp_plan = kwargs.pop(\"tp_plan\", None)\n",
        "        tp_size = kwargs.pop(\"tp_size\", None)\n",
        "        device_mesh = kwargs.pop(\"device_mesh\", None)\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        use_kernels = kwargs.pop(\"use_kernels\", False)\n",
        "\n",
        "        key_mapping = kwargs.pop(\"key_mapping\", None)\n",
        "        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model\n",
        "        if key_mapping is None and any(\n",
        "            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n",
        "        ):\n",
        "            key_mapping = cls._checkpoint_conversion_mapping\n",
        "\n",
        "        # Not used anymore -- remove them from the kwargs\n",
        "        _ = kwargs.pop(\"resume_download\", None)\n",
        "        _ = kwargs.pop(\"mirror\", None)\n",
        "        _ = kwargs.pop(\"_fast_init\", True)\n",
        "        _ = kwargs.pop(\"low_cpu_mem_usage\", None)\n",
        "\n",
        "        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):\n",
        "            raise ValueError(\n",
        "                \"`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies.\"\n",
        "            )\n",
        "        if tp_size is not None and tp_plan is None:\n",
        "            raise ValueError(\"tp_plan has to be set when tp_size is passed.\")\n",
        "        if tp_plan is not None and tp_plan != \"auto\":\n",
        "            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.\n",
        "            raise ValueError(f\"tp_plan supports 'auto' only for now but got {tp_plan}.\")\n",
        "        if tp_plan is not None and device_map is not None:\n",
        "            raise ValueError(\n",
        "                \"`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization.\"\n",
        "            )\n",
        "\n",
        "        if device_map == \"auto\" and int(os.environ.get(\"WORLD_SIZE\", 0)):\n",
        "            logger.info(\n",
        "                \"You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. \"\n",
        "                \"If your plan is to load the model on each device, you should set device_map={\"\n",
        "                \": PartialState().process_index} where PartialState comes from accelerate library\"\n",
        "            )\n",
        "\n",
        "        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple\n",
        "        # `device_map` pointing to the correct device\n",
        "        if tp_plan is not None:\n",
        "            if device_mesh is None and tp_plan is not None:\n",
        "                tp_plan, device_map, device_mesh = initialize_tensor_parallelism(tp_plan, tp_size=None)\n",
        "            else:\n",
        "                # TODO: make device_mesh support multiple dimensions\n",
        "                if device_mesh.ndim == 1:\n",
        "                    raise ValueError(\"device_mesh must be 1 dimensional and will be used for TP\")\n",
        "                device_map = torch.device(device_mesh.device_type, int(os.environ[\"LOCAL_RANK\"]))\n",
        "\n",
        "            if tp_size is None:\n",
        "                tp_size = torch.distributed.get_world_size()\n",
        "\n",
        "        if use_auth_token is not None:\n",
        "            warnings.warn(\n",
        "                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            if token is not None:\n",
        "                raise ValueError(\n",
        "                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n",
        "                )\n",
        "            token = use_auth_token\n",
        "\n",
        "        if token is not None and adapter_kwargs is not None and \"token\" not in adapter_kwargs:\n",
        "            adapter_kwargs[\"token\"] = token\n",
        "\n",
        "        if use_safetensors is None and not is_safetensors_available():\n",
        "            use_safetensors = False\n",
        "\n",
        "        if gguf_file is not None and not is_accelerate_available():\n",
        "            raise ValueError(\"accelerate is required when loading a GGUF file `pip install accelerate`.\")\n",
        "\n",
        "        if commit_hash is None:\n",
        "            if not isinstance(config, PretrainedConfig):\n",
        "                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n",
        "                resolved_config_file = cached_file(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    CONFIG_NAME,\n",
        "                    cache_dir=cache_dir,\n",
        "                    force_download=force_download,\n",
        "                    proxies=proxies,\n",
        "                    local_files_only=local_files_only,\n",
        "                    token=token,\n",
        "                    revision=revision,\n",
        "                    subfolder=subfolder,\n",
        "                    _raise_exceptions_for_gated_repo=False,\n",
        "                    _raise_exceptions_for_missing_entries=False,\n",
        "                    _raise_exceptions_for_connection_errors=False,\n",
        "                )\n",
        "                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "            else:\n",
        "                commit_hash = getattr(config, \"_commit_hash\", None)\n",
        "\n",
        "        if is_peft_available():\n",
        "            _adapter_model_path = adapter_kwargs.pop(\"_adapter_model_path\", None)\n",
        "\n",
        "            if _adapter_model_path is None:\n",
        "                _adapter_model_path = find_adapter_config_file(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    cache_dir=cache_dir,\n",
        "                    force_download=force_download,\n",
        "                    proxies=proxies,\n",
        "                    local_files_only=local_files_only,\n",
        "                    _commit_hash=commit_hash,\n",
        "                    **adapter_kwargs,\n",
        "                )\n",
        "            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):\n",
        "                with open(_adapter_model_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    _adapter_model_path = pretrained_model_name_or_path\n",
        "                    pretrained_model_name_or_path = json.load(f)[\"base_model_name_or_path\"]\n",
        "        else:\n",
        "            _adapter_model_path = None\n",
        "\n",
        "        # Potentially detect context manager or global device, and use it (only if no device_map was provided)\n",
        "        if device_map is None and not is_deepspeed_zero3_enabled():\n",
        "            device_in_context = get_torch_context_manager_or_global_device()\n",
        "            #if device_in_context == torch.device(\"meta\"):\n",
        "                # TODO Cyril: raise an error instead of the warning in v4.53 (and change the test to check for raise instead of success)\n",
        "                #logger.warning(\n",
        "                    #\"We detected that you are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`\\n\"\n",
        "                    #\"This is an anti-pattern and will raise an Error in version v4.53\\nIf you want to initialize a model on the meta device, use \"\n",
        "                    #\"the context manager or global device with `from_config`, or `ModelClass(config)`\"\n",
        "                #)\n",
        "            device_map = device_in_context\n",
        "\n",
        "        # change device_map into a map if we passed an int, a str or a torch.device\n",
        "        if isinstance(device_map, torch.device):\n",
        "            device_map = {\"\": device_map}\n",
        "        elif isinstance(device_map, str) and device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:\n",
        "            try:\n",
        "                device_map = {\"\": torch.device(device_map)}\n",
        "            except RuntimeError:\n",
        "                raise ValueError(\n",
        "                    \"When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or \"\n",
        "                    f\"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}.\"\n",
        "                )\n",
        "        elif isinstance(device_map, int):\n",
        "            if device_map < 0:\n",
        "                raise ValueError(\n",
        "                    \"You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' \"\n",
        "                )\n",
        "            else:\n",
        "                device_map = {\"\": device_map}\n",
        "\n",
        "        if device_map is not None:\n",
        "            if is_deepspeed_zero3_enabled():\n",
        "                raise ValueError(\"DeepSpeed Zero-3 is not compatible with passing a `device_map`.\")\n",
        "            if not is_accelerate_available():\n",
        "                raise ValueError(\n",
        "                    \"Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \"\n",
        "                    \"requires `accelerate`. You can install it with `pip install accelerate`\"\n",
        "                )\n",
        "\n",
        "        # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\n",
        "        if load_in_4bit or load_in_8bit:\n",
        "            if quantization_config is not None:\n",
        "                raise ValueError(\n",
        "                    \"You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing \"\n",
        "                    \"`quantization_config` argument at the same time.\"\n",
        "                )\n",
        "\n",
        "            # preparing BitsAndBytesConfig from kwargs\n",
        "            config_dict = {k: v for k, v in kwargs.items() if k in inspect.signature(BitsAndBytesConfig).parameters}\n",
        "            config_dict = {**config_dict, \"load_in_4bit\": load_in_4bit, \"load_in_8bit\": load_in_8bit}\n",
        "            quantization_config, kwargs = BitsAndBytesConfig.from_dict(\n",
        "                config_dict=config_dict, return_unused_kwargs=True, **kwargs\n",
        "            )\n",
        "            #logger.warning(\n",
        "                #\"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. \"\n",
        "                #\"Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\"\n",
        "            #)\n",
        "\n",
        "        from_pt = not (from_tf | from_flax)\n",
        "\n",
        "        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n",
        "        if from_pipeline is not None:\n",
        "            user_agent[\"using_pipeline\"] = from_pipeline\n",
        "\n",
        "        if is_offline_mode() and not local_files_only:\n",
        "            #logger.info(\"Offline mode: forcing local_files_only=True\")\n",
        "            local_files_only = True\n",
        "\n",
        "        # Load config if we don't provide a configuration\n",
        "        if not isinstance(config, PretrainedConfig):\n",
        "            config_path = config if config is not None else pretrained_model_name_or_path\n",
        "            config, model_kwargs = cls.config_class.from_pretrained(\n",
        "                config_path,\n",
        "                cache_dir=cache_dir,\n",
        "                return_unused_kwargs=True,\n",
        "                force_download=force_download,\n",
        "                proxies=proxies,\n",
        "                local_files_only=local_files_only,\n",
        "                token=token,\n",
        "                revision=revision,\n",
        "                subfolder=subfolder,\n",
        "                gguf_file=gguf_file,\n",
        "                _from_auto=from_auto_class,\n",
        "                _from_pipeline=from_pipeline,\n",
        "                **kwargs,\n",
        "            )\n",
        "            if \"gguf_file\" in model_kwargs:\n",
        "                model_kwargs.pop(\"gguf_file\")\n",
        "        else:\n",
        "            # In case one passes a config to `from_pretrained` + \"attn_implementation\"\n",
        "            # override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\n",
        "            # Please see: https://github.com/huggingface/transformers/issues/28038\n",
        "\n",
        "            # Overwrite `config._attn_implementation` by the one from the kwargs --> in auto-factory\n",
        "            # we pop attn_implementation from the kwargs but this handles the case where users\n",
        "            # passes manually the config to `from_pretrained`.\n",
        "            config = copy.deepcopy(config)\n",
        "\n",
        "            kwarg_attn_imp = kwargs.pop(\"attn_implementation\", None)\n",
        "            if kwarg_attn_imp is not None:\n",
        "                config._attn_implementation = kwarg_attn_imp\n",
        "\n",
        "            model_kwargs = kwargs\n",
        "\n",
        "        transformers_explicit_filename = getattr(config, \"transformers_weights\", None)\n",
        "\n",
        "        if transformers_explicit_filename is not None:\n",
        "            if not transformers_explicit_filename.endswith(\n",
        "                \".safetensors\"\n",
        "            ) and not transformers_explicit_filename.endswith(\".safetensors.index.json\"):\n",
        "                raise ValueError(\n",
        "                    \"The transformers file in the config seems to be incorrect: it is neither a safetensors file \"\n",
        "                    \"(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \"\n",
        "                    f\"{transformers_explicit_filename}\"\n",
        "                )\n",
        "\n",
        "        pre_quantized = hasattr(config, \"quantization_config\")\n",
        "        if pre_quantized and not AutoHfQuantizer.supports_quant_method(config.quantization_config):\n",
        "            pre_quantized = False\n",
        "\n",
        "        if pre_quantized or quantization_config is not None:\n",
        "            if pre_quantized:\n",
        "                config.quantization_config = AutoHfQuantizer.merge_quantization_configs(\n",
        "                    config.quantization_config, quantization_config\n",
        "                )\n",
        "            else:\n",
        "                config.quantization_config = quantization_config\n",
        "\n",
        "            hf_quantizer = AutoHfQuantizer.from_config(\n",
        "                config.quantization_config,\n",
        "                pre_quantized=pre_quantized,\n",
        "            )\n",
        "        else:\n",
        "            hf_quantizer = None\n",
        "\n",
        "        if hf_quantizer is not None:\n",
        "            hf_quantizer.validate_environment(\n",
        "                torch_dtype=torch_dtype,\n",
        "                from_tf=from_tf,\n",
        "                from_flax=from_flax,\n",
        "                device_map=device_map,\n",
        "                weights_only=weights_only,\n",
        "            )\n",
        "            torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n",
        "            device_map = hf_quantizer.update_device_map(device_map)\n",
        "            config = hf_quantizer.update_tp_plan(config)\n",
        "\n",
        "            # In order to ensure popular quantization methods are supported. Can be disable with `disable_telemetry`\n",
        "            if hasattr(hf_quantizer.quantization_config.quant_method, \"value\"):\n",
        "                user_agent[\"quant\"] = hf_quantizer.quantization_config.quant_method.value\n",
        "            else:\n",
        "                user_agent[\"quant\"] = hf_quantizer.quantization_config.quant_method\n",
        "\n",
        "        if gguf_file is not None and hf_quantizer is not None:\n",
        "            raise ValueError(\n",
        "                \"You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\"\n",
        "            )\n",
        "\n",
        "        if (\n",
        "            gguf_file\n",
        "            and device_map is not None\n",
        "            and ((isinstance(device_map, dict) and \"disk\" in device_map.values()) or \"disk\" in device_map)\n",
        "        ):\n",
        "            raise RuntimeError(\n",
        "                \"One or more modules is configured to be mapped to disk. Disk offload is not supported for models \"\n",
        "                \"loaded from GGUF files.\"\n",
        "            )\n",
        "\n",
        "        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n",
        "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "            subfolder=subfolder,\n",
        "            variant=variant,\n",
        "            gguf_file=gguf_file,\n",
        "            from_tf=from_tf,\n",
        "            from_flax=from_flax,\n",
        "            use_safetensors=use_safetensors,\n",
        "            cache_dir=cache_dir,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            local_files_only=local_files_only,\n",
        "            token=token,\n",
        "            user_agent=user_agent,\n",
        "            revision=revision,\n",
        "            commit_hash=commit_hash,\n",
        "            is_remote_code=cls._auto_class is not None,\n",
        "            transformers_explicit_filename=transformers_explicit_filename,\n",
        "        )\n",
        "\n",
        "        is_sharded = sharded_metadata is not None\n",
        "        is_quantized = hf_quantizer is not None\n",
        "        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None\n",
        "\n",
        "        if (\n",
        "            is_safetensors_available()\n",
        "            and is_from_file\n",
        "            and not is_sharded\n",
        "            and checkpoint_files[0].endswith(\".safetensors\")\n",
        "        ):\n",
        "            with safe_open(checkpoint_files[0], framework=\"pt\") as f:\n",
        "                metadata = f.metadata()\n",
        "\n",
        "            if metadata is None:\n",
        "                # Assume it's a pytorch checkpoint (introduced for timm checkpoints)\n",
        "                pass\n",
        "            elif metadata.get(\"format\") == \"pt\":\n",
        "                pass\n",
        "            elif metadata.get(\"format\") == \"tf\":\n",
        "                from_tf = True\n",
        "                #logger.info(\"A TensorFlow safetensors file is being loaded in a PyTorch model.\")\n",
        "            elif metadata.get(\"format\") == \"flax\":\n",
        "                from_flax = True\n",
        "                #logger.info(\"A Flax safetensors file is being loaded in a PyTorch model.\")\n",
        "            elif metadata.get(\"format\") == \"mlx\":\n",
        "                # This is a mlx file, we assume weights are compatible with pt\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"Incompatible safetensors file. File metadata is not ['pt', 'tf', 'flax', 'mlx'] but {metadata.get('format')}\"\n",
        "                )\n",
        "\n",
        "        from_pt = not (from_tf | from_flax)\n",
        "\n",
        "        if from_pt:\n",
        "            if gguf_file:\n",
        "                from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n",
        "\n",
        "                # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was\n",
        "                # passed directly as a kwarg from now on\n",
        "                with torch.device(\"meta\"):\n",
        "                    dummy_model = cls(config)\n",
        "                state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[\n",
        "                    \"tensors\"\n",
        "                ]\n",
        "\n",
        "            # Find the correct dtype based on current state\n",
        "            config, torch_dtype, dtype_orig = _get_torch_dtype(\n",
        "                cls, torch_dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n",
        "            )\n",
        "\n",
        "        config.name_or_path = pretrained_model_name_or_path\n",
        "\n",
        "        # Instantiate model.\n",
        "        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n",
        "\n",
        "        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n",
        "        with ContextManagers(model_init_context):\n",
        "            # Let's make sure we don't run the init function of buffer modules\n",
        "            model = cls(config, *model_args, **model_kwargs)\n",
        "\n",
        "        # Make sure to tie the weights correctly\n",
        "        model.tie_weights()\n",
        "\n",
        "        # Last check for tp\n",
        "        if device_mesh is not None and not model.supports_tp_plan:\n",
        "            if config.base_model_tp_plan is None and config.get_text_config().base_model_tp_plan is None:\n",
        "                raise NotImplementedError(\"This model does not have a tensor parallel plan.\")\n",
        "\n",
        "        # make sure we use the model's config since the __init__ call might have copied it\n",
        "        config = model.config\n",
        "\n",
        "        # Find fp32 modules if needed\n",
        "        keep_in_fp32_modules = []\n",
        "        # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precis\n",
        "        # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n",
        "        # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n",
        "        if model._keep_in_fp32_modules is not None and (\n",
        "            torch_dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n",
        "        ):\n",
        "            keep_in_fp32_modules.extend(model._keep_in_fp32_modules)\n",
        "\n",
        "        if model._keep_in_fp32_modules_strict is not None and (\n",
        "            torch_dtype == torch.float16 or torch_dtype == torch.bfloat16\n",
        "        ):\n",
        "            keep_in_fp32_modules.extend(model._keep_in_fp32_modules_strict)\n",
        "\n",
        "        keep_in_fp32_regex = None\n",
        "        if keep_in_fp32_modules:\n",
        "            # We need to match exact layers, so we add either `.` on each side, or start/end of string\n",
        "            keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n",
        "\n",
        "        if hf_quantizer is not None:\n",
        "            hf_quantizer.preprocess_model(\n",
        "                model=model, device_map=device_map, keep_in_fp32_modules=model._keep_in_fp32_modules, config=config\n",
        "            )\n",
        "            # We store the original dtype for quantized models as we cannot easily retrieve it\n",
        "            # once the weights have been quantized\n",
        "            # Note that once you have loaded a quantized model, you can't change its dtype so this will\n",
        "            # remain a single source of truth\n",
        "            original_dtype = torch_dtype if torch_dtype is not None else torch.get_default_dtype()\n",
        "\n",
        "            def _assign_original_dtype(module):\n",
        "                for child in module.children():\n",
        "                    if isinstance(child, PreTrainedModel):\n",
        "                        child.config._pre_quantization_dtype = original_dtype\n",
        "                    _assign_original_dtype(child)\n",
        "\n",
        "            config._pre_quantization_dtype = original_dtype\n",
        "            _assign_original_dtype(model)\n",
        "\n",
        "        # Prepare the full device map\n",
        "        if device_map is not None:\n",
        "            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n",
        "\n",
        "        # Finalize model weight initialization\n",
        "        if from_tf:\n",
        "            model, loading_info = cls._load_from_tf(model, config, checkpoint_files)\n",
        "        elif from_flax:\n",
        "            model = cls._load_from_flax(model, checkpoint_files)\n",
        "        elif from_pt:\n",
        "            # restore default dtype\n",
        "            if dtype_orig is not None:\n",
        "                torch.set_default_dtype(dtype_orig)\n",
        "\n",
        "            (\n",
        "                model,\n",
        "                missing_keys,\n",
        "                unexpected_keys,\n",
        "                mismatched_keys,\n",
        "                offload_index,\n",
        "                error_msgs,\n",
        "            ) = cls._load_pretrained_model(\n",
        "                model,\n",
        "                state_dict,\n",
        "                checkpoint_files,\n",
        "                pretrained_model_name_or_path,\n",
        "                ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
        "                sharded_metadata=sharded_metadata,\n",
        "                device_map=device_map,\n",
        "                disk_offload_folder=offload_folder,\n",
        "                offload_state_dict=offload_state_dict,\n",
        "                dtype=torch_dtype,\n",
        "                hf_quantizer=hf_quantizer,\n",
        "                keep_in_fp32_regex=keep_in_fp32_regex,\n",
        "                device_mesh=device_mesh,\n",
        "                key_mapping=key_mapping,\n",
        "                weights_only=weights_only,\n",
        "            )\n",
        "\n",
        "        # record tp degree the model sharded to\n",
        "        model._tp_size = tp_size\n",
        "        model._device_mesh = device_mesh\n",
        "\n",
        "        # make sure token embedding weights are still tied if needed\n",
        "        model.tie_weights()\n",
        "\n",
        "        # Set model in evaluation mode to deactivate DropOut modules by default\n",
        "        model.eval()\n",
        "\n",
        "        # check if using kernels\n",
        "        if use_kernels:\n",
        "            from kernels import Device, kernelize\n",
        "\n",
        "            kernelize(model, device=Device(type=model.device.type))\n",
        "\n",
        "        # If it is a model with generation capabilities, attempt to load generation files (generation config,\n",
        "        # custom generate function)\n",
        "        if model.can_generate() and generation_config is not None:\n",
        "            #logger.info(\"The user-defined `generation_config` will be used to override the default generation config.\")\n",
        "            model.generation_config = model.generation_config.from_dict(generation_config.to_dict())\n",
        "        elif model.can_generate() and pretrained_model_name_or_path is not None:\n",
        "            repo_loading_kwargs = {\n",
        "                \"cache_dir\": cache_dir,\n",
        "                \"force_download\": force_download,\n",
        "                \"proxies\": proxies,\n",
        "                \"local_files_only\": local_files_only,\n",
        "                \"token\": token,\n",
        "                \"revision\": revision,\n",
        "                \"subfolder\": subfolder,\n",
        "                **kwargs,\n",
        "            }\n",
        "            # Load generation config\n",
        "            try:\n",
        "                model.generation_config = GenerationConfig.from_pretrained(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    _from_auto=from_auto_class,\n",
        "                    _from_pipeline=from_pipeline,\n",
        "                    **repo_loading_kwargs,\n",
        "                )\n",
        "            except OSError:\n",
        "                #logger.info(\n",
        "                    #\"Generation config file not found, using a generation config created from the model config.\"\n",
        "                #)\n",
        "                pass\n",
        "            # Load custom generate function if `pretrained_model_name_or_path` defines it (and override `generate`)\n",
        "            if hasattr(model, \"load_custom_generate\"):\n",
        "                try:\n",
        "                    custom_generate = model.load_custom_generate(\n",
        "                        pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **repo_loading_kwargs\n",
        "                    )\n",
        "                    model.generate = functools.partial(custom_generate, model=model)\n",
        "                except OSError:  # there is no custom generate function\n",
        "                    pass\n",
        "\n",
        "        # Dispatch model with hooks on all devices if necessary (not needed with a tp_plan, so we skip it as it slightly\n",
        "        # harm performances)\n",
        "        if device_map is not None and device_mesh is None:\n",
        "            device_map_kwargs = {\n",
        "                \"device_map\": device_map,\n",
        "                \"offload_dir\": offload_folder,\n",
        "                \"offload_index\": offload_index,\n",
        "                \"offload_buffers\": offload_buffers,\n",
        "            }\n",
        "            if \"skip_keys\" in inspect.signature(dispatch_model).parameters:\n",
        "                device_map_kwargs[\"skip_keys\"] = model._skip_keys_device_placement\n",
        "            # For HQQ method we force-set the hooks for single GPU envs\n",
        "            if (\n",
        "                \"force_hooks\" in inspect.signature(dispatch_model).parameters\n",
        "                and hf_quantizer is not None\n",
        "                and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ\n",
        "            ):\n",
        "                device_map_kwargs[\"force_hooks\"] = True\n",
        "            if (\n",
        "                hf_quantizer is not None\n",
        "                and hf_quantizer.quantization_config.quant_method == QuantizationMethod.FBGEMM_FP8\n",
        "                and isinstance(device_map, dict)\n",
        "                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n",
        "            ):\n",
        "                device_map_kwargs[\"offload_buffers\"] = True\n",
        "\n",
        "            if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n",
        "                dispatch_model(model, **device_map_kwargs)\n",
        "\n",
        "        if hf_quantizer is not None:\n",
        "            hf_quantizer.postprocess_model(model, config=config)\n",
        "            model.hf_quantizer = hf_quantizer\n",
        "\n",
        "        if _adapter_model_path is not None:\n",
        "            adapter_kwargs[\"key_mapping\"] = key_mapping\n",
        "            model.load_adapter(\n",
        "                _adapter_model_path,\n",
        "                adapter_name=adapter_name,\n",
        "                token=token,\n",
        "                adapter_kwargs=adapter_kwargs,\n",
        "            )\n",
        "\n",
        "        if output_loading_info:\n",
        "            if from_pt:\n",
        "                loading_info = {\n",
        "                    \"missing_keys\": missing_keys,\n",
        "                    \"unexpected_keys\": unexpected_keys,\n",
        "                    \"mismatched_keys\": mismatched_keys,\n",
        "                    \"error_msgs\": error_msgs,\n",
        "                }\n",
        "            elif from_flax:\n",
        "                loading_info = None\n",
        "            return model, loading_info\n",
        "        return model\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n",
        "        if not dtype.is_floating_point:\n",
        "            raise ValueError(\n",
        "                f\"Can't instantiate {cls.__name__} model under dtype={dtype} since it is not a floating point dtype\"\n",
        "            )\n",
        "\n",
        "        #logger.info(f\"Instantiating {cls.__name__} model under default dtype {dtype}.\")\n",
        "        dtype_orig = torch.get_default_dtype()\n",
        "        torch.set_default_dtype(dtype)\n",
        "        return dtype_orig\n",
        "\n",
        "    @classmethod\n",
        "    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n",
        "        if is_deepspeed_zero3_enabled():\n",
        "            import deepspeed\n",
        "\n",
        "            init_contexts = [no_init_weights()]\n",
        "            # We cannot initialize the model on meta device with deepspeed when not quantized\n",
        "            if not is_quantized and not _is_ds_init_called:\n",
        "                #logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n",
        "                init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n",
        "            elif is_quantized:\n",
        "                init_contexts.extend([init_empty_weights(), set_quantized_state()])\n",
        "        else:\n",
        "            init_contexts = [no_init_weights(), init_empty_weights()]\n",
        "\n",
        "        return init_contexts\n",
        "\n",
        "    def _sdpa_can_dispatch(self, hard_check_only: bool = False) -> bool:\n",
        "        if hard_check_only:\n",
        "            if not self._supports_sdpa:\n",
        "                raise ValueError(\n",
        "                    f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n",
        "                    \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n",
        "                    ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n",
        "                )\n",
        "            if not is_torch_sdpa_available():\n",
        "                raise ImportError(\n",
        "                    \"PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.1.1.\"\n",
        "                )\n",
        "\n",
        "        if (\n",
        "            torch.version.hip is not None\n",
        "            and torch.cuda.device_count() > 1\n",
        "            and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n",
        "        ):\n",
        "            logger.warning_once(\n",
        "                \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n",
        "            )\n",
        "            torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n",
        "        if not is_torch_sdpa_available() or not self._supports_sdpa or _is_bettertransformer:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    @classmethod\n",
        "    def can_generate(cls) -> bool:\n",
        "        if \"GenerationMixin\" in str(cls.__bases__):\n",
        "            return True\n",
        "        for base in cls.__bases__:\n",
        "            if not hasattr(base, \"can_generate\"):\n",
        "                continue\n",
        "            if \"PreTrainedModel\" not in str(base) and base.can_generate():\n",
        "                return True\n",
        "\n",
        "        if hasattr(cls, \"prepare_inputs_for_generation\"):  # implicit: doesn't inherit `GenerationMixin`\n",
        "            logger.warning(\n",
        "                f\"{cls.__name__} has generative capabilities, as `prepare_inputs_for_generation` is explicitly \"\n",
        "                \"defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, \"\n",
        "                \"`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability \"\n",
        "                \"to call `generate` and other related functions.\"\n",
        "                \"\\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the \"\n",
        "                \"model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\"\n",
        "                \"\\n  - If you are the owner of the model architecture code, please modify your model class such that \"\n",
        "                \"it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\"\n",
        "                \"\\n  - If you are not the owner of the model architecture class, please contact the model code owner \"\n",
        "                \"to update it.\"\n",
        "            )\n",
        "\n",
        "        return False\n",
        "\n",
        "    def post_init(self):\n",
        "        self.init_weights()\n",
        "        self._backward_compatibility_gradient_checkpointing()\n",
        "\n",
        "        # Make sure the modules correctly exist if the flag is active\n",
        "        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n",
        "            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n",
        "            unique_module_names = set()\n",
        "            # Get all unique module names in the module graph, without the prefixes\n",
        "            for param in all_parameters:\n",
        "                unique_module_names.update(\n",
        "                    [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n",
        "                )\n",
        "            # Check that every module in the keep_in_fp32 list is part of the module graph\n",
        "            if self._keep_in_fp32_modules is not None:\n",
        "                for module in self._keep_in_fp32_modules:\n",
        "                    if module not in unique_module_names:\n",
        "                        raise ValueError(\n",
        "                            f\"{module} was specified in the `_keep_in_fp32_modules` list, but is not part of the modules in\"\n",
        "                            f\" {self.__class__.__name__}\"\n",
        "                        )\n",
        "\n",
        "            if self._keep_in_fp32_modules_strict is not None:\n",
        "                for module in self._keep_in_fp32_modules_strict:\n",
        "                    if module not in unique_module_names:\n",
        "                        raise ValueError(\n",
        "                            f\"{module} was specified in the `_keep_in_fp32_modules_strict` list, but is not part of the modules in\"\n",
        "                            f\" {self.__class__.__name__}\"\n",
        "                        )\n",
        "\n",
        "        # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n",
        "        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n",
        "        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n",
        "        for name, module in self.named_children():\n",
        "            if plan := getattr(module, \"_tp_plan\", None):\n",
        "                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n",
        "\n",
        "        if self._tp_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n",
        "            for v in self._tp_plan.values():\n",
        "                if v not in ALL_PARALLEL_STYLES:\n",
        "                    raise ValueError(\n",
        "                        f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\"\n",
        "                    )\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        pass\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Prune heads if needed\n",
        "        if self.config.pruned_heads:\n",
        "            self.prune_heads(self.config.pruned_heads)\n",
        "\n",
        "        if _init_weights:\n",
        "            # Initialize weights\n",
        "            self.initialize_weights()\n",
        "\n",
        "            # Tie weights should be skipped when not initializing all weights\n",
        "            # since from_pretrained(...) calls tie weights anyways\n",
        "            self.tie_weights()\n",
        "\n",
        "    def _backward_compatibility_gradient_checkpointing(self):\n",
        "        if self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\n",
        "            self.gradient_checkpointing_enable()\n",
        "            # Remove the attribute now that is has been consumed, so it's no saved in the config.\n",
        "            delattr(self.config, \"gradient_checkpointing\")\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n",
        "            output_embeddings = self.get_output_embeddings()\n",
        "            if output_embeddings is not None:\n",
        "                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
        "\n",
        "        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n",
        "            if hasattr(self, self.base_model_prefix):\n",
        "                self = getattr(self, self.base_model_prefix)\n",
        "            tied_weights = self._tie_encoder_decoder_weights(\n",
        "                self.encoder, self.decoder, self.base_model_prefix, \"encoder\"\n",
        "            )\n",
        "            self._dynamic_tied_weights_keys = tied_weights\n",
        "\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, \"_tie_weights\"):\n",
        "                module._tie_weights()\n",
        "\n",
        "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
        "        if self.config.torchscript:\n",
        "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
        "        else:\n",
        "            output_embeddings.weight = input_embeddings.weight\n",
        "\n",
        "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
        "            output_embeddings.bias.data = nn.functional.pad(\n",
        "                output_embeddings.bias.data,\n",
        "                (\n",
        "                    0,\n",
        "                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
        "                ),\n",
        "                \"constant\",\n",
        "                0,\n",
        "            )\n",
        "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
        "            output_embeddings.out_features = input_embeddings.num_embeddings\n",
        "\n",
        "    @classmethod\n",
        "    def _load_pretrained_model(\n",
        "        cls,\n",
        "        model: \"PreTrainedModel\",\n",
        "        state_dict: Optional[dict],\n",
        "        checkpoint_files: Optional[list[str]],\n",
        "        pretrained_model_name_or_path: Optional[str],\n",
        "        ignore_mismatched_sizes: bool = False,\n",
        "        sharded_metadata: Optional[dict] = None,\n",
        "        device_map: Optional[dict] = None,\n",
        "        disk_offload_folder: Optional[str] = None,\n",
        "        offload_state_dict: Optional[bool] = None,\n",
        "        dtype: Optional[torch.dtype] = None,\n",
        "        hf_quantizer: Optional[HfQuantizer] = None,\n",
        "        keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "        device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        weights_only: bool = True,\n",
        "    ):\n",
        "        # Useful flags\n",
        "        is_quantized = hf_quantizer is not None\n",
        "        is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "            QuantizationMethod.HQQ,\n",
        "            QuantizationMethod.QUARK,\n",
        "        }\n",
        "        is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "            QuantizationMethod.HQQ,\n",
        "            QuantizationMethod.BITS_AND_BYTES,\n",
        "        }\n",
        "\n",
        "        # Get all the keys of the state dicts that we have to initialize the model\n",
        "        if sharded_metadata is not None:\n",
        "            original_checkpoint_keys = sharded_metadata[\"all_checkpoint_keys\"]\n",
        "        elif state_dict is not None:\n",
        "            original_checkpoint_keys = list(state_dict.keys())\n",
        "        else:\n",
        "            original_checkpoint_keys = list(\n",
        "                load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
        "            )\n",
        "\n",
        "        # Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\n",
        "        prefix = model.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "        has_prefix_module = any(s.startswith(prefix) for s in original_checkpoint_keys) if len(prefix) > 0 else False\n",
        "        expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n",
        "        loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n",
        "        loading_base_model_from_task_state_dict = has_prefix_module and not expects_prefix_module\n",
        "\n",
        "        # Find the key names that the model expects from the serialized keys\n",
        "        key_renaming_mapping = model._get_key_renaming_mapping(\n",
        "            original_checkpoint_keys,\n",
        "            key_mapping,\n",
        "            loading_base_model_from_task_state_dict,\n",
        "            loading_task_model_from_base_state_dict,\n",
        "        )\n",
        "        checkpoint_keys = list(key_renaming_mapping.values())\n",
        "\n",
        "        # Find missing and unexpected keys from the state dict\n",
        "        missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(\n",
        "            cls,\n",
        "            model,\n",
        "            original_checkpoint_keys,\n",
        "            checkpoint_keys,\n",
        "            loading_base_model_from_task_state_dict,\n",
        "            hf_quantizer,\n",
        "            device_map,\n",
        "        )\n",
        "        # Find all the keys with shape mismatch (if we ignore the mismatch, the weights need to be newly initialized the\n",
        "        # same way as missing keys)\n",
        "        mismatched_keys, mismatched_shapes = _find_mismatched_keys(\n",
        "            model,\n",
        "            state_dict,\n",
        "            checkpoint_files,\n",
        "            ignore_mismatched_sizes,\n",
        "            key_renaming_mapping,\n",
        "            is_quantized,\n",
        "            weights_only,\n",
        "        )\n",
        "\n",
        "        # We need to update both the mapping and the list of checkpoint keys to remove the mismatched ones\n",
        "        key_renaming_mapping = {k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys}\n",
        "        checkpoint_keys = list(key_renaming_mapping.values())\n",
        "\n",
        "        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n",
        "        # loading the weights as they are not in the loaded state dict)\n",
        "        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, unexpected_keys, dtype, hf_quantizer)\n",
        "\n",
        "        # correctly initialize the missing (and potentially mismatched) keys\n",
        "        model._initialize_missing_keys(checkpoint_keys, ignore_mismatched_sizes, is_quantized)\n",
        "\n",
        "        # Set some modules to fp32 if needed\n",
        "        if keep_in_fp32_regex is not None:\n",
        "            for name, param in model.named_parameters():\n",
        "                if keep_in_fp32_regex.search(name):\n",
        "                    # param = param.to(torch.float32) does not work here as only in the local scope.\n",
        "                    param.data = param.data.to(torch.float32)\n",
        "\n",
        "        # Make sure we are able to load base models as well as derived models (specific task models, with heads)\n",
        "        model_to_load = model\n",
        "        # In this case, we load a ForTaskModel with keys from a BaseModel -> only load keys to the BaseModel\n",
        "        if loading_task_model_from_base_state_dict:\n",
        "            model_to_load = getattr(model, prefix)\n",
        "            # Here we need to remove the prefix we added to correctly find missing/unexpected keys, as we will load\n",
        "            # in the submodule\n",
        "            key_renaming_mapping = {k: v[len(_prefix) :] for k, v in key_renaming_mapping.items()}\n",
        "            checkpoint_keys = list(key_renaming_mapping.values())\n",
        "            # We need to update the device map as well\n",
        "            if device_map is not None:\n",
        "                device_map = {k[len(_prefix) :] if k.startswith(_prefix) else k: v for k, v in device_map.items()}\n",
        "            # small sanity check: the base model should not contain task-specific head keys\n",
        "            task_specific_expected_keys = [s for s in model.state_dict().keys() if not s.startswith(_prefix)]\n",
        "            base_model_expected_keys = list(model_to_load.state_dict().keys())\n",
        "            if any(\n",
        "                key in task_specific_expected_keys and key not in base_model_expected_keys for key in checkpoint_keys\n",
        "            ):\n",
        "                raise ValueError(\n",
        "                    \"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\n",
        "                    \"properly saved?\"\n",
        "                )\n",
        "\n",
        "        # Get reverse key mapping\n",
        "        reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n",
        "\n",
        "        is_offloaded_safetensors = False\n",
        "        # This offload index if for params explicitly on the \"disk\" in the device_map\n",
        "        disk_offload_index = None\n",
        "        disk_only_shard_files = []\n",
        "        # Prepare parameters offloading if needed\n",
        "        if device_map is not None and \"disk\" in device_map.values():\n",
        "            if offload_state_dict is None:\n",
        "                offload_state_dict = True\n",
        "            if disk_offload_folder is not None:\n",
        "                os.makedirs(disk_offload_folder, exist_ok=True)\n",
        "            is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n",
        "            if disk_offload_folder is None and not is_offloaded_safetensors:\n",
        "                raise ValueError(\n",
        "                    \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\"\n",
        "                    \" for them. Alternatively, make sure you have `safetensors` installed if the model you are using\"\n",
        "                    \" offers the weights in this format.\"\n",
        "                )\n",
        "            if is_offloaded_safetensors:\n",
        "                param_device_map = expand_device_map(device_map, checkpoint_keys)\n",
        "                str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n",
        "                if sharded_metadata is None:\n",
        "                    weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n",
        "                else:\n",
        "                    folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n",
        "                    # Fix the weight map keys according to the key mapping\n",
        "                    weight_map = {\n",
        "                        key_renaming_mapping[k]: v\n",
        "                        for k, v in sharded_metadata[\"weight_map\"].items()\n",
        "                        if k in key_renaming_mapping\n",
        "                    }\n",
        "                    weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n",
        "                    # Find potential checkpoints containing only offloaded weights\n",
        "                    disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n",
        "                disk_offload_index = {\n",
        "                    name: {\n",
        "                        \"safetensors_file\": file,\n",
        "                        \"weight_name\": reverse_key_renaming_mapping[name],\n",
        "                        \"dtype\": str_dtype,\n",
        "                    }\n",
        "                    for name, file in weight_map.items()\n",
        "                    if param_device_map[name] == \"disk\"\n",
        "                }\n",
        "            else:\n",
        "                disk_offload_index = {}\n",
        "\n",
        "        # This offload index if for params that are supposed to be on the \"cpu\", either with or without a device_map\n",
        "        # It allows to load parameters one-by-one from the state dict, avoiding a memory peak of 2 x state_dict_size,\n",
        "        # i.e. 1x to load it, and 1x to copy it to model\n",
        "        cpu_offload_folder = None\n",
        "        cpu_offload_index = None\n",
        "        if offload_state_dict:\n",
        "            cpu_offload_folder = tempfile.mkdtemp()\n",
        "            cpu_offload_index = {}\n",
        "\n",
        "        # To be able to iterate, even if we don't use it if the state_dict is already provided\n",
        "        elif state_dict is not None:\n",
        "            checkpoint_files = [\"\"]\n",
        "\n",
        "        # Compute expected model keys\n",
        "        expected_keys = list(model_to_load.state_dict().keys())\n",
        "        if hf_quantizer is not None:\n",
        "            expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n",
        "\n",
        "        #if logger.level >= logging.WARNING:\n",
        "            #verify_tp_plan(expected_keys, getattr(model_to_load, \"_tp_plan\", None))\n",
        "\n",
        "        # Warmup cuda to load the weights much faster on devices\n",
        "        if device_map is not None and not is_hqq_or_quark:\n",
        "            expanded_device_map = expand_device_map(device_map, expected_keys)\n",
        "            caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)\n",
        "\n",
        "        # Prepare and compatabilize arguments for serial and parallel shard loading\n",
        "        args_list = [\n",
        "            (\n",
        "                shard_file,\n",
        "                state_dict,\n",
        "                disk_only_shard_files,\n",
        "                is_hqq_or_bnb,\n",
        "                is_quantized,\n",
        "                device_map,\n",
        "                hf_quantizer,\n",
        "                key_renaming_mapping,\n",
        "                weights_only,\n",
        "                model_to_load,\n",
        "                expected_keys,\n",
        "                reverse_key_renaming_mapping,\n",
        "                disk_offload_folder,\n",
        "                disk_offload_index,\n",
        "                cpu_offload_folder,\n",
        "                cpu_offload_index,\n",
        "                is_offloaded_safetensors,\n",
        "                keep_in_fp32_regex,\n",
        "                unexpected_keys,\n",
        "                device_mesh,\n",
        "            )\n",
        "            for shard_file in checkpoint_files\n",
        "        ]\n",
        "\n",
        "        error_msgs = []\n",
        "\n",
        "        if (\n",
        "            os.environ.get(\"HF_ENABLE_PARALLEL_LOADING\", \"\").upper() in ENV_VARS_TRUE_VALUES\n",
        "            and not is_deepspeed_zero3_enabled()\n",
        "        ):\n",
        "            _error_msgs, disk_offload_index, cpu_offload_index = load_shard_files_with_threadpool(args_list)\n",
        "            error_msgs += _error_msgs\n",
        "        else:\n",
        "            if len(args_list) > 1:\n",
        "                args_list = logging.tqdm(args_list, desc=\"Loading checkpoint shards\")\n",
        "\n",
        "            for args in args_list:\n",
        "                _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n",
        "                error_msgs += _error_msgs\n",
        "\n",
        "        if disk_offload_index is not None and len(disk_offload_index) > 0:\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                prefix = cls.base_model_prefix\n",
        "                if not is_offloaded_safetensors:\n",
        "                    for weight_name in disk_offload_index:\n",
        "                        shutil.move(\n",
        "                            os.path.join(disk_offload_folder, f\"{weight_name}.dat\"),\n",
        "                            os.path.join(disk_offload_folder, f\"{prefix}.{weight_name}.dat\"),\n",
        "                        )\n",
        "                disk_offload_index = {f\"{prefix}.{key}\": value for key, value in disk_offload_index.items()}\n",
        "            if not is_offloaded_safetensors:\n",
        "                save_offload_index(disk_offload_index, disk_offload_folder)\n",
        "                disk_offload_index = None\n",
        "        if offload_state_dict:\n",
        "            load_offloaded_weights(model_to_load, cpu_offload_index, cpu_offload_folder)\n",
        "            shutil.rmtree(cpu_offload_folder)\n",
        "\n",
        "        if hf_quantizer is not None:\n",
        "            missing_keys = hf_quantizer.update_missing_keys_after_loading(model_to_load, missing_keys, prefix)\n",
        "\n",
        "        if device_mesh is not None:\n",
        "            tp_device = list(device_map.values())[0]\n",
        "            for buffer in model.buffers():\n",
        "                if buffer.device != tp_device:\n",
        "                    buffer.data = buffer.to(tp_device)\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                parameters_to_initialize = {\n",
        "                    name: param for name, param in model.named_parameters() if not name.startswith(prefix)\n",
        "                }\n",
        "                for name, param in parameters_to_initialize.items():\n",
        "                    # If it is still on meta here, it means that it's a tied weight that will be tied later anyway -> skip it\n",
        "                    if param.device.type == \"meta\":\n",
        "                        continue\n",
        "                    # Shard the param\n",
        "                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param, keep_in_fp32_regex)\n",
        "                    shard_and_distribute_module(\n",
        "                        model,\n",
        "                        param.to(tp_device),\n",
        "                        param,\n",
        "                        name,\n",
        "                        casting_dtype,\n",
        "                        to_contiguous,\n",
        "                        device_mesh.get_local_rank(),\n",
        "                        device_mesh,\n",
        "                    )\n",
        "\n",
        "        # All potential warnings/infos\n",
        "        if len(error_msgs) > 0:\n",
        "            error_msg = \"\\n\\t\".join(error_msgs)\n",
        "            if \"size mismatch\" in error_msg:\n",
        "                error_msg += (\n",
        "                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n",
        "                )\n",
        "            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n",
        "\n",
        "\n",
        "        if len(unexpected_keys) > 0:\n",
        "            archs = [] if model.config.architectures is None else model.config.architectures\n",
        "            warner = logger.warning if model.__class__.__name__ in archs else logger.info\n",
        "            warner(\n",
        "                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n",
        "                f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n",
        "                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n",
        "                \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n",
        "                \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n",
        "                f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n",
        "                \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n",
        "            )\n",
        "        else:\n",
        "            logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.warning(\n",
        "                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n",
        "                f\" {pretrained_model_name_or_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n",
        "                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n",
        "            )\n",
        "        elif len(mismatched_keys) == 0:\n",
        "            logger.info(\n",
        "                f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n",
        "                f\" {pretrained_model_name_or_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n",
        "                f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n",
        "                \" training.\"\n",
        "            )\n",
        "        if len(mismatched_keys) > 0:\n",
        "            mismatched_warning = \"\\n\".join(\n",
        "                [\n",
        "                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n",
        "                    for key, (shape1, shape2) in zip(mismatched_keys, mismatched_shapes)\n",
        "                ]\n",
        "            )\n",
        "            logger.warning(\n",
        "                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n",
        "                f\" {pretrained_model_name_or_path} and are newly initialized because the shapes did not\"\n",
        "                f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n",
        "                \" to use it for predictions and inference.\"\n",
        "            )\n",
        "\n",
        "\n",
        "        return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n",
        "\n",
        "    def _get_key_renaming_mapping(\n",
        "        self,\n",
        "        checkpoint_keys: list[str],\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        loading_base_model_from_task_state_dict: bool = False,\n",
        "        loading_task_model_from_base_state_dict: bool = False,\n",
        "    ):\n",
        "        prefix = self.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "\n",
        "        renamed_keys = {}\n",
        "        key_renaming_mapping = {}\n",
        "        for key in checkpoint_keys:\n",
        "            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n",
        "\n",
        "            if key_mapping is not None:\n",
        "                for pattern, replacement in key_mapping.items():\n",
        "                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n",
        "                    if n_replace > 0:\n",
        "                        has_changed = True\n",
        "                        break\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                new_key = \".\".join([prefix, new_key])\n",
        "            elif loading_base_model_from_task_state_dict:\n",
        "                if not new_key.startswith(_prefix):\n",
        "                    continue\n",
        "                new_key = new_key[len(_prefix) :]\n",
        "\n",
        "            key_renaming_mapping[key] = new_key\n",
        "\n",
        "            if has_changed:\n",
        "                if key.endswith(\"LayerNorm.gamma\"):\n",
        "                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n",
        "                elif key.endswith(\"LayerNorm.beta\"):\n",
        "                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n",
        "\n",
        "\n",
        "        if renamed_keys:\n",
        "            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n",
        "            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n",
        "            for old_key, new_key in renamed_keys.values():\n",
        "                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n",
        "            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n",
        "            #logger.info_once(warning_msg)\n",
        "\n",
        "\n",
        "        return key_renaming_mapping\n",
        "\n",
        "    @staticmethod\n",
        "    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:\n",
        "        \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n",
        "        # Rename LayerNorm beta & gamma params for some early models ported from Tensorflow (e.g. Bert)\n",
        "        # This rename is logged.\n",
        "        if key.endswith(\"LayerNorm.beta\"):\n",
        "            return key.replace(\"LayerNorm.beta\", \"LayerNorm.bias\"), True\n",
        "        if key.endswith(\"LayerNorm.gamma\"):\n",
        "            return key.replace(\"LayerNorm.gamma\", \"LayerNorm.weight\"), True\n",
        "\n",
        "        # Rename weight norm parametrizations to match changes across torch versions.\n",
        "        # Impacts a number of speech/wav2vec models. e.g. Hubert, Wav2Vec2, and others.\n",
        "        # This rename is not logged.\n",
        "        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n",
        "            if key.endswith(\"weight_g\"):\n",
        "                return key.replace(\"weight_g\", \"parametrizations.weight.original0\"), True\n",
        "            if key.endswith(\"weight_v\"):\n",
        "                return key.replace(\"weight_v\", \"parametrizations.weight.original1\"), True\n",
        "        else:\n",
        "            if key.endswith(\"parametrizations.weight.original0\"):\n",
        "                return key.replace(\"parametrizations.weight.original0\", \"weight_g\"), True\n",
        "            if key.endswith(\"parametrizations.weight.original1\"):\n",
        "                return key.replace(\"parametrizations.weight.original1\", \"weight_v\"), True\n",
        "\n",
        "        return key, False\n",
        "\n",
        "    def _move_missing_keys_from_meta_to_cpu(\n",
        "        self,\n",
        "        missing_keys: list[str],\n",
        "        unexpected_keys: list[str],\n",
        "        dtype: Optional[torch.dtype],\n",
        "        hf_quantizer: Optional[HfQuantizer],\n",
        "    ) -> \"PreTrainedModel\":\n",
        "        \"\"\"Move the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts) back\n",
        "        from meta device to cpu.\n",
        "        \"\"\"\n",
        "        is_quantized = hf_quantizer is not None\n",
        "\n",
        "        # In this case we need to move everything back\n",
        "        if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n",
        "            # We only do it for the parameters, as the buffers are not initialized on the meta device by default\n",
        "            for key, param in self.named_parameters():\n",
        "                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n",
        "                _load_parameter_into_model(self, key, value)\n",
        "            return\n",
        "\n",
        "        model_state_dict = self.state_dict()\n",
        "        for key in missing_keys:\n",
        "            param = model_state_dict[key]\n",
        "            # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n",
        "            if param.device == torch.device(\"meta\"):\n",
        "                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n",
        "                if (\n",
        "                    not is_quantized\n",
        "                    or (getattr(hf_quantizer, \"requires_parameters_quantization\", False))\n",
        "                    or not hf_quantizer.check_quantized_param(self, param_value=value, param_name=key, state_dict={})\n",
        "                ):\n",
        "                    _load_parameter_into_model(self, key, value)\n",
        "                else:\n",
        "                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\", model_state_dict, unexpected_keys)\n",
        "\n",
        "    def _initialize_missing_keys(\n",
        "        self,\n",
        "        loaded_keys: list[str],\n",
        "        ignore_mismatched_sizes: bool,\n",
        "        is_quantized: bool,\n",
        "    ) -> \"PreTrainedModel\":\n",
        "        if not ignore_mismatched_sizes:\n",
        "            not_initialized_submodules = set_initialized_submodules(self, loaded_keys)\n",
        "            if (\n",
        "                hasattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\")\n",
        "                and self.config.get_text_config(decoder=True).tie_word_embeddings\n",
        "            ):\n",
        "                output_embeddings = self.get_output_embeddings()\n",
        "                if output_embeddings is not None:\n",
        "                    if not hasattr(output_embeddings, \"bias\") or output_embeddings.bias is None:\n",
        "                        output_embeddings._is_hf_initialized = True\n",
        "        else:\n",
        "            not_initialized_submodules = dict(self.named_modules())\n",
        "        if is_deepspeed_zero3_enabled() and not is_quantized:\n",
        "            import deepspeed\n",
        "\n",
        "            not_initialized_parameters = list(\n",
        "                set(\n",
        "                    itertools.chain.from_iterable(\n",
        "                        submodule.parameters(recurse=False) for submodule in not_initialized_submodules.values()\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n",
        "                self.initialize_weights()\n",
        "        else:\n",
        "            self.initialize_weights()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def initialize_weights(self):\n",
        "        if not hasattr(torch.nn.Module, \"smart_apply\"):\n",
        "            def smart_apply(self, fn):\n",
        "                for module in self.children():\n",
        "                    if isinstance(module, PreTrainedModel):\n",
        "                        module.smart_apply(module._initialize_weights)\n",
        "                    else:\n",
        "                        module.smart_apply(fn)\n",
        "                fn(self)\n",
        "                return self\n",
        "\n",
        "            torch.nn.Module.smart_apply = smart_apply\n",
        "\n",
        "        self.smart_apply(self._initialize_weights)\n",
        "\n",
        "    def _initialize_weights(self, module):\n",
        "        if getattr(module, \"_is_hf_initialized\", False):\n",
        "            return\n",
        "        self._init_weights(module)\n",
        "        module._is_hf_initialized = True\n",
        "\n",
        "    def get_parameter_or_buffer(self, target: str):\n",
        "        try:\n",
        "            return self.get_parameter(target)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        try:\n",
        "            return self.get_buffer(target)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        module, param_name = get_module_from_name(self, target)\n",
        "        if (\n",
        "            param_name == \"_extra_state\"\n",
        "            and getattr(module.__class__, \"get_extra_state\", torch.nn.Module.get_extra_state)\n",
        "            is not torch.nn.Module.get_extra_state\n",
        "        ):\n",
        "            return module.get_extra_state()\n",
        "\n",
        "        raise AttributeError(f\"`{target}` is neither a parameter, buffer, nor extra state.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "xzvgfrXiAjc8"
      },
      "outputs": [],
      "source": [
        "tokens_per_layer = [[] for _ in range(28)]\n",
        "\n",
        "class DirectionalMaskedLinear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None, threshold=0.3):\n",
        "        super().__init__(in_features, out_features, bias=bias, device=device, dtype=dtype)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        # 通常の線形演算\n",
        "        output = input @ self.weight.T\n",
        "        if self.bias is not None:\n",
        "            output += self.bias\n",
        "        return output\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
        "    cos = cos.unsqueeze(unsqueeze_dim)\n",
        "    sin = sin.unsqueeze(unsqueeze_dim)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "def sdpa_attention_forward(\n",
        "    module: torch.nn.Module,\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    dropout: float = 0.0,\n",
        "    scaling: Optional[float] = None,\n",
        "    is_causal: Optional[bool] = None,\n",
        "    **kwargs,\n",
        ") -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "    #if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n",
        "        #logger.warning_once(\n",
        "            #\"`sdpa` attention does not support `output_attentions=True` or `head_mask`.\"\n",
        "            #\" Please set your attention to `eager` if you want any of these features.\"\n",
        "        #)\n",
        "\n",
        "    if hasattr(module, \"num_key_value_groups\"):\n",
        "        key = repeat_kv(key, module.num_key_value_groups)\n",
        "        value = repeat_kv(value, module.num_key_value_groups)\n",
        "\n",
        "    if attention_mask is not None and attention_mask.ndim == 4:\n",
        "        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
        "\n",
        "    query = query.contiguous()\n",
        "    key = key.contiguous()\n",
        "    value = value.contiguous()\n",
        "\n",
        "    if is_causal is None:\n",
        "        is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n",
        "\n",
        "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
        "        is_causal = is_causal.item()\n",
        "\n",
        "    attn_output = scaled_dot_product_attention(\n",
        "        query,\n",
        "        key,\n",
        "        value,\n",
        "        attn_mask=attention_mask,\n",
        "        dropout_p=dropout,\n",
        "        scale=scaling,\n",
        "        is_causal=is_causal,\n",
        "    )\n",
        "\n",
        "    attn_weights = None\n",
        "\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "    return attn_output, attn_weights\n",
        "\n",
        "class CustomQwen2Attention(nn.Module):\n",
        "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "        self.attention_dropout = config.attention_dropout\n",
        "        self.is_causal = True\n",
        "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n",
        "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n",
        "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n",
        "        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n",
        "        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
        "        attention_mask: Optional[torch.Tensor],\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n",
        "        input_shape = hidden_states.shape[:-1]\n",
        "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
        "\n",
        "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "\n",
        "        cos, sin = position_embeddings\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
        "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "        attn_output, attn_weights = sdpa_attention_forward(\n",
        "            self,\n",
        "            query_states,\n",
        "            key_states,\n",
        "            value_states,\n",
        "            attention_mask,\n",
        "            dropout=0.0 if not self.training else self.attention_dropout,\n",
        "            scaling=self.scaling,\n",
        "            sliding_window=self.sliding_window,\n",
        "            is_causal=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "class CustomQwen2RMSNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "        return self.weight * hidden_states.to(input_dtype)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n",
        "\n",
        "\n",
        "class CustomQwen2MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
        "        self.act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, x):\n",
        "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "        return down_proj\n",
        "\n",
        "class Qwen2DecoderLayer(GradientCheckpointingLayer):\n",
        "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        self.self_attn = CustomQwen2Attention(config=config, layer_idx=layer_idx)\n",
        "\n",
        "        self.mlp = CustomQwen2MLP(config)\n",
        "        self.input_layernorm = CustomQwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.post_attention_layernorm = CustomQwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.attention_type = config.layer_types[layer_idx]\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n",
        "        **kwargs,\n",
        "    ) -> tuple[torch.Tensor]:\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.input_layernorm(hidden_states)\n",
        "        hidden_states, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_value=past_key_value,\n",
        "            use_cache=use_cache,\n",
        "            cache_position=cache_position,\n",
        "            position_embeddings=position_embeddings,\n",
        "            **kwargs,\n",
        "        )\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        hidden_states = residual + hidden_states\n",
        "        return hidden_states\n",
        "\n",
        "class Qwen2PreTrainedModel(PreTrainedModel):\n",
        "    config_class = Qwen2Config\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [\"Qwen2DecoderLayer\"]\n",
        "    _skip_keys_device_placement = [\"past_key_values\"]\n",
        "    _supports_flash_attn_2 = True\n",
        "    _supports_flash_attn_3 = True\n",
        "    _supports_sdpa = True\n",
        "    _supports_flex_attn = True\n",
        "    _supports_cache_class = True\n",
        "    _supports_quantized_cache = True\n",
        "    _supports_static_cache = True\n",
        "    _supports_attention_backend = True\n",
        "    _can_record_outputs = {\n",
        "        \"hidden_states\": Qwen2DecoderLayer,\n",
        "        \"attentions\": CustomQwen2Attention,\n",
        "    }\n",
        "\n",
        "class Qwen2RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, config: Qwen2Config, device=None):\n",
        "        super().__init__()\n",
        "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
        "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
        "        else:\n",
        "            self.rope_type = \"default\"\n",
        "        self.max_seq_len_cached = config.max_position_embeddings\n",
        "        self.original_max_seq_len = config.max_position_embeddings\n",
        "\n",
        "        self.config = config\n",
        "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
        "\n",
        "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self.original_inv_freq = self.inv_freq\n",
        "\n",
        "    def forward(self, x, position_ids):\n",
        "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
        "        position_ids_expanded = position_ids[:, None, :].float()\n",
        "\n",
        "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
        "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
        "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1)\n",
        "            cos = emb.cos() * self.attention_scaling\n",
        "            sin = emb.sin() * self.attention_scaling\n",
        "\n",
        "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
        "\n",
        "\n",
        "class CustomQwen2Model(Qwen2PreTrainedModel):\n",
        "    def __init__(self, config: Qwen2Config):\n",
        "        super().__init__(config)\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "        )\n",
        "        self.norm = CustomQwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.rotary_emb = Qwen2RotaryEmbedding(config=config)\n",
        "        self.gradient_checkpointing = False\n",
        "        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> BaseModelOutputWithPast:\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
        "            mask_kwargs = {\n",
        "                \"config\": self.config,\n",
        "                \"input_embeds\": inputs_embeds,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"cache_position\": cache_position,\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"position_ids\": position_ids,\n",
        "            }\n",
        "\n",
        "            causal_mask_mapping = {\n",
        "                \"full_attention\": create_causal_mask(**mask_kwargs),\n",
        "            }\n",
        "\n",
        "        hidden_states = inputs_embeds\n",
        "\n",
        "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
        "\n",
        "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
        "            hidden_states = decoder_layer(\n",
        "                hidden_states,\n",
        "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
        "                position_ids=position_ids,\n",
        "                past_key_value=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                cache_position=cache_position,\n",
        "                position_embeddings=position_embeddings,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=past_key_values if use_cache else None,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "i1OFblecjuny"
      },
      "outputs": [],
      "source": [
        "class AttentionInterface(GeneralInterface):\n",
        "    _global_mapping = {\n",
        "        \"sdpa\": sdpa_attention_forward,\n",
        "    }\n",
        "\n",
        "ALL_ATTENTION_FUNCTIONS: AttentionInterface = AttentionInterface()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "_is_ds_init_called =  False\n",
        "_init_weights =  True\n",
        "_CAN_RECORD_REGISTRY = {}\n",
        "TORCH_INIT_FUNCTIONS = {}\n",
        "\n",
        "\"\"\"\n",
        "TORCH_INIT_FUNCTIONS = {\n",
        "    \"uniform_\" : nn.init.uniform,\n",
        "    \"normal_\" : nn.init.normal_,\n",
        "    \"trunc_normal_\" : nn.init.trunc_normal,\n",
        "    \"constant_\" : nn.init.constant,\n",
        "    \"xavier_uniform_\" : nn.init.xavier_uniform_,\n",
        "    \"xavier_normal_\" : nn.init.xavier_normal_,\n",
        "    \"kaiming_uniform_\" : nn.init.kaiming_uniform,\n",
        "    \"kaiming_normal_\" : nn.init.kaiming_normal_,\n",
        "    \"uniform\" : nn.init.uniform,\n",
        "    \"normal\" : nn.init.normal,\n",
        "    \"xavier_uniform\" : nn.init.xavier_uniform,\n",
        "    \"xavier_normal\" : nn.init.xavier_normal,\n",
        "    \"kaiming_uniform\": nn.init.kaiming_uniform,\n",
        "    \"kaiming_normal\": nn.init.kaiming_normal,\n",
        "}\n",
        "\"\"\"\n",
        "@contextmanager\n",
        "def no_init_weights():\n",
        "    global _init_weights\n",
        "    old_init_weights = _init_weights\n",
        "\n",
        "    _init_weights = False\n",
        "\n",
        "    def _skip_init(*args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    # Save the original initialization functions\n",
        "    for name, init_func in TORCH_INIT_FUNCTIONS.items():\n",
        "        setattr(torch.nn.init, name, _skip_init)\n",
        "\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        _init_weights = old_init_weights\n",
        "        # Restore the original initialization functions\n",
        "        for name, init_func in TORCH_INIT_FUNCTIONS.items():\n",
        "            setattr(torch.nn.init, name, init_func)\n",
        "\n",
        "def _get_torch_dtype(\n",
        "    cls,\n",
        "    torch_dtype: Optional[Union[str, torch.dtype, dict]],\n",
        "    checkpoint_files: Optional[list[str]],\n",
        "    config: PretrainedConfig,\n",
        "    sharded_metadata: Optional[dict],\n",
        "    state_dict: Optional[dict],\n",
        "    weights_only: bool,\n",
        ") -> tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n",
        "    dtype_orig = None\n",
        "    is_sharded = sharded_metadata is not None\n",
        "\n",
        "    if torch_dtype is not None:\n",
        "        if isinstance(torch_dtype, str):\n",
        "            if torch_dtype == \"auto\":\n",
        "                if hasattr(config, \"torch_dtype\") and config.torch_dtype is not None:\n",
        "                    torch_dtype = config.torch_dtype\n",
        "                    #logger.info(f\"Will use torch_dtype={torch_dtype} as defined in model's config object\")\n",
        "                else:\n",
        "                    if is_sharded and \"dtype\" in sharded_metadata:\n",
        "                        torch_dtype = sharded_metadata[\"dtype\"]\n",
        "                    elif state_dict is not None:\n",
        "                        torch_dtype = get_state_dict_dtype(state_dict)\n",
        "                    else:\n",
        "                        state_dict = load_state_dict(\n",
        "                            checkpoint_files[0], map_location=\"meta\", weights_only=weights_only\n",
        "                        )\n",
        "                        torch_dtype = get_state_dict_dtype(state_dict)\n",
        "                    #logger.info(\n",
        "                        #\"Since the `torch_dtype` attribute can't be found in model's config object, \"\n",
        "                        #\"will use torch_dtype={torch_dtype} as derived from model's weights\"\n",
        "                    #)\n",
        "            elif hasattr(torch, torch_dtype):\n",
        "                torch_dtype = getattr(torch, torch_dtype)\n",
        "                config.torch_dtype = torch_dtype\n",
        "                for sub_config_key in config.sub_configs.keys():\n",
        "                    sub_config = getattr(config, sub_config_key)\n",
        "                    sub_config.torch_dtype = torch_dtype\n",
        "        elif isinstance(torch_dtype, torch.dtype):\n",
        "            config.torch_dtype = torch_dtype\n",
        "            for sub_config_key in config.sub_configs.keys():\n",
        "                sub_config = getattr(config, sub_config_key)\n",
        "                sub_config.torch_dtype = torch_dtype\n",
        "        elif isinstance(torch_dtype, dict):\n",
        "            for key, curr_dtype in torch_dtype.items():\n",
        "                if hasattr(config, key):\n",
        "                    value = getattr(config, key)\n",
        "                    curr_dtype = curr_dtype if not isinstance(curr_dtype, str) else getattr(torch, curr_dtype)\n",
        "                    value.torch_dtype = curr_dtype\n",
        "            # main torch dtype for modules that aren't part of any sub-config\n",
        "            torch_dtype = torch_dtype.get(\"\")\n",
        "            torch_dtype = torch_dtype if not isinstance(torch_dtype, str) else getattr(torch, torch_dtype)\n",
        "            config.torch_dtype = torch_dtype\n",
        "            if torch_dtype is None:\n",
        "                torch_dtype = torch.float32\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"`torch_dtype` can be one of: `torch.dtype`, `'auto'`, a string of a valid `torch.dtype` or a `dict` with valid `torch_dtype` \"\n",
        "                f\"for each sub-config in composite configs, but received {torch_dtype}\"\n",
        "            )\n",
        "\n",
        "        dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n",
        "    else:\n",
        "        # set fp32 as the default dtype for BC\n",
        "        default_dtype = torch.get_default_dtype()\n",
        "        config.torch_dtype = default_dtype\n",
        "        for key in config.sub_configs.keys():\n",
        "            value = getattr(config, key)\n",
        "            value.torch_dtype = default_dtype\n",
        "\n",
        "    return config, torch_dtype, dtype_orig"
      ],
      "metadata": {
        "id": "OZGtDLdXvYhD"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ax-_brflSzX"
      },
      "source": [
        "# メイン"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_resolved_checkpoint_files(\n",
        "    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
        "    subfolder: str,\n",
        "    variant: Optional[str],\n",
        "    gguf_file: Optional[str],\n",
        "    from_tf: bool,\n",
        "    from_flax: bool,\n",
        "    use_safetensors: bool,\n",
        "    cache_dir: str,\n",
        "    force_download: bool,\n",
        "    proxies: Optional[dict[str, str]],\n",
        "    local_files_only: bool,\n",
        "    token: Optional[Union[str, bool]],\n",
        "    user_agent: dict,\n",
        "    revision: str,\n",
        "    commit_hash: Optional[str],\n",
        "    is_remote_code: bool,  # Because we can't determine this inside this function, we need it to be passed in\n",
        "    transformers_explicit_filename: Optional[str] = None,\n",
        ") -> tuple[Optional[list[str]], Optional[dict]]:\n",
        "\n",
        "    is_sharded = False\n",
        "\n",
        "    if pretrained_model_name_or_path is not None and gguf_file is None:\n",
        "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "        if is_local:\n",
        "            if transformers_explicit_filename is not None:\n",
        "                # If the filename is explicitly defined, load this by default.\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, transformers_explicit_filename)\n",
        "                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n",
        "            elif from_tf and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n",
        "            ):\n",
        "                # Load from a TF 1.0 checkpoint in priority if from_tf\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n",
        "            elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)):\n",
        "                # Load from a TF 2.0 checkpoint in priority if from_tf\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)\n",
        "            elif from_flax and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n",
        "            ):\n",
        "                # Load from a Flax checkpoint in priority if from_flax\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n",
        "            elif use_safetensors is not False and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant))\n",
        "            ):\n",
        "                # Load from a safetensors checkpoint\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant)\n",
        "                )\n",
        "            elif use_safetensors is not False and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant))\n",
        "            ):\n",
        "                # Load from a sharded safetensors checkpoint\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant)\n",
        "                )\n",
        "                is_sharded = True\n",
        "            elif not use_safetensors and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant))\n",
        "            ):\n",
        "                # Load from a PyTorch checkpoint\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant)\n",
        "                )\n",
        "            elif not use_safetensors and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant))\n",
        "            ):\n",
        "                # Load from a sharded PyTorch checkpoint\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant)\n",
        "                )\n",
        "                is_sharded = True\n",
        "            # At this stage we don't have a weight file so we will raise an error.\n",
        "            elif not use_safetensors and (\n",
        "                os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\"))\n",
        "                or os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME))\n",
        "            ):\n",
        "                raise OSError(\n",
        "                    f\"Error no file named {_add_variant(WEIGHTS_NAME, variant)} found in directory\"\n",
        "                    f\" {pretrained_model_name_or_path} but there is a file for TensorFlow weights. Use\"\n",
        "                    \" `from_tf=True` to load this model from those weights.\"\n",
        "                )\n",
        "            elif not use_safetensors and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n",
        "            ):\n",
        "                raise OSError(\n",
        "                    f\"Error no file named {_add_variant(WEIGHTS_NAME, variant)} found in directory\"\n",
        "                    f\" {pretrained_model_name_or_path} but there is a file for Flax weights. Use `from_flax=True`\"\n",
        "                    \" to load this model from those weights.\"\n",
        "                )\n",
        "            elif use_safetensors:\n",
        "                raise OSError(\n",
        "                    f\"Error no file named {_add_variant(SAFE_WEIGHTS_NAME, variant)} found in directory\"\n",
        "                    f\" {pretrained_model_name_or_path}.\"\n",
        "                )\n",
        "            else:\n",
        "                raise OSError(\n",
        "                    f\"Error no file named {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\n",
        "                    f\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME + '.index'} or {FLAX_WEIGHTS_NAME} found in directory\"\n",
        "                    f\" {pretrained_model_name_or_path}.\"\n",
        "                )\n",
        "        elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n",
        "            archive_file = pretrained_model_name_or_path\n",
        "            is_local = True\n",
        "        elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path + \".index\")):\n",
        "            if not from_tf:\n",
        "                raise ValueError(\n",
        "                    f\"We found a TensorFlow checkpoint at {pretrained_model_name_or_path + '.index'}, please set \"\n",
        "                    \"from_tf to True to load from this checkpoint.\"\n",
        "                )\n",
        "            archive_file = os.path.join(subfolder, pretrained_model_name_or_path + \".index\")\n",
        "            is_local = True\n",
        "        elif is_remote_url(pretrained_model_name_or_path):\n",
        "            filename = pretrained_model_name_or_path\n",
        "            resolved_archive_file = download_url(pretrained_model_name_or_path)\n",
        "        else:\n",
        "            # set correct filename\n",
        "            if transformers_explicit_filename is not None:\n",
        "                filename = transformers_explicit_filename\n",
        "                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n",
        "            elif from_tf:\n",
        "                filename = TF2_WEIGHTS_NAME\n",
        "            elif from_flax:\n",
        "                filename = FLAX_WEIGHTS_NAME\n",
        "            elif use_safetensors is not False:\n",
        "                filename = _add_variant(SAFE_WEIGHTS_NAME, variant)\n",
        "            else:\n",
        "                filename = _add_variant(WEIGHTS_NAME, variant)\n",
        "\n",
        "            try:\n",
        "                # Load from URL or cache if already cached\n",
        "                cached_file_kwargs = {\n",
        "                    \"cache_dir\": cache_dir,\n",
        "                    \"force_download\": force_download,\n",
        "                    \"proxies\": proxies,\n",
        "                    \"local_files_only\": local_files_only,\n",
        "                    \"token\": token,\n",
        "                    \"user_agent\": user_agent,\n",
        "                    \"revision\": revision,\n",
        "                    \"subfolder\": subfolder,\n",
        "                    \"_raise_exceptions_for_gated_repo\": False,\n",
        "                    \"_raise_exceptions_for_missing_entries\": False,\n",
        "                    \"_commit_hash\": commit_hash,\n",
        "                }\n",
        "                resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
        "\n",
        "                if resolved_archive_file is None and filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n",
        "                    # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n",
        "                    resolved_archive_file = cached_file(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant),\n",
        "                        **cached_file_kwargs,\n",
        "                    )\n",
        "                    if resolved_archive_file is not None:\n",
        "                        is_sharded = True\n",
        "                    elif use_safetensors:\n",
        "                        if revision == \"main\":\n",
        "                            resolved_archive_file, revision, is_sharded = auto_conversion(\n",
        "                                pretrained_model_name_or_path, **cached_file_kwargs\n",
        "                            )\n",
        "                        cached_file_kwargs[\"revision\"] = revision\n",
        "                        if resolved_archive_file is None:\n",
        "                            raise OSError(\n",
        "                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n",
        "                                f\" {_add_variant(SAFE_WEIGHTS_NAME, variant)} or {_add_variant(SAFE_WEIGHTS_INDEX_NAME, variant)} \"\n",
        "                                \"and thus cannot be loaded with `safetensors`. Please make sure that the model has \"\n",
        "                                \"been saved with `safe_serialization=True` or do not set `use_safetensors=True`.\"\n",
        "                            )\n",
        "                    else:\n",
        "                        # This repo has no safetensors file of any kind, we switch to PyTorch.\n",
        "                        filename = _add_variant(WEIGHTS_NAME, variant)\n",
        "                        resolved_archive_file = cached_file(\n",
        "                            pretrained_model_name_or_path, filename, **cached_file_kwargs\n",
        "                        )\n",
        "                if resolved_archive_file is None and filename == _add_variant(WEIGHTS_NAME, variant):\n",
        "                    # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n",
        "                    resolved_archive_file = cached_file(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        _add_variant(WEIGHTS_INDEX_NAME, variant),\n",
        "                        **cached_file_kwargs,\n",
        "                    )\n",
        "                    if resolved_archive_file is not None:\n",
        "                        is_sharded = True\n",
        "                if not local_files_only and not is_offline_mode():\n",
        "                    if resolved_archive_file is not None:\n",
        "                        if filename in [WEIGHTS_NAME, WEIGHTS_INDEX_NAME]:\n",
        "                            # If the PyTorch file was found, check if there is a safetensors file on the repository\n",
        "                            # If there is no safetensors file on the repositories, start an auto conversion\n",
        "                            safe_weights_name = SAFE_WEIGHTS_INDEX_NAME if is_sharded else SAFE_WEIGHTS_NAME\n",
        "                            has_file_kwargs = {\n",
        "                                \"revision\": revision,\n",
        "                                \"proxies\": proxies,\n",
        "                                \"token\": token,\n",
        "                                \"cache_dir\": cache_dir,\n",
        "                                \"local_files_only\": local_files_only,\n",
        "                            }\n",
        "                            cached_file_kwargs = {\n",
        "                                \"cache_dir\": cache_dir,\n",
        "                                \"force_download\": force_download,\n",
        "                                \"local_files_only\": local_files_only,\n",
        "                                \"user_agent\": user_agent,\n",
        "                                \"subfolder\": subfolder,\n",
        "                                \"_raise_exceptions_for_gated_repo\": False,\n",
        "                                \"_raise_exceptions_for_missing_entries\": False,\n",
        "                                \"_commit_hash\": commit_hash,\n",
        "                                **has_file_kwargs,\n",
        "                            }\n",
        "                            if (\n",
        "                                not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs)\n",
        "                                and not is_remote_code\n",
        "                            ):\n",
        "                                Thread(\n",
        "                                    target=auto_conversion,\n",
        "                                    args=(pretrained_model_name_or_path,),\n",
        "                                    kwargs={\"ignore_errors_during_conversion\": True, **cached_file_kwargs},\n",
        "                                    name=\"Thread-auto_conversion\",\n",
        "                                ).start()\n",
        "                    else:\n",
        "                        # Otherwise, no PyTorch file was found, maybe there is a TF or Flax model file.\n",
        "                        # We try those to give a helpful error message.\n",
        "                        has_file_kwargs = {\n",
        "                            \"revision\": revision,\n",
        "                            \"proxies\": proxies,\n",
        "                            \"token\": token,\n",
        "                            \"cache_dir\": cache_dir,\n",
        "                            \"local_files_only\": local_files_only,\n",
        "                        }\n",
        "                        if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n",
        "                            raise OSError(\n",
        "                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n",
        "                                f\" {_add_variant(WEIGHTS_NAME, variant)} but there is a file for TensorFlow weights.\"\n",
        "                                \" Use `from_tf=True` to load this model from those weights.\"\n",
        "                            )\n",
        "                        elif has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, **has_file_kwargs):\n",
        "                            raise OSError(\n",
        "                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n",
        "                                f\" {_add_variant(WEIGHTS_NAME, variant)} but there is a file for Flax weights. Use\"\n",
        "                                \" `from_flax=True` to load this model from those weights.\"\n",
        "                            )\n",
        "                        elif variant is not None and has_file(\n",
        "                            pretrained_model_name_or_path, WEIGHTS_NAME, **has_file_kwargs\n",
        "                        ):\n",
        "                            raise OSError(\n",
        "                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n",
        "                                f\" {_add_variant(WEIGHTS_NAME, variant)} but there is a file without the variant\"\n",
        "                                f\" {variant}. Use `variant=None` to load this model from those weights.\"\n",
        "                            )\n",
        "                        else:\n",
        "                            raise OSError(\n",
        "                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n",
        "                                f\" {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\n",
        "                                f\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or {FLAX_WEIGHTS_NAME}.\"\n",
        "                            )\n",
        "\n",
        "            except OSError:\n",
        "                 raise\n",
        "            except Exception as e:\n",
        "                # For any other exception, we throw a generic error.\n",
        "                raise OSError(\n",
        "                    f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it\"\n",
        "                    \" from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n",
        "                    f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n",
        "                    f\" directory containing a file named {_add_variant(WEIGHTS_NAME, variant)},\"\n",
        "                    f\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or {FLAX_WEIGHTS_NAME}.\"\n",
        "                ) from e\n",
        "\n",
        "        if is_local:\n",
        "            #logger.info(f\"loading weights file {archive_file}\")\n",
        "            resolved_archive_file = archive_file\n",
        "        #else:\n",
        "            #logger.info(f\"loading weights file {filename} from cache at {resolved_archive_file}\")\n",
        "\n",
        "    elif gguf_file:\n",
        "        # Case 1: the GGUF file is present locally\n",
        "        if os.path.isfile(gguf_file):\n",
        "            resolved_archive_file = gguf_file\n",
        "        # Case 2: The GGUF path is a location on the Hub\n",
        "        # Load from URL or cache if already cached\n",
        "        else:\n",
        "            cached_file_kwargs = {\n",
        "                \"cache_dir\": cache_dir,\n",
        "                \"force_download\": force_download,\n",
        "                \"proxies\": proxies,\n",
        "                \"local_files_only\": local_files_only,\n",
        "                \"token\": token,\n",
        "                \"user_agent\": user_agent,\n",
        "                \"revision\": revision,\n",
        "                \"subfolder\": subfolder,\n",
        "                \"_raise_exceptions_for_gated_repo\": False,\n",
        "                \"_raise_exceptions_for_missing_entries\": False,\n",
        "                \"_commit_hash\": commit_hash,\n",
        "            }\n",
        "\n",
        "            resolved_archive_file = cached_file(pretrained_model_name_or_path, gguf_file, **cached_file_kwargs)\n",
        "\n",
        "    # We now download and resolve all checkpoint files if the checkpoint is sharded\n",
        "    sharded_metadata = None\n",
        "    if is_sharded:\n",
        "        checkpoint_files, sharded_metadata = get_checkpoint_shard_files(\n",
        "            pretrained_model_name_or_path,\n",
        "            resolved_archive_file,\n",
        "            cache_dir=cache_dir,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            local_files_only=local_files_only,\n",
        "            token=token,\n",
        "            user_agent=user_agent,\n",
        "            revision=revision,\n",
        "            subfolder=subfolder,\n",
        "            _commit_hash=commit_hash,\n",
        "        )\n",
        "    else:\n",
        "        checkpoint_files = [resolved_archive_file] if pretrained_model_name_or_path is not None else None\n",
        "\n",
        "    return checkpoint_files, sharded_metadata"
      ],
      "metadata": {
        "id": "n1F24HvdkFLY"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deprecate_kwarg(\n",
        "    old_name: str,\n",
        "    version: str,\n",
        "    new_name: Optional[str] = None,\n",
        "    warn_if_greater_or_equal_version: bool = False,\n",
        "    raise_if_greater_or_equal_version: bool = False,\n",
        "    raise_if_both_names: bool = False,\n",
        "    additional_message: Optional[str] = None,\n",
        "):\n",
        "    deprecated_version = packaging.version.parse(version)\n",
        "    current_version = packaging.version.parse(__version__)\n",
        "    is_greater_or_equal_version = current_version >= deprecated_version\n",
        "\n",
        "    if is_greater_or_equal_version:\n",
        "        version_message = f\"and removed starting from version {version}\"\n",
        "    else:\n",
        "        version_message = f\"and will be removed in version {version}\"\n",
        "\n",
        "    def wrapper(func):\n",
        "        # Required for better warning message\n",
        "        sig = inspect.signature(func)\n",
        "        function_named_args = set(sig.parameters.keys())\n",
        "        is_instance_method = \"self\" in function_named_args\n",
        "        is_class_method = \"cls\" in function_named_args\n",
        "\n",
        "        @wraps(func)\n",
        "        def wrapped_func(*args, **kwargs):\n",
        "            # Get class + function name (just for better warning message)\n",
        "            func_name = func.__name__\n",
        "            if is_instance_method:\n",
        "                func_name = f\"{args[0].__class__.__name__}.{func_name}\"\n",
        "            elif is_class_method:\n",
        "                func_name = f\"{args[0].__name__}.{func_name}\"\n",
        "\n",
        "            minimum_action = Action.NONE\n",
        "            message = None\n",
        "\n",
        "            # deprecated kwarg and its new version are set for function call -> replace it with new name\n",
        "            if old_name in kwargs and new_name in kwargs:\n",
        "                minimum_action = Action.RAISE if raise_if_both_names else Action.NOTIFY_ALWAYS\n",
        "                message = f\"Both `{old_name}` and `{new_name}` are set for `{func_name}`. Using `{new_name}={kwargs[new_name]}` and ignoring deprecated `{old_name}={kwargs[old_name]}`.\"\n",
        "                kwargs.pop(old_name)\n",
        "\n",
        "            # only deprecated kwarg is set for function call -> replace it with new name\n",
        "            elif old_name in kwargs and new_name is not None and new_name not in kwargs:\n",
        "                minimum_action = Action.NOTIFY\n",
        "                message = f\"`{old_name}` is deprecated {version_message} for `{func_name}`. Use `{new_name}` instead.\"\n",
        "                kwargs[new_name] = kwargs.pop(old_name)\n",
        "\n",
        "            # deprecated kwarg is not set for function call and new name is not specified -> just notify\n",
        "            elif old_name in kwargs:\n",
        "                minimum_action = Action.NOTIFY\n",
        "                message = f\"`{old_name}` is deprecated {version_message} for `{func_name}`.\"\n",
        "\n",
        "            if message is not None and additional_message is not None:\n",
        "                message = f\"{message} {additional_message}\"\n",
        "\n",
        "            # update minimum_action if argument is ALREADY deprecated (current version >= deprecated version)\n",
        "            if is_greater_or_equal_version:\n",
        "                # change to (NOTIFY, NOTIFY_ALWAYS) -> RAISE if specified\n",
        "                # in case we want to raise error for already deprecated arguments\n",
        "                if raise_if_greater_or_equal_version and minimum_action != Action.NONE:\n",
        "                    minimum_action = Action.RAISE\n",
        "\n",
        "                # change to NOTIFY -> NONE if specified (NOTIFY_ALWAYS can't be changed to NONE)\n",
        "                # in case we want to ignore notifications for already deprecated arguments\n",
        "                elif not warn_if_greater_or_equal_version and minimum_action == Action.NOTIFY:\n",
        "                    minimum_action = Action.NONE\n",
        "\n",
        "            # raise error or notify user\n",
        "            if minimum_action == Action.RAISE:\n",
        "                raise ValueError(message)\n",
        "            # If we are compiling, we do not raise the warning as it would break compilation\n",
        "            elif minimum_action in (Action.NOTIFY, Action.NOTIFY_ALWAYS) and not is_torchdynamo_compiling():\n",
        "                # DeprecationWarning is ignored by default, so we use FutureWarning instead\n",
        "                warnings.warn(message, FutureWarning, stacklevel=2)\n",
        "\n",
        "            return func(*args, **kwargs)\n",
        "\n",
        "        return wrapped_func\n",
        "\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "831nZiHjytLE"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "ZbZs8FfdR0ws"
      },
      "outputs": [],
      "source": [
        "class AutoTokenizer:\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "        config = kwargs.pop(\"config\", None)\n",
        "        use_fast = kwargs.pop(\"use_fast\", True)\n",
        "        tokenizer_type = kwargs.pop(\"tokenizer_type\", None)\n",
        "\n",
        "        tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
        "        if \"_commit_hash\" in tokenizer_config:\n",
        "            kwargs[\"_commit_hash\"] = tokenizer_config[\"_commit_hash\"]\n",
        "        config_tokenizer_class = tokenizer_config.get(\"tokenizer_class\")\n",
        "\n",
        "        if config_tokenizer_class is not None:\n",
        "            tokenizer_class = None\n",
        "            if use_fast and not config_tokenizer_class.endswith(\"Fast\"):\n",
        "                tokenizer_class_candidate = f\"{config_tokenizer_class}Fast\"\n",
        "                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n",
        "            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "kNPxiQ-a2JzS"
      },
      "outputs": [],
      "source": [
        "METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n",
        "\n",
        "class ExplicitEnum(str, Enum):\n",
        "    @classmethod\n",
        "    def _missing_(cls, value):\n",
        "        raise ValueError(\n",
        "            f\"{value} is not a valid {cls.__name__}, please select one of {list(cls._value2member_map_.keys())}\"\n",
        "        )\n",
        "\n",
        "class GenerationMode(ExplicitEnum):\n",
        "    # Non-beam methods\n",
        "    CONTRASTIVE_SEARCH = \"contrastive_search\"\n",
        "    GREEDY_SEARCH = \"greedy_search\"\n",
        "    SAMPLE = \"sample\"\n",
        "    ASSISTED_GENERATION = \"assisted_generation\"\n",
        "    DOLA_GENERATION = \"dola_generation\"\n",
        "    # Beam methods\n",
        "    BEAM_SEARCH = \"beam_search\"\n",
        "    BEAM_SAMPLE = \"beam_sample\"\n",
        "    CONSTRAINED_BEAM_SEARCH = \"constrained_beam_search\"\n",
        "    GROUP_BEAM_SEARCH = \"group_beam_search\"\n",
        "\n",
        "class GenerationConfig(): #PushToHubMixin\n",
        "    extra_output_flags = (\"output_attentions\", \"output_hidden_states\", \"output_scores\", \"output_logits\")\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        # Parameters that control the length of the output\n",
        "        self.max_length = kwargs.pop(\"max_length\", 20)\n",
        "        self.max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\n",
        "        self.min_length = kwargs.pop(\"min_length\", 0)\n",
        "        self.min_new_tokens = kwargs.pop(\"min_new_tokens\", None)\n",
        "        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n",
        "        self.max_time = kwargs.pop(\"max_time\", None)\n",
        "        self.stop_strings = kwargs.pop(\"stop_strings\", None)\n",
        "\n",
        "        # Parameters that control the generation strategy used\n",
        "        self.do_sample = kwargs.pop(\"do_sample\", False)\n",
        "        self.num_beams = kwargs.pop(\"num_beams\", 1)\n",
        "        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n",
        "        self.penalty_alpha = kwargs.pop(\"penalty_alpha\", None)\n",
        "        self.dola_layers = kwargs.pop(\"dola_layers\", None)\n",
        "\n",
        "        # Parameters that control the cache\n",
        "        self.use_cache = kwargs.pop(\"use_cache\", True)\n",
        "        self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n",
        "        self.cache_config = kwargs.pop(\"cache_config\", None)\n",
        "        if self.cache_implementation is not None and self.cache_implementation in CACHE_CONFIG_MAPPING:\n",
        "            cache_config_class = CACHE_CONFIG_MAPPING[self.cache_implementation]\n",
        "            if isinstance(self.cache_config, dict):\n",
        "                self.cache_config = cache_config_class.from_dict(self.cache_config)\n",
        "        self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n",
        "        self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)\n",
        "\n",
        "        # Parameters for manipulation of the model output logits\n",
        "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
        "        self.top_k = kwargs.pop(\"top_k\", 50)\n",
        "        self.top_p = kwargs.pop(\"top_p\", 1.0)\n",
        "        self.min_p = kwargs.pop(\"min_p\", None)\n",
        "        self.typical_p = kwargs.pop(\"typical_p\", 1.0)\n",
        "        self.epsilon_cutoff = kwargs.pop(\"epsilon_cutoff\", 0.0)\n",
        "        self.eta_cutoff = kwargs.pop(\"eta_cutoff\", 0.0)\n",
        "        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n",
        "        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n",
        "        self.encoder_repetition_penalty = kwargs.pop(\"encoder_repetition_penalty\", 1.0)\n",
        "        self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n",
        "        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n",
        "        self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n",
        "        self.force_words_ids = kwargs.pop(\"force_words_ids\", None)\n",
        "        self.renormalize_logits = kwargs.pop(\"renormalize_logits\", False)\n",
        "        self.constraints = kwargs.pop(\"constraints\", None)\n",
        "        self.forced_bos_token_id = kwargs.pop(\"forced_bos_token_id\", None)\n",
        "        self.forced_eos_token_id = kwargs.pop(\"forced_eos_token_id\", None)\n",
        "        self.remove_invalid_values = kwargs.pop(\"remove_invalid_values\", False)\n",
        "        self.exponential_decay_length_penalty = kwargs.pop(\"exponential_decay_length_penalty\", None)\n",
        "        self.suppress_tokens = kwargs.pop(\"suppress_tokens\", None)\n",
        "        self.begin_suppress_tokens = kwargs.pop(\"begin_suppress_tokens\", None)\n",
        "        self.sequence_bias = kwargs.pop(\"sequence_bias\", None)\n",
        "        self.token_healing = kwargs.pop(\"token_healing\", False)\n",
        "        self.guidance_scale = kwargs.pop(\"guidance_scale\", None)\n",
        "        self.low_memory = kwargs.pop(\"low_memory\", None)\n",
        "        watermarking_config = kwargs.pop(\"watermarking_config\", None)\n",
        "        if watermarking_config is None:\n",
        "            self.watermarking_config = None\n",
        "        elif isinstance(watermarking_config, BaseWatermarkingConfig):\n",
        "            self.watermarking_config = watermarking_config\n",
        "        else:\n",
        "            self.watermarking_config = WatermarkingConfig.from_dict(watermarking_config)\n",
        "\n",
        "        # Parameters that define the output variables of `generate`\n",
        "        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n",
        "        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n",
        "        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n",
        "        self.output_scores = kwargs.pop(\"output_scores\", False)\n",
        "        self.output_logits = kwargs.pop(\"output_logits\", None)\n",
        "        self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n",
        "\n",
        "        # Special tokens that can be used at generation time\n",
        "        self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n",
        "        self.bos_token_id = kwargs.pop(\"bos_token_id\", None)\n",
        "        self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n",
        "\n",
        "        # Generation parameters exclusive to encoder-decoder models\n",
        "        self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", 0)\n",
        "        self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n",
        "\n",
        "        # Assistant generation\n",
        "        self.is_assistant = False\n",
        "        self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", 20)\n",
        "        self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", \"constant\")\n",
        "        self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", 0.4)\n",
        "        self.prompt_lookup_num_tokens = kwargs.pop(\"prompt_lookup_num_tokens\", None)\n",
        "        self.max_matching_ngram_size = kwargs.pop(\"max_matching_ngram_size\", None)\n",
        "        self.assistant_early_exit = kwargs.pop(\"assistant_early_exit\", None)\n",
        "        ## assistant generation for different tokenizers, the windows size for assistant/target model\n",
        "        self.assistant_lookbehind = kwargs.pop(\"assistant_lookbehind\", 10)\n",
        "        self.target_lookbehind = kwargs.pop(\"target_lookbehind\", 10)\n",
        "\n",
        "        # Performance\n",
        "        self.compile_config = kwargs.pop(\"compile_config\", None)\n",
        "        self.disable_compile = kwargs.pop(\"disable_compile\", False)\n",
        "\n",
        "        # The remaining attributes do not parametrize `.generate()`, but are informative and/or used by the hub\n",
        "        # interface.\n",
        "        self._from_model_config = kwargs.pop(\"_from_model_config\", False)\n",
        "        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "        self.transformers_version = kwargs.pop(\"transformers_version\", __version__)\n",
        "\n",
        "        # Additional attributes without default values\n",
        "        if not self._from_model_config:\n",
        "            # we don't want to copy values from the model config if we're initializing a `GenerationConfig` from a\n",
        "            # model's default configuration file\n",
        "            for key, value in kwargs.items():\n",
        "                try:\n",
        "                    setattr(self, key, value)\n",
        "                except AttributeError as err:\n",
        "                    #logger.error(f\"Can't set {key} with value {value} for {self}\")\n",
        "                    raise err\n",
        "\n",
        "        # Validate the values of the attributes\n",
        "        self.validate()\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.to_json_string(ignore_metadata=True))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, GenerationConfig):\n",
        "            return False\n",
        "\n",
        "        self_without_metadata = self.to_json_string(use_diff=False, ignore_metadata=True)\n",
        "        other_without_metadata = other.to_json_string(use_diff=False, ignore_metadata=True)\n",
        "        return self_without_metadata == other_without_metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__} {self.to_json_string(ignore_metadata=True)}\"\n",
        "\n",
        "    def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = None) -> GenerationMode:\n",
        "        if self.constraints is not None or self.force_words_ids is not None:\n",
        "            generation_mode = GenerationMode.CONSTRAINED_BEAM_SEARCH\n",
        "        elif self.num_beams == 1:\n",
        "            if self.do_sample is False:\n",
        "                if (\n",
        "                    self.top_k is not None\n",
        "                    and self.top_k > 1\n",
        "                    and self.penalty_alpha is not None\n",
        "                    and self.penalty_alpha > 0\n",
        "                ):\n",
        "                    generation_mode = GenerationMode.CONTRASTIVE_SEARCH\n",
        "                else:\n",
        "                    generation_mode = GenerationMode.GREEDY_SEARCH\n",
        "            else:\n",
        "                generation_mode = GenerationMode.SAMPLE\n",
        "        else:\n",
        "            if self.num_beam_groups > 1:\n",
        "                generation_mode = GenerationMode.GROUP_BEAM_SEARCH\n",
        "            elif self.do_sample is True:\n",
        "                generation_mode = GenerationMode.BEAM_SAMPLE\n",
        "            else:\n",
        "                generation_mode = GenerationMode.BEAM_SEARCH\n",
        "\n",
        "        # Assisted generation may extend some generation modes\n",
        "        if (\n",
        "            assistant_model is not None\n",
        "            or self.prompt_lookup_num_tokens is not None\n",
        "            or self.assistant_early_exit is not None\n",
        "        ):\n",
        "            if generation_mode in (\"greedy_search\", \"sample\"):\n",
        "                generation_mode = GenerationMode.ASSISTED_GENERATION\n",
        "            #else:\n",
        "                #logger.warning(\n",
        "                    #\"You've set `assistant_model`, which triggers assisted generate. Currently, assisted generate \"\n",
        "                    #\"is only supported with Greedy Search and Sample. However, the base decoding mode (based on \"\n",
        "                    #f\"current flags) is {generation_mode} -- some of the set flags will be ignored.\"\n",
        "                #)\n",
        "\n",
        "        # DoLa generation may extend some generation modes\n",
        "        if self.dola_layers is not None:\n",
        "            if generation_mode in (\"greedy_search\", \"sample\"):\n",
        "                generation_mode = GenerationMode.DOLA_GENERATION\n",
        "            #else:\n",
        "                #logger.warning(\n",
        "                    #\"You've set `dola_layers`, which triggers DoLa generate. Currently, DoLa generate \"\n",
        "                    #\"is only supported with Greedy Search and Sample.  However, the base decoding mode (based on \"\n",
        "                    #f\"current flags) is {generation_mode} -- some of the set flags will be ignored.\"\n",
        "                #)\n",
        "        return generation_mode\n",
        "\n",
        "    @deprecate_kwarg(\"is_init\", version=\"4.54.0\")\n",
        "    def validate(self, strict=False):\n",
        "        minor_issues = {}  # format: {attribute_name: issue_description}\n",
        "\n",
        "        # 1. Validation of individual attributes\n",
        "        # 1.1. Decoding attributes\n",
        "        if self.early_stopping not in {True, False, \"never\"}:\n",
        "            raise ValueError(f\"`early_stopping` must be a boolean or 'never', but is {self.early_stopping}.\")\n",
        "        if self.max_new_tokens is not None and self.max_new_tokens <= 0:\n",
        "            raise ValueError(f\"`max_new_tokens` must be greater than 0, but is {self.max_new_tokens}.\")\n",
        "        if self.pad_token_id is not None and self.pad_token_id < 0:\n",
        "            minor_issues[\"pad_token_id\"] = (\n",
        "                f\"`pad_token_id` should be positive but got {self.pad_token_id}. This will cause errors when batch \"\n",
        "                \"generating, if there is padding. Please set `pad_token_id` explicitly as \"\n",
        "                \"`model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\"\n",
        "            )\n",
        "        # 1.2. Cache attributes\n",
        "        if self.cache_implementation is not None and self.cache_implementation not in ALL_CACHE_IMPLEMENTATIONS:\n",
        "            raise ValueError(\n",
        "                f\"Invalid `cache_implementation` ({self.cache_implementation}). Choose one of: \"\n",
        "                f\"{ALL_CACHE_IMPLEMENTATIONS}\"\n",
        "            )\n",
        "        if self.cache_config is not None:\n",
        "            cache_class = CACHE_CONFIG_MAPPING.get(self.cache_implementation)\n",
        "            if cache_class is None:\n",
        "                raise ValueError(\n",
        "                    \"You provided a `cache_config` but the cache implementation you are using \"\n",
        "                    f\"({self.cache_implementation}) does not require any config. Make sure to use the \"\n",
        "                    \"correct cache implementation matching your cache config.\"\n",
        "                )\n",
        "            if not isinstance(self.cache_config, cache_class):\n",
        "                self.cache_config = cache_class.from_dict(self.cache_config)\n",
        "            self.cache_config.validate()\n",
        "        # 1.3. Performance attributes\n",
        "        if self.compile_config is not None and not isinstance(self.compile_config, CompileConfig):\n",
        "            raise ValueError(\n",
        "                f\"You provided `compile_config` as an instance of {type(self.compile_config)}, but it must be an \"\n",
        "                \"instance of `CompileConfig`.\"\n",
        "            )\n",
        "        # 1.4. Watermarking attributes\n",
        "        if self.watermarking_config is not None:\n",
        "            if not (\n",
        "                isinstance(self.watermarking_config, WatermarkingConfig)\n",
        "                or isinstance(self.watermarking_config, SynthIDTextWatermarkingConfig)\n",
        "            ):\n",
        "                minor_issues[\"watermarking_config\"] = (\n",
        "                    \"`watermarking_config` as a dict is deprecated and will be removed in v4.54.0. Please construct \"\n",
        "                    \"`watermarking_config` object with `WatermarkingConfig` or `SynthIDTextWatermarkingConfig` class.\"\n",
        "                )\n",
        "                self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n",
        "            self.watermarking_config.validate()\n",
        "\n",
        "        # 2. Validation of attribute combinations\n",
        "        # 2.1. detect sampling-only parameterization when not in sampling mode\n",
        "        if self.do_sample is False:\n",
        "            greedy_wrong_parameter_msg = (\n",
        "                \"`do_sample` is set to `False`. However, `{flag_name}` is set to `{flag_value}` -- this flag is only \"\n",
        "                \"used in sample-based generation modes. You should set `do_sample=True` or unset `{flag_name}`.\"\n",
        "            )\n",
        "            if self.temperature is not None and self.temperature != 1.0:\n",
        "                minor_issues[\"temperature\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"temperature\", flag_value=self.temperature\n",
        "                )\n",
        "            if self.top_p is not None and self.top_p != 1.0:\n",
        "                minor_issues[\"top_p\"] = greedy_wrong_parameter_msg.format(flag_name=\"top_p\", flag_value=self.top_p)\n",
        "            if self.min_p is not None:\n",
        "                minor_issues[\"min_p\"] = greedy_wrong_parameter_msg.format(flag_name=\"min_p\", flag_value=self.min_p)\n",
        "            if self.typical_p is not None and self.typical_p != 1.0:\n",
        "                minor_issues[\"typical_p\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"typical_p\", flag_value=self.typical_p\n",
        "                )\n",
        "            if (\n",
        "                self.top_k is not None and self.top_k != 50 and self.penalty_alpha is None\n",
        "            ):  # contrastive search uses top_k\n",
        "                minor_issues[\"top_k\"] = greedy_wrong_parameter_msg.format(flag_name=\"top_k\", flag_value=self.top_k)\n",
        "            if self.epsilon_cutoff is not None and self.epsilon_cutoff != 0.0:\n",
        "                minor_issues[\"epsilon_cutoff\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"epsilon_cutoff\", flag_value=self.epsilon_cutoff\n",
        "                )\n",
        "            if self.eta_cutoff is not None and self.eta_cutoff != 0.0:\n",
        "                minor_issues[\"eta_cutoff\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"eta_cutoff\", flag_value=self.eta_cutoff\n",
        "                )\n",
        "\n",
        "        # 2.2. detect beam-only parameterization when not in beam mode\n",
        "        if self.num_beams == 1:\n",
        "            single_beam_wrong_parameter_msg = (\n",
        "                \"`num_beams` is set to 1. However, `{flag_name}` is set to `{flag_value}` -- this flag is only used \"\n",
        "                \"in beam-based generation modes. You should set `num_beams>1` or unset `{flag_name}`.\"\n",
        "            )\n",
        "            if self.early_stopping is not False:\n",
        "                minor_issues[\"early_stopping\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"early_stopping\", flag_value=self.early_stopping\n",
        "                )\n",
        "            if self.num_beam_groups is not None and self.num_beam_groups != 1:\n",
        "                minor_issues[\"num_beam_groups\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"num_beam_groups\", flag_value=self.num_beam_groups\n",
        "                )\n",
        "            if self.diversity_penalty is not None and self.diversity_penalty != 0.0:\n",
        "                minor_issues[\"diversity_penalty\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"diversity_penalty\", flag_value=self.diversity_penalty\n",
        "                )\n",
        "            if self.length_penalty is not None and self.length_penalty != 1.0:\n",
        "                minor_issues[\"length_penalty\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"length_penalty\", flag_value=self.length_penalty\n",
        "                )\n",
        "            if self.constraints is not None:\n",
        "                minor_issues[\"constraints\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"constraints\", flag_value=self.constraints\n",
        "                )\n",
        "            # DoLa generation needs num_beams == 1\n",
        "            if self.dola_layers is not None and (self.repetition_penalty is None or self.repetition_penalty < 1.2):\n",
        "                minor_issues[\"repetition_penalty\"] = (\n",
        "                    \"`dola_layers` is set to trigger DoLa decoding, but `repetition_penalty` is set to a value of \"\n",
        "                    f\"{self.repetition_penalty}, which could induce unwanted repetition. The recommended value for \"\n",
        "                    \"DoLa decoding is `repetition_penalty>=1.2`.\",\n",
        "                )\n",
        "\n",
        "        # 2.3. detect incorrect parameterization specific to advanced beam modes\n",
        "        else:\n",
        "            # constrained beam search\n",
        "            if self.constraints is not None or self.force_words_ids is not None:\n",
        "                constrained_wrong_parameter_msg = (\n",
        "                    \"one of `constraints`, `force_words_ids` is not `None`, triggering constrained beam search. \"\n",
        "                    \"However, `{flag_name}` is set to `{flag_value}`, which is incompatible with this generation \"\n",
        "                    \"mode. Set `constraints` and `force_words_ids` to `None` or unset `{flag_name}` to continue.\"\n",
        "                )\n",
        "                if self.do_sample is True:\n",
        "                    raise ValueError(\n",
        "                        constrained_wrong_parameter_msg.format(flag_name=\"do_sample\", flag_value=self.do_sample)\n",
        "                    )\n",
        "                if self.num_beam_groups is not None and self.num_beam_groups != 1:\n",
        "                    raise ValueError(\n",
        "                        constrained_wrong_parameter_msg.format(\n",
        "                            flag_name=\"num_beam_groups\", flag_value=self.num_beam_groups\n",
        "                        )\n",
        "                    )\n",
        "            # group beam search\n",
        "            elif self.diversity_penalty != 0.0 or self.num_beam_groups != 1:\n",
        "                group_error_prefix = (\n",
        "                    \"`diversity_penalty` is not 0.0 or `num_beam_groups` is not 1, triggering group beam search. In \"\n",
        "                    \"this generation mode, \"\n",
        "                )\n",
        "                if self.do_sample is True:\n",
        "                    raise ValueError(group_error_prefix + \"`do_sample` must be set to `False`\")\n",
        "                if self.num_beams % self.num_beam_groups != 0:\n",
        "                    raise ValueError(group_error_prefix + \"`num_beams` should be divisible by `num_beam_groups`\")\n",
        "                if self.diversity_penalty == 0.0:\n",
        "                    raise ValueError(\n",
        "                        group_error_prefix\n",
        "                        + \"`diversity_penalty` should be greater than `0.0`, otherwise your groups will be identical.\"\n",
        "                    )\n",
        "\n",
        "        # 2.4. check `num_return_sequences`\n",
        "        if self.num_return_sequences != 1:\n",
        "            if self.num_beams == 1:\n",
        "                if self.do_sample is False:\n",
        "                    raise ValueError(\n",
        "                        \"Greedy methods without beam search do not support `num_return_sequences` different than 1 \"\n",
        "                        f\"(got {self.num_return_sequences}).\"\n",
        "                    )\n",
        "            elif self.num_return_sequences > self.num_beams:\n",
        "                raise ValueError(\n",
        "                    f\"`num_return_sequences` ({self.num_return_sequences}) has to be smaller or equal to `num_beams` \"\n",
        "                    f\"({self.num_beams}).\"\n",
        "                )\n",
        "\n",
        "        # 2.5. check cache-related arguments\n",
        "        if self.use_cache is False:\n",
        "            # In this case, all cache-related arguments should be unset. However, since `use_cache=False` is often used\n",
        "            # passed to `generate` directly to hot-fix cache issues, let's raise a warning instead of an error\n",
        "            # (otherwise a user might need to overwrite several parameters).\n",
        "            no_cache_warning = (\n",
        "                \"You have set `use_cache` to `False`, but {cache_arg} is set to {cache_arg_value}. {cache_arg} will \"\n",
        "                \"have no effect.\"\n",
        "            )\n",
        "            for arg_name in (\"cache_implementation\", \"cache_config\", \"return_legacy_cache\"):\n",
        "                if getattr(self, arg_name) is not None:\n",
        "                    minor_issues[arg_name] = no_cache_warning.format(\n",
        "                        cache_arg=arg_name, cache_arg_value=getattr(self, arg_name)\n",
        "                    )\n",
        "\n",
        "        # 2.6. other incorrect combinations\n",
        "        if self.return_dict_in_generate is not True:\n",
        "            for extra_output_flag in self.extra_output_flags:\n",
        "                if getattr(self, extra_output_flag) is True:\n",
        "                    minor_issues[extra_output_flag] = (\n",
        "                        f\"`return_dict_in_generate` is NOT set to `True`, but `{extra_output_flag}` is. When \"\n",
        "                        f\"`return_dict_in_generate` is not `True`, `{extra_output_flag}` is ignored.\"\n",
        "                    )\n",
        "\n",
        "        # 3. Check common issue: passing `generate` arguments inside the generation config\n",
        "        generate_arguments = (\n",
        "            \"logits_processor\",\n",
        "            \"stopping_criteria\",\n",
        "            \"prefix_allowed_tokens_fn\",\n",
        "            \"synced_gpus\",\n",
        "            \"assistant_model\",\n",
        "            \"streamer\",\n",
        "            \"negative_prompt_ids\",\n",
        "            \"negative_prompt_attention_mask\",\n",
        "            \"use_model_defaults\",\n",
        "        )\n",
        "        for arg in generate_arguments:\n",
        "            if hasattr(self, arg):\n",
        "                raise ValueError(\n",
        "                    f\"Argument `{arg}` is not a valid argument of `GenerationConfig`. It should be passed to \"\n",
        "                    \"`generate()` (or a pipeline) directly.\"\n",
        "                )\n",
        "\n",
        "        # Finally, handle caught minor issues. With default parameterization, we will throw a minimal warning.\n",
        "        if len(minor_issues) > 0:\n",
        "            # Full list of issues with potential fixes\n",
        "            info_message = []\n",
        "            for attribute_name, issue_description in minor_issues.items():\n",
        "                info_message.append(f\"- `{attribute_name}`: {issue_description}\")\n",
        "            info_message = \"\\n\".join(info_message)\n",
        "            info_message += (\n",
        "                \"\\nIf you're using a pretrained model, note that some of these attributes may be set through the \"\n",
        "                \"model's `generation_config.json` file.\"\n",
        "            )\n",
        "\n",
        "            if strict:\n",
        "                raise ValueError(\"GenerationConfig is invalid: \\n\" + info_message)\n",
        "            else:\n",
        "                attributes_with_issues = list(minor_issues.keys())\n",
        "                warning_message = (\n",
        "                    f\"The following generation flags are not valid and may be ignored: {attributes_with_issues}.\"\n",
        "                )\n",
        "                if logging.get_verbosity() >= logging.WARNING:\n",
        "                    warning_message += \" Set `TRANSFORMERS_VERBOSITY=info` for more details.\"\n",
        "                #logger.warning(warning_message)\n",
        "                #logger.info(info_message)\n",
        "\n",
        "    def save_pretrained(\n",
        "        self,\n",
        "        save_directory: Union[str, os.PathLike],\n",
        "        config_file_name: Optional[Union[str, os.PathLike]] = None,\n",
        "        push_to_hub: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        try:\n",
        "            self.validate(strict=True)\n",
        "        except ValueError as exc:\n",
        "            raise ValueError(str(exc) + \"\\n\\nFix these issues to save the configuration.\")\n",
        "\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "\n",
        "        if use_auth_token is not None:\n",
        "            warnings.warn(\n",
        "                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. \"\n",
        "                \"Please use `token` instead.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            if kwargs.get(\"token\", None) is not None:\n",
        "                raise ValueError(\n",
        "                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n",
        "                )\n",
        "            kwargs[\"token\"] = use_auth_token\n",
        "\n",
        "        config_file_name = config_file_name if config_file_name is not None else GENERATION_CONFIG_NAME\n",
        "\n",
        "        if os.path.isfile(save_directory):\n",
        "            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
        "\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "        if push_to_hub:\n",
        "            commit_message = kwargs.pop(\"commit_message\", None)\n",
        "            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n",
        "            repo_id = self._create_repo(repo_id, **kwargs)\n",
        "            files_timestamps = self._get_files_timestamps(save_directory)\n",
        "\n",
        "        output_config_file = os.path.join(save_directory, config_file_name)\n",
        "\n",
        "        self.to_json_file(output_config_file, use_diff=True)\n",
        "        #logger.info(f\"Configuration saved in {output_config_file}\")\n",
        "\n",
        "        if push_to_hub:\n",
        "            self._upload_modified_files(\n",
        "                save_directory,\n",
        "                repo_id,\n",
        "                files_timestamps,\n",
        "                commit_message=commit_message,\n",
        "                token=kwargs.get(\"token\"),\n",
        "            )\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        pretrained_model_name: Union[str, os.PathLike],\n",
        "        config_file_name: Optional[Union[str, os.PathLike]] = None,\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        **kwargs,\n",
        "    ) -> \"GenerationConfig\":\n",
        "        config_file_name = config_file_name if config_file_name is not None else GENERATION_CONFIG_NAME\n",
        "\n",
        "        resume_download = kwargs.pop(\"resume_download\", None)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        if use_auth_token is not None:\n",
        "            warnings.warn(\n",
        "                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            if token is not None:\n",
        "                raise ValueError(\n",
        "                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n",
        "                )\n",
        "            token = use_auth_token\n",
        "\n",
        "        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n",
        "        if from_pipeline is not None:\n",
        "            user_agent[\"using_pipeline\"] = from_pipeline\n",
        "\n",
        "        config_path = os.path.join(pretrained_model_name, config_file_name)\n",
        "        config_path = str(config_path)\n",
        "\n",
        "        is_local = os.path.exists(config_path)\n",
        "        if os.path.isfile(os.path.join(subfolder, config_path)):\n",
        "            # Special case when config_path is a local file\n",
        "            resolved_config_file = config_path\n",
        "            is_local = True\n",
        "        elif is_remote_url(config_path):\n",
        "            configuration_file = config_path\n",
        "            resolved_config_file = download_url(config_path)\n",
        "        else:\n",
        "            configuration_file = config_file_name\n",
        "            try:\n",
        "                # Load from local folder or from cache or download from model Hub and cache\n",
        "                resolved_config_file = cached_file(\n",
        "                    pretrained_model_name,\n",
        "                    configuration_file,\n",
        "                    cache_dir=cache_dir,\n",
        "                    force_download=force_download,\n",
        "                    proxies=proxies,\n",
        "                    resume_download=resume_download,\n",
        "                    local_files_only=local_files_only,\n",
        "                    token=token,\n",
        "                    user_agent=user_agent,\n",
        "                    revision=revision,\n",
        "                    subfolder=subfolder,\n",
        "                    _commit_hash=commit_hash,\n",
        "                )\n",
        "                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "            except OSError:\n",
        "                # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n",
        "                # the original exception.\n",
        "                raise\n",
        "            except Exception:\n",
        "                # For any other exception, we throw a generic error.\n",
        "                raise OSError(\n",
        "                    f\"Can't load the configuration of '{pretrained_model_name}'. If you were trying to load it\"\n",
        "                    \" from 'https://huggingface.co/models', make sure you don't have a local directory with the same\"\n",
        "                    f\" name. Otherwise, make sure '{pretrained_model_name}' is the correct path to a directory\"\n",
        "                    f\" containing a {configuration_file} file\"\n",
        "                )\n",
        "\n",
        "        try:\n",
        "            # Load config dict\n",
        "            config_dict = cls._dict_from_json_file(resolved_config_file)\n",
        "            config_dict[\"_commit_hash\"] = commit_hash\n",
        "        except (json.JSONDecodeError, UnicodeDecodeError):\n",
        "            raise OSError(f\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\")\n",
        "\n",
        "        #if is_local:\n",
        "            #logger.info(f\"loading configuration file {resolved_config_file}\")\n",
        "        #else:\n",
        "            #logger.info(f\"loading configuration file {configuration_file} from cache at {resolved_config_file}\")\n",
        "\n",
        "        if kwargs.get(\"return_unused_kwargs\") is True:\n",
        "            config, unused_kwargs = cls.from_dict(config_dict, **kwargs)\n",
        "            config._original_object_hash = hash(config)  # Hash to detect whether the instance was modified\n",
        "            return config, unused_kwargs\n",
        "        else:\n",
        "            config = cls.from_dict(config_dict, **kwargs)\n",
        "            config._original_object_hash = hash(config)  # Hash to detect whether the instance was modified\n",
        "            return config\n",
        "\n",
        "    @classmethod\n",
        "    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
        "        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "            text = reader.read()\n",
        "        return json.loads(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"GenerationConfig\":\n",
        "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
        "        # Those arguments may be passed along for our internal telemetry.\n",
        "        # We remove them so they don't appear in `return_unused_kwargs`.\n",
        "        kwargs.pop(\"_from_auto\", None)\n",
        "        kwargs.pop(\"_from_pipeline\", None)\n",
        "        # The commit hash might have been updated in the `config_dict`, we don't want the kwargs to erase that update.\n",
        "        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n",
        "            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        # The line below allows model-specific config to be loaded as well through kwargs, with safety checks.\n",
        "        # See https://github.com/huggingface/transformers/pull/21269\n",
        "        config = cls(**{**config_dict, **kwargs})\n",
        "        unused_kwargs = config.update(**kwargs)\n",
        "\n",
        "        #logger.info(f\"Generate config {config}\")\n",
        "        if return_unused_kwargs:\n",
        "            return config, unused_kwargs\n",
        "        else:\n",
        "            return config\n",
        "\n",
        "    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n",
        "        if d.get(\"torch_dtype\", None) is not None and not isinstance(d[\"torch_dtype\"], str):\n",
        "            d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self.dict_torch_dtype_to_str(value)\n",
        "\n",
        "    def to_diff_dict(self) -> dict[str, Any]:\n",
        "        config_dict = self.to_dict()\n",
        "\n",
        "        # get the default config dict\n",
        "        default_config_dict = GenerationConfig().to_dict()\n",
        "\n",
        "        serializable_config_dict = {}\n",
        "\n",
        "        # only serialize values that differ from the default config\n",
        "        for key, value in config_dict.items():\n",
        "            if key not in default_config_dict or key == \"transformers_version\" or value != default_config_dict[key]:\n",
        "                serializable_config_dict[key] = value\n",
        "\n",
        "        self.dict_torch_dtype_to_str(serializable_config_dict)\n",
        "        return serializable_config_dict\n",
        "\n",
        "    def to_dict(self) -> dict[str, Any]:\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "\n",
        "        # Fields to ignore at serialization time\n",
        "        if \"_commit_hash\" in output:\n",
        "            del output[\"_commit_hash\"]\n",
        "        if \"_original_object_hash\" in output:\n",
        "            del output[\"_original_object_hash\"]\n",
        "        if \"compile_config\" in output:\n",
        "            del output[\"compile_config\"]\n",
        "\n",
        "        # Transformers version when serializing this file\n",
        "        output[\"transformers_version\"] = __version__\n",
        "\n",
        "        self.dict_torch_dtype_to_str(output)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self, use_diff: bool = True, ignore_metadata: bool = False) -> str:\n",
        "        if use_diff is True:\n",
        "            config_dict = self.to_diff_dict()\n",
        "        else:\n",
        "            config_dict = self.to_dict()\n",
        "\n",
        "        if ignore_metadata:\n",
        "            for metadata_field in METADATA_FIELDS:\n",
        "                config_dict.pop(metadata_field, None)\n",
        "\n",
        "        def convert_keys_to_string(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {str(key): convert_keys_to_string(value) for key, value in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_keys_to_string(item) for item in obj]\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        def convert_dataclass_to_dict(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {key: convert_dataclass_to_dict(value) for key, value in obj.items()}\n",
        "            elif is_dataclass(obj):\n",
        "                return obj.to_dict()\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        config_dict = convert_keys_to_string(config_dict)\n",
        "        config_dict = convert_dataclass_to_dict(config_dict)\n",
        "\n",
        "        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "    def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True):\n",
        "        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n",
        "            writer.write(self.to_json_string(use_diff=use_diff))\n",
        "\n",
        "    @classmethod\n",
        "    def from_model_config(cls, model_config: PretrainedConfig) -> \"GenerationConfig\":\n",
        "        config_dict = model_config.to_dict()\n",
        "        config_dict.pop(\"_from_model_config\", None)\n",
        "\n",
        "        # Removes all `None` from the model config dict -- this lets the generation config defaults to take hold\n",
        "        config_dict = {key: value for key, value in config_dict.items() if value is not None}\n",
        "\n",
        "        generation_config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)\n",
        "\n",
        "        # Special case: some models have generation attributes set in the decoder. Use them if still unset in the\n",
        "        # generation config (which in turn is defined from the outer attributes of model config).\n",
        "        decoder_config = model_config.get_text_config(decoder=True)\n",
        "        if decoder_config is not model_config:\n",
        "            default_generation_config = GenerationConfig()\n",
        "            decoder_config_dict = decoder_config.to_dict()\n",
        "            for attr in generation_config.to_dict().keys():\n",
        "                is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n",
        "                if attr in decoder_config_dict and is_unset:\n",
        "                    setattr(generation_config, attr, decoder_config_dict[attr])\n",
        "\n",
        "        # If any `output_...` flag is set to `True`, we ensure `return_dict_in_generate` is set to `True`.\n",
        "        if generation_config.return_dict_in_generate is False:\n",
        "            if any(\n",
        "                getattr(generation_config, extra_output_flag, False)\n",
        "                for extra_output_flag in generation_config.extra_output_flags\n",
        "            ):\n",
        "                generation_config.return_dict_in_generate = True\n",
        "\n",
        "        # Hash to detect whether the instance was modified\n",
        "        generation_config._original_object_hash = hash(generation_config)\n",
        "        return generation_config\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        to_remove = []\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "                to_remove.append(key)\n",
        "\n",
        "        # Confirm that the updated instance is still valid\n",
        "        self.validate()\n",
        "\n",
        "        # Remove all the attributes that were updated, without modifying the input dict\n",
        "        unused_kwargs = {key: value for key, value in kwargs.items() if key not in to_remove}\n",
        "        return unused_kwargs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StoppingCriteriaList(list):\n",
        "    #@add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        is_done = torch.full((input_ids.shape[0],), False, device=input_ids.device, dtype=torch.bool)\n",
        "        for criteria in self:\n",
        "            is_done = is_done | criteria(input_ids, scores, **kwargs)\n",
        "        return is_done\n",
        "\n",
        "    @property\n",
        "    def max_length(self) -> Optional[int]:\n",
        "        for stopping_criterium in self:\n",
        "            if isinstance(stopping_criterium, MaxLengthCriteria):\n",
        "                return stopping_criterium.max_length\n",
        "        return None\n",
        "\n",
        "@dataclass\n",
        "class CausalLMOutputWithPast(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: Optional[torch.FloatTensor] = None\n",
        "    past_key_values: Optional[Cache] = None\n",
        "    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "\n",
        "def is_safetensors_available():\n",
        "    return _safetensors_available\n",
        "\n",
        "if is_safetensors_available():\n",
        "    from safetensors import safe_open\n",
        "    from safetensors.torch import load_file as safe_load_file\n",
        "    from safetensors.torch import save_file as safe_save_file"
      ],
      "metadata": {
        "id": "cFfd1HhHzkdg"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "iv_O48i-8slz"
      },
      "outputs": [],
      "source": [
        "_has_opentelemetry = True\n",
        "\n",
        "class RequestStatus(Enum):\n",
        "    PENDING = \"pending\"\n",
        "    PREFILLING = \"prefilling\"\n",
        "    PREFILLING_SPLIT = \"prefilling_split\"\n",
        "    SPLIT_PENDING_REMAINDER = \"split_pending_remainder\"\n",
        "    DECODING = \"decoding\"\n",
        "    FINISHED = \"finished\"\n",
        "    FAILED = \"failed\"\n",
        "\n",
        "@dataclass\n",
        "class GenerationOutput:\n",
        "    request_id: str\n",
        "    prompt_ids: list[int] = field(default_factory=list)\n",
        "    generated_tokens: list[int] = field(default_factory=list)\n",
        "    logprobs: list[float] = field(default_factory=list)\n",
        "    error: Optional[str] = None\n",
        "    status: RequestStatus = RequestStatus.PENDING\n",
        "    created_time: float = field(default_factory=time.time)\n",
        "    next_token: Optional[int] = field(default_factory=int)\n",
        "\n",
        "def attach_tracer(tracer_name_template=None):\n",
        "    if not _has_opentelemetry:\n",
        "        return lambda cls: cls\n",
        "\n",
        "    def decorator(cls):\n",
        "        original_init = cls.__init__\n",
        "\n",
        "        @functools.wraps(original_init)\n",
        "        def init_with_tracer(self, *args, **kwargs):\n",
        "            original_init(self, *args, **kwargs)\n",
        "\n",
        "            module_name = cls.__module__\n",
        "            class_name = cls.__qualname__\n",
        "\n",
        "            if tracer_name_template is None:\n",
        "                if module_name.startswith(\"transformers.\"):\n",
        "                    tracer_name = f\"{module_name}.{class_name}\"\n",
        "                else:\n",
        "                    tracer_name = f\"transformers.{module_name}.{class_name}\"\n",
        "            else:\n",
        "                tracer_name = tracer_name_template.format(module=module_name, class_name=class_name)\n",
        "\n",
        "            self.tracer = get_tracer(tracer_name)\n",
        "\n",
        "        cls.__init__ = init_with_tracer\n",
        "        return cls\n",
        "\n",
        "    return decorator\n",
        "\n",
        "def traced(\n",
        "    func=None,\n",
        "    *,\n",
        "    span_name=None,\n",
        "    standalone=False,\n",
        "    additional_attributes: Optional[list[tuple[str, str, Union[Any, Callable[[Any], Any]]]]] = None,\n",
        "):\n",
        "    def decorator(func):\n",
        "        if not _has_opentelemetry:\n",
        "            return func\n",
        "\n",
        "        import functools\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            instance = args[0] if args and (hasattr(func, \"__self__\") and func.__self__ is not None) else None\n",
        "            is_method = instance is not None\n",
        "\n",
        "            if is_method and hasattr(instance, \"tracer\"):\n",
        "                tracer = instance.tracer\n",
        "            else:\n",
        "                tracer = get_tracer(f\"transformers.{func.__module__}.{func.__name__}\")\n",
        "\n",
        "            name = span_name or func.__name__\n",
        "            span_fn = tracer.start_span if standalone else tracer.start_as_current_span\n",
        "            with span_fn(name) as span:\n",
        "                span.set_attribute(\"function.name\", func.__name__)\n",
        "                span.set_attribute(\"function.module\", func.__module__)\n",
        "                span.set_attribute(\"function.is_method\", is_method)\n",
        "\n",
        "                if args:\n",
        "                    for i, arg in enumerate(args):\n",
        "                        if isinstance(arg, (str, int, float, bool)) or arg is None:\n",
        "                            span.set_attribute(f\"args.{i}\", str(arg))\n",
        "                        else:\n",
        "                            span.set_attribute(f\"args.{i}\", str(type(arg)))\n",
        "                if kwargs:\n",
        "                    for key, value in kwargs.items():\n",
        "                        if isinstance(value, (str, int, float, bool)) or value is None:\n",
        "                            span.set_attribute(f\"kwargs.{key}\", str(value))\n",
        "                        else:\n",
        "                            span.set_attribute(f\"kwargs.{key}\", str(type(value)))\n",
        "\n",
        "                if additional_attributes and is_method:\n",
        "                    for attr_config in additional_attributes:\n",
        "                        instance_attribute_name, span_attribute_key, value_or_transform_function = attr_config\n",
        "                        if hasattr(instance, instance_attribute_name):\n",
        "                            attribute_value = getattr(instance, instance_attribute_name)\n",
        "                            if callable(value_or_transform_function):\n",
        "                                transformed_value = value_or_transform_function(attribute_value)\n",
        "                            else:\n",
        "                                transformed_value = value_or_transform_function\n",
        "                            span.set_attribute(span_attribute_key, transformed_value)\n",
        "\n",
        "                try:\n",
        "                    result = func(*args, **kwargs)\n",
        "                    return result\n",
        "                except Exception as e:\n",
        "                    span.set_status(Status(StatusCode.ERROR))\n",
        "                    span.record_exception(e)\n",
        "                    raise\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    if func is None:\n",
        "        return decorator\n",
        "    return decorator(func)\n",
        "\n",
        "@dataclass\n",
        "class RequestState:\n",
        "    request_id: str\n",
        "    prompt_ids: Optional[list[int]] = None  # the one being processed\n",
        "    full_prompt_ids: Optional[list[int]] = None  # the full prompt\n",
        "    remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests\n",
        "    static_outputs: list[int] = field(default_factory=list)\n",
        "    allocated_blocks: list[int] = field(default_factory=list)\n",
        "    position_offset: int = 0  # Current position in the sequence for position_ids\n",
        "    status: RequestStatus = RequestStatus.PENDING\n",
        "    max_new_tokens: int = 20\n",
        "    eos_token_id: int = -1\n",
        "    created_time: float = field(default_factory=time.time)\n",
        "    error: Optional[str] = None\n",
        "    next_token: Optional[str] = None\n",
        "\n",
        "    def current_len(self) -> int:\n",
        "        return self.position_offset\n",
        "\n",
        "    def generated_len(self) -> int:\n",
        "        return len(self.static_outputs)\n",
        "\n",
        "    @traced\n",
        "    def update_with_token(self, token_id: int) -> bool:\n",
        "        if self.status != RequestStatus.DECODING:\n",
        "            return False\n",
        "\n",
        "        is_eos = token_id == self.eos_token_id and self.eos_token_id != -1\n",
        "        is_max_len = self.generated_len() >= self.max_new_tokens\n",
        "\n",
        "        if not (is_max_len and not is_eos):\n",
        "            self.static_outputs.extend([token_id])\n",
        "\n",
        "        if is_eos or is_max_len:\n",
        "            self.status = RequestStatus.FINISHED\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"RequestState(\\n\\trequest_id={self.request_id},\\n\\tstatus={self.status},\\n\\tout_tokens={self.generated_len()},\\n\\tquery_length={len(self.prompt_ids)}, \\n\\tremaining_tokens={len(self.remaining_prompt_ids)}, \\n\\tkv_length={self.position_offset}\\n\\tfull_prompt_lenght={len(self.full_prompt_ids)},\\n\\tallocated_blocks={self.allocated_blocks},\\n\\tgenerated_tokens={self.static_outputs}\\n)\"\n",
        "\n",
        "    def to_generation_output(self):\n",
        "        return GenerationOutput(\n",
        "            request_id=self.request_id,\n",
        "            prompt_ids=self.full_prompt_ids,\n",
        "            status=self.status,\n",
        "            generated_tokens=self.static_outputs,\n",
        "            logprobs=[],\n",
        "            error=self.error,\n",
        "            next_token=self.next_token,\n",
        "        )\n",
        "\n",
        "class PagedAttentionCache(Cache):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: PretrainedConfig,\n",
        "        generation_config: GenerationConfig,\n",
        "        device: torch.device,\n",
        "        dtype: torch.dtype = torch.float16,\n",
        "        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n",
        "        initial_prompt_shapes: Optional[list[list[int]]] = None,\n",
        "        tp_size: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        self.num_key_value_heads = (\n",
        "            config.num_attention_heads\n",
        "            if getattr(config, \"num_key_value_heads\", None) is None\n",
        "            else config.num_key_value_heads\n",
        "        )\n",
        "        self.head_dim = (\n",
        "            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n",
        "        )\n",
        "        self.num_hidden_layers = config.num_hidden_layers\n",
        "\n",
        "        num_blocks = getattr(generation_config, \"num_blocks\", None)\n",
        "        block_size = getattr(generation_config, \"block_size\", None)\n",
        "        if num_blocks is None or block_size is None:\n",
        "            #logger.info(\"Calculating optimal block size and number...\")\n",
        "            num_blocks, block_size = compute_optimal_blocks(\n",
        "                device, config, generation_config, initial_prompt_shapes or [], dtype, median_prefill_length=200\n",
        "            )\n",
        "            #logger.info(f\"Using calculated num_blocks={num_blocks}, block_size={block_size}\")\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.num_blocks = num_blocks\n",
        "        num_key_value_heads = self.num_key_value_heads\n",
        "        if tp_size is not None and tp_size > 1:\n",
        "            if num_key_value_heads % tp_size != 0:\n",
        "                raise ValueError(\n",
        "                    f\"Number of key value heads {num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n",
        "                )\n",
        "            num_key_value_heads //= tp_size\n",
        "\n",
        "        self.cache_shape = (num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n",
        "\n",
        "        self.dtype = dtype\n",
        "        self.device = device\n",
        "\n",
        "        self.key_cache: list[torch.Tensor] = []\n",
        "        self.value_cache: list[torch.Tensor] = []\n",
        "        for idx in range(config.num_hidden_layers):\n",
        "            layer_device = layer_device_map[idx] if layer_device_map is not None else device\n",
        "            new_layer_key_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n",
        "            new_layer_value_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n",
        "            torch._dynamo.mark_static_address(new_layer_key_cache)\n",
        "            torch._dynamo.mark_static_address(new_layer_value_cache)\n",
        "            self.key_cache.append(new_layer_key_cache)\n",
        "            self.value_cache.append(new_layer_value_cache)\n",
        "\n",
        "        self._free_blocks = deque(range(num_blocks))\n",
        "        self._block_tables: dict[str, list[int]] = {}\n",
        "\n",
        "    @traced\n",
        "    def allocate_blocks(self, n_blocks: int, request_id: str) -> list[int]:\n",
        "        if len(self._free_blocks) < n_blocks:\n",
        "            return False\n",
        "\n",
        "        allocated = []\n",
        "        for _ in range(n_blocks):\n",
        "            allocated.append(self._free_blocks.popleft())\n",
        "\n",
        "        if request_id not in self._block_tables:\n",
        "            self._block_tables[request_id] = []\n",
        "        self._block_tables[request_id].extend(allocated)\n",
        "        return allocated\n",
        "\n",
        "    @traced\n",
        "    def free_blocks(self, request_id: str) -> None:\n",
        "        if request_id in self._block_tables:\n",
        "            blocks_to_free = self._block_tables.pop(request_id)\n",
        "            self._free_blocks.extend(blocks_to_free)\n",
        "        #else:\n",
        "            #logger.warning(f\"Attempted to free blocks for non-existent request_id: {request_id}\")\n",
        "\n",
        "    def get_num_free_blocks(self) -> int:\n",
        "        return len(self._free_blocks)\n",
        "\n",
        "    def get_block_table(self, request_id: str) -> list[int]:\n",
        "        return self._block_tables.get(request_id, [])\n",
        "\n",
        "    @traced\n",
        "    def _get_physical_indices(self, state: RequestState, logical_indices: list[int]) -> list[int]:\n",
        "        request_id = state.request_id\n",
        "        block_table = self._block_tables.get(request_id)\n",
        "        if not block_table:\n",
        "            raise ValueError(f\"No block table found for request {request_id}\")\n",
        "\n",
        "        block_size = self.block_size\n",
        "        physical_indices = []\n",
        "\n",
        "        for idx in logical_indices:\n",
        "            block_idx = idx // block_size\n",
        "            block_offset = idx % block_size\n",
        "\n",
        "            if block_idx >= len(block_table):\n",
        "                raise IndexError(\n",
        "                    f\"Logical index {idx} maps to block index {block_idx} which is out of bounds \"\n",
        "                    f\"for request {request_id}\"\n",
        "                )\n",
        "\n",
        "            physical_block_num = block_table[block_idx]\n",
        "            physical_index = physical_block_num * block_size + block_offset\n",
        "            physical_indices.append(physical_index)\n",
        "\n",
        "        return physical_indices\n",
        "\n",
        "    @traced\n",
        "    def update(\n",
        "        self,\n",
        "        key_states: torch.Tensor,\n",
        "        value_states: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "        read_index,\n",
        "        write_index,\n",
        "        **kwargs,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        total_slots = self.num_blocks * self.block_size\n",
        "        k_cache_flat = self.key_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n",
        "        v_cache_flat = self.value_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n",
        "        k_cache_flat[:, write_index, :] = key_states[0]\n",
        "        v_cache_flat[:, write_index, :] = value_states[0]\n",
        "        return k_cache_flat[None, :, read_index, :], v_cache_flat[None, :, read_index, :]\n",
        "\n",
        "@dataclass\n",
        "class PagedAttentionArgs:\n",
        "    input_ids: torch.Tensor\n",
        "    attention_mask: torch.Tensor\n",
        "    position_ids: torch.Tensor\n",
        "    cumulative_seqlens_q: torch.Tensor\n",
        "    cumulative_seqlens_k: torch.Tensor\n",
        "    max_seqlen_q: int\n",
        "    max_seqlen_k: int\n",
        "    write_index: torch.Tensor\n",
        "    read_index: torch.Tensor\n",
        "    logits_indices: torch.Tensor\n",
        "    block_tables: dict[str, list[int]]\n",
        "    cache: PagedAttentionCache\n",
        "    use_cache: bool = False\n",
        "\n",
        "class Scheduler(ABC):\n",
        "    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False):\n",
        "        self.active_requests: dict[str, RequestState] = {}\n",
        "        self.waiting_requests: dict[str, RequestState] = {}\n",
        "        self.waiting_requests_order: deque[str] = deque()\n",
        "        self.cache = cache\n",
        "        self.retain_cache_on_finish = retain_cache_on_finish\n",
        "\n",
        "    @abstractmethod\n",
        "    def add_waiting_request(self, state: RequestState):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n",
        "        pass\n",
        "\n",
        "    @traced\n",
        "    def has_pending_requests(self) -> bool:\n",
        "        return self.active_requests or self.waiting_requests\n",
        "\n",
        "    @abstractmethod\n",
        "    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n",
        "        pass\n",
        "\n",
        "    @traced\n",
        "    def get_active_request_static_outputs(self, request_id: str) -> list[int]:\n",
        "        if request_id in self.active_requests:\n",
        "            return self.active_requests[request_id].static_outputs\n",
        "        return []\n",
        "\n",
        "@attach_tracer()\n",
        "class ContinuousBatchProcessor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        cache: PagedAttentionCache,\n",
        "        config: PretrainedConfig,\n",
        "        generation_config: GenerationConfig,\n",
        "        input_queue: queue.Queue,\n",
        "        output_queue: queue.Queue,\n",
        "        stop_event: threading.Event,\n",
        "        model_device: torch.device,\n",
        "        model_dtype: torch.dtype,\n",
        "        scheduler: Scheduler,\n",
        "        streaming: bool = False,\n",
        "        manual_eviction: bool = False,\n",
        "    ):\n",
        "        self.cache = cache\n",
        "        self.config = config\n",
        "        self.generation_config = generation_config\n",
        "        self.input_queue = input_queue\n",
        "        self.output_queue = output_queue\n",
        "        self.stop_event = stop_event\n",
        "        self.model_device = model_device\n",
        "        self.model_dtype = model_dtype\n",
        "        self.scheduler = scheduler\n",
        "        self.streaming = streaming\n",
        "        self.manual_eviction = manual_eviction\n",
        "\n",
        "        self.requests_in_batch: list[RequestState] = []\n",
        "\n",
        "        self._configure_batch_parameters()\n",
        "\n",
        "        self.metrics = ContinuousBatchProcessorMetrics(self.max_batch_tokens)\n",
        "\n",
        "        self.setup_static_tensors()\n",
        "\n",
        "        self.tokenizer = Tokenizer.from_pretrained(self.config._name_or_path)\n",
        "        self.decode_stream = DecodeStream(skip_special_tokens=True)\n",
        "\n",
        "    @traced(standalone=True)\n",
        "    def setup_static_tensors(self):\n",
        "        T = self.max_batch_tokens\n",
        "        max_token_budget = self.cache.num_blocks * self.cache.block_size\n",
        "        tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n",
        "        self.tensor_metadata = tensor_metadata\n",
        "        self.input_ids = torch.zeros((1, T), **tensor_metadata)\n",
        "        self.position_ids = torch.zeros((1, T), **tensor_metadata)\n",
        "        self.attention_mask = torch.zeros(\n",
        "            (1, 1, T, max_token_budget), dtype=self.model_dtype, device=self.model_device\n",
        "        )\n",
        "        self.cumulative_seqlens_q = torch.zeros((T + 1,), **tensor_metadata)\n",
        "        self.cumulative_seqlens_k = torch.zeros((T + 1,), **tensor_metadata)\n",
        "        self.write_index = torch.zeros((T,), **tensor_metadata)\n",
        "        self.read_index = torch.zeros((max_token_budget,), **tensor_metadata)\n",
        "        self.logits_indices = torch.full((T,), -1, **tensor_metadata)\n",
        "        self.max_seqlen_q = 0\n",
        "        self.max_seqlen_k = 0\n",
        "        self.output_ids = torch.full((1, T), -1, **tensor_metadata)\n",
        "\n",
        "    @traced\n",
        "    @torch.no_grad()\n",
        "    def reset_static_tensors(self):\n",
        "        self.input_ids.zero_()\n",
        "        self.position_ids.zero_()\n",
        "        self.attention_mask.fill_(torch.finfo(self.model_dtype).min)\n",
        "        self.cumulative_seqlens_q.zero_()\n",
        "        self.cumulative_seqlens_k.zero_()\n",
        "        self.write_index.fill_(-1)\n",
        "        self.read_index.fill_(-1)\n",
        "        self.logits_indices.fill_(-1)\n",
        "        self.max_seqlen_q = 0\n",
        "        self.max_seqlen_k = 0\n",
        "        self.output_ids.zero_()\n",
        "\n",
        "    def get_model_kwargs(self) -> PagedAttentionArgs:\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids,\n",
        "            \"position_ids\": self.position_ids,\n",
        "            \"attention_mask\": self.attention_mask,\n",
        "            \"cumulative_seqlens_q\": self.cumulative_seqlens_q,\n",
        "            \"cumulative_seqlens_k\": self.cumulative_seqlens_k,\n",
        "            \"write_index\": self.write_index,\n",
        "            \"read_index\": self.read_index,\n",
        "            \"logits_indices\": self.logits_indices,\n",
        "            \"max_seqlen_q\": self.max_seqlen_q,\n",
        "            \"max_seqlen_k\": self.max_seqlen_k,\n",
        "            \"block_tables\": self.cache._block_tables,\n",
        "            \"cache\": self.cache,\n",
        "            \"use_cache\": False,\n",
        "        }\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n",
        "            + self.get_model_kwargs().__repr__()\n",
        "        )\n",
        "\n",
        "    @traced(standalone=True)\n",
        "    def _configure_batch_parameters(self):\n",
        "        total_cache_tokens = self.cache.num_blocks * self.cache.block_size\n",
        "\n",
        "        user_batch_tokens = getattr(self.generation_config, \"max_batch_tokens\", None)\n",
        "        if user_batch_tokens is not None:\n",
        "            self.max_batch_tokens = user_batch_tokens\n",
        "        else:\n",
        "            self.max_context_len = getattr(self.generation_config, \"max_position_embeddings\", 2048)\n",
        "            recommended_batch_size = min(total_cache_tokens // 8, self.max_context_len)\n",
        "            self.max_batch_tokens = max(64, recommended_batch_size)\n",
        "\n",
        "        self.max_context_len = getattr(self.generation_config, \"max_position_embeddings\", 2048)\n",
        "\n",
        "    @traced\n",
        "    def _get_new_requests(self):\n",
        "        while not self.input_queue.empty():\n",
        "            try:\n",
        "                state = self.input_queue.get_nowait()\n",
        "                if state is None:  # Sentinel value\n",
        "                    continue\n",
        "                self.scheduler.add_waiting_request(state)\n",
        "\n",
        "            except queue.Empty:\n",
        "                break\n",
        "            except Exception as e:\n",
        "                #logger.error(f\"Error processing new request: {e}\", exc_info=True)\n",
        "                state: RequestState = locals().get(\"state\")\n",
        "                if state is not None:\n",
        "                    self._handle_request_error(e, state)\n",
        "\n",
        "    @traced\n",
        "    def _handle_request_error(self, error, state: RequestState):\n",
        "        state.status = RequestStatus.FAILED\n",
        "        state.error = str(error)\n",
        "\n",
        "        if isinstance(state.request_id, str):\n",
        "            state.static_outputs = self.scheduler.get_active_request_static_outputs(state.request_id)\n",
        "        else:\n",
        "            state.static_outputs = []\n",
        "\n",
        "        self.metrics.record_request_completion(state.created_time, state.request_id)\n",
        "        self.output_queue.put(state.to_generation_output())\n",
        "\n",
        "    @traced\n",
        "    def prepare_next_batch(self):\n",
        "        self._get_new_requests()\n",
        "        if not self.scheduler.has_pending_requests():\n",
        "            return None\n",
        "\n",
        "        self.metrics.record_queue_metrics(len(self.scheduler.active_requests), len(self.scheduler.waiting_requests))\n",
        "\n",
        "        self.requests_in_batch = self.scheduler.schedule_batch(self.max_batch_tokens)\n",
        "        if not self.requests_in_batch:\n",
        "            return None\n",
        "\n",
        "        self.reset_static_tensors()\n",
        "        position_ids = []\n",
        "        input_ids = []\n",
        "        read_index = []\n",
        "        write_index = []\n",
        "        cumulative_seqlens_q = [0]\n",
        "        cumulative_seqlens_k = [0]\n",
        "        logits_indices = []\n",
        "        self.metrics.record_batch_metrics(self.requests_in_batch)\n",
        "\n",
        "        for state in self.requests_in_batch:\n",
        "            next_input_ids = state.prompt_ids\n",
        "            input_ids.extend(next_input_ids)\n",
        "            past_length = state.position_offset\n",
        "            query_length = len(next_input_ids)\n",
        "            key_length = query_length + past_length\n",
        "            cache_index = list(range(key_length))\n",
        "\n",
        "            positions_to_add = cache_index[past_length:]\n",
        "            read_indices = self.cache._get_physical_indices(state, cache_index)\n",
        "            write_indices = read_indices[-query_length:]\n",
        "\n",
        "            position_ids.extend(positions_to_add)\n",
        "            read_index.extend(read_indices)\n",
        "            write_index.extend(write_indices)\n",
        "            cumulative_seqlens_q.append(cumulative_seqlens_q[-1] + query_length)\n",
        "            cumulative_seqlens_k.append(cumulative_seqlens_k[-1] + key_length)\n",
        "            if len(state.remaining_prompt_ids) == 0:\n",
        "                logits_indices.append(cumulative_seqlens_q[-1] - 1)\n",
        "            self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n",
        "            self.max_seqlen_k = max(self.max_seqlen_k, key_length)\n",
        "            state.position_offset += query_length\n",
        "\n",
        "        #logger.info(\n",
        "            #f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. cum KV: {cumulative_seqlens_k[-1]}, free blocks: {self.cache.get_num_free_blocks()}\"\n",
        "        #)\n",
        "        self._build_tensors(\n",
        "            input_ids,\n",
        "            position_ids,\n",
        "            read_index,\n",
        "            write_index,\n",
        "            cumulative_seqlens_q,\n",
        "            cumulative_seqlens_k,\n",
        "            logits_indices,\n",
        "        )\n",
        "\n",
        "        self.metrics.record_kv_cache_memory_metrics(self.cache)\n",
        "\n",
        "    @traced\n",
        "    def _build_tensors(\n",
        "        self,\n",
        "        input_ids,\n",
        "        position_ids,\n",
        "        read_index,\n",
        "        write_index,\n",
        "        cumulative_seqlens_q,\n",
        "        cumulative_seqlens_k,\n",
        "        logits_indices,\n",
        "    ):\n",
        "        to_tensor = partial(torch.tensor, **self.tensor_metadata)\n",
        "        self.input_ids[:, : len(input_ids)] = to_tensor(input_ids)\n",
        "        self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n",
        "        self.write_index[: len(write_index)] = to_tensor(write_index)\n",
        "        self.read_index[: len(read_index)] = to_tensor(read_index)\n",
        "        self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n",
        "        self.cumulative_seqlens_k[: len(cumulative_seqlens_k)] = to_tensor(cumulative_seqlens_k)\n",
        "        self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n",
        "        min_value = torch.finfo(self.model_dtype).min\n",
        "        if self.config._attn_implementation != \"paged_attention\":  # we set `is_causal` to True in paged call`\n",
        "            for i in range(len(cumulative_seqlens_q) - 1):\n",
        "                if (\n",
        "                    cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]\n",
        "                    < cumulative_seqlens_k[i + 1] - cumulative_seqlens_k[i]\n",
        "                    and cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i] >= 1\n",
        "                ):\n",
        "                    diagonal = (\n",
        "                        cumulative_seqlens_k[i + 1] - (cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]) + 1\n",
        "                    )\n",
        "                    diagonal = diagonal - cumulative_seqlens_k[i]\n",
        "                else:\n",
        "                    diagonal = 1\n",
        "                query_range = slice(cumulative_seqlens_q[i], cumulative_seqlens_q[i + 1])\n",
        "                key_range = slice(cumulative_seqlens_k[i], cumulative_seqlens_k[i + 1])\n",
        "\n",
        "                mask = torch.triu(\n",
        "                    torch.full(\n",
        "                        self.attention_mask[..., query_range, key_range].shape,\n",
        "                        min_value,\n",
        "                        dtype=self.model_dtype,\n",
        "                        device=self.model_device,\n",
        "                    ),\n",
        "                    diagonal=diagonal,\n",
        "                )\n",
        "                self.attention_mask[..., query_range, key_range] = mask\n",
        "\n",
        "    @traced\n",
        "    def _sync(self):\n",
        "        return self.output_ids.tolist()[0]  # should be the only synch we do\n",
        "\n",
        "    @traced\n",
        "    def _maybe_send_output(self, state: RequestState, token: int):\n",
        "        if self.streaming:\n",
        "            state.next_token = self.decode_stream.step(self.tokenizer, state.static_outputs[-1])\n",
        "            self.output_queue.put(state.to_generation_output())\n",
        "        elif state.status == RequestStatus.FINISHED:\n",
        "            self.output_queue.put(state.to_generation_output())\n",
        "\n",
        "    @traced\n",
        "    def update_batch(self):\n",
        "        out_tokens = self._sync()\n",
        "        finished_request_ids = []\n",
        "        for i, state in enumerate(self.requests_in_batch):\n",
        "            req_id = state.request_id\n",
        "            if len(state.remaining_prompt_ids) == 0:\n",
        "                self.metrics.record_ttft_metric(state.created_time, state.request_id)\n",
        "                state.status = RequestStatus.DECODING\n",
        "                token = out_tokens[self.logits_indices[i]]\n",
        "                state.prompt_ids = [token]\n",
        "                if state.update_with_token(token):\n",
        "                    self.metrics.record_request_completion(state.created_time, state.request_id)\n",
        "                    self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n",
        "                    finished_request_ids.append(req_id)\n",
        "                self._maybe_send_output(state, token)\n",
        "            elif state.status == RequestStatus.PREFILLING_SPLIT:\n",
        "                state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n",
        "\n",
        "    @traced\n",
        "    def has_pending_requests(self) -> bool:\n",
        "        return self.scheduler.has_pending_requests()\n",
        "\n",
        "    @traced\n",
        "    def handle_batch_error(self, error):\n",
        "        failed_reqs = self.requests_in_batch\n",
        "        for req in failed_reqs:\n",
        "            self._handle_request_error(error, req)\n",
        "            self.scheduler.finish_request(req.request_id)\n",
        "\n",
        "    @traced\n",
        "    def fail_all_requests(self, error):\n",
        "        for state in self.scheduler.active_requests.values():\n",
        "            self._handle_request_error(error, state)\n",
        "            self.scheduler.finish_request(state.request_id)\n",
        "\n",
        "        for req_id in list(self.scheduler.waiting_requests.keys()):\n",
        "            state = self.scheduler.waiting_requests.pop(req_id)\n",
        "            self._handle_request_error(error, state)\n",
        "\n",
        "        self.scheduler.waiting_requests_order.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "uoEGmcOvWdeR"
      },
      "outputs": [],
      "source": [
        "class ContinuousBatchingManager:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        generation_config: GenerationConfig,\n",
        "        manual_eviction: bool = False,\n",
        "        max_queue_size=0,\n",
        "        streaming: bool = True,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.generation_config = generation_config\n",
        "        self.input_queue = queue.Queue(maxsize=max_queue_size)\n",
        "        self.output_queue = queue.Queue()\n",
        "        self.stop_event = threading.Event()\n",
        "        self.streaming = streaming\n",
        "        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n",
        "        self._generation_thread = None\n",
        "        self._request_counter = 0\n",
        "        self._request_lock = threading.Lock()\n",
        "        self.model.generation_config.top_p = None\n",
        "        self.do_sample = getattr(generation_config, \"do_sample\", True)\n",
        "        self.logit_processor = self.model._get_logits_processor(self.model.generation_config)\n",
        "        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n",
        "        self.profile = getattr(generation_config, \"profile\", False)\n",
        "        self.manual_eviction = manual_eviction\n",
        "        self.batch_processor: Optional[ContinuousBatchProcessor] = None\n",
        "        self.decode_stream = DecodeStream(skip_special_tokens=True)\n",
        "\n",
        "    @traced\n",
        "    def start(self):\n",
        "        if self._generation_thread is not None and self._generation_thread.is_alive():\n",
        "            #logger.warning(\"Manager thread is already running.\")\n",
        "            return\n",
        "\n",
        "        self._result_queue = queue.Queue()\n",
        "        self._generation_thread = threading.Thread(target=self._run_generation_loop)\n",
        "        self._generation_thread.start()\n",
        "        #logger.info(\"Continuous batching manager started.\")\n",
        "\n",
        "    def is_running(self):\n",
        "        return self._generation_thread is not None and self._generation_thread.is_alive()\n",
        "\n",
        "    def stop(self, block: bool = False, timeout: Optional[float] = None):\n",
        "        if self._generation_thread is None:\n",
        "            #logger.warning(\"Manager not started.\")\n",
        "            return\n",
        "\n",
        "        if not self.stop_event.is_set():\n",
        "            self.stop_event.set()\n",
        "            #logger.info(\"Stopping continuous batching manager...\")\n",
        "\n",
        "        if block:\n",
        "            self.join(timeout)\n",
        "\n",
        "    def join(self, timeout: Optional[float] = None):\n",
        "        if self._generation_thread is not None:\n",
        "            self._generation_thread.join(timeout=timeout)\n",
        "            #if self._generation_thread.is_alive():\n",
        "                #logger.warning(\"Generation thread did not exit after join timeout.\")\n",
        "            #else:\n",
        "                #logger.info(\"Continuous Batching Manager stopped.\")\n",
        "            self._generation_thread = None\n",
        "\n",
        "    def add_request(\n",
        "        self, input_ids: list[int], request_id: Optional[str] = None, max_new_tokens: Optional[int] = None\n",
        "    ) -> str:\n",
        "        if request_id is None:\n",
        "            with self._request_lock:\n",
        "                request_id = f\"req_{self._request_counter}\"\n",
        "                self._request_counter += 1\n",
        "\n",
        "        max_new_tokens = self.generation_config.max_new_tokens if max_new_tokens is None else max_new_tokens\n",
        "\n",
        "        state = RequestState(\n",
        "            request_id=request_id,\n",
        "            prompt_ids=list(input_ids),\n",
        "            full_prompt_ids=list(input_ids),\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=self.generation_config.eos_token_id,\n",
        "        )\n",
        "\n",
        "        # Use block=True with timeout to handle backpressure if queue is full\n",
        "        self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n",
        "        #logger.debug(f\"Added request {request_id} to queue.\")\n",
        "        return request_id\n",
        "\n",
        "    def add_requests(self, inputs: list[list[int]], **kwargs):\n",
        "        for i, input_ids in enumerate(inputs):\n",
        "            # Assign a predictable request ID for ordering results later\n",
        "            req_id = f\"batch_req_{i}\"\n",
        "            self.add_request(input_ids, request_id=req_id, **kwargs)\n",
        "\n",
        "    def get_result(self, timeout=None) -> Optional[GenerationOutput]:\n",
        "        if self._generation_thread is None and self.output_queue.empty():\n",
        "            return None\n",
        "        try:\n",
        "            result = self.output_queue.get(block=True, timeout=timeout)\n",
        "            #logger.debug(f\"Retrieved result for request {result.request_id}\")\n",
        "            return result\n",
        "        except queue.Empty:\n",
        "            return None\n",
        "\n",
        "    def __iter__(self):\n",
        "        while (\n",
        "            self._generation_thread is not None and self._generation_thread.is_alive() or not self.output_queue.empty()\n",
        "        ):\n",
        "            result = self.get_result(timeout=0.1)  # allow the model to run for 10 seconds\n",
        "            if result is not None:\n",
        "                yield result\n",
        "\n",
        "    @traced\n",
        "    def warmup(self, batch_processor):\n",
        "        stream = torch.cuda.Stream()\n",
        "        stream.wait_stream(torch.cuda.current_stream())\n",
        "        with torch.cuda.stream(stream):\n",
        "            self._generation_step(batch_processor)\n",
        "        torch.cuda.current_stream().wait_stream(stream)\n",
        "\n",
        "        self.graph = torch.cuda.CUDAGraph()\n",
        "        with torch.cuda.graph(self.graph):\n",
        "            self._generation_step(batch_processor)\n",
        "\n",
        "    @traced\n",
        "    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n",
        "        \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n",
        "        batch_data = batch_processor.get_model_kwargs()\n",
        "        with torch.no_grad():\n",
        "            logits = self._model_forward(batch_data)\n",
        "            if self.log_prob_generation:\n",
        "                batch_processor.output_probs.copy_(logits)  # TODO\n",
        "            probs = self._process_logit(batch_data, logits)\n",
        "            self._sample(batch_processor, probs)\n",
        "\n",
        "    @traced(span_name=\"model_forward\")\n",
        "    def _model_forward(self, batch_data):\n",
        "        return self.model(**batch_data).logits\n",
        "\n",
        "    @traced(span_name=\"logit_processing\")\n",
        "    def _process_logit(self, batch_data, logits):\n",
        "        return self.logit_processor(batch_data[\"input_ids\"], logits)\n",
        "\n",
        "    @traced(span_name=\"sampling\")\n",
        "    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n",
        "        if self.do_sample:  # sample\n",
        "            probs = nn.functional.softmax(probs, dim=-1)\n",
        "            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(1)\n",
        "        else:\n",
        "            next_tokens = torch.argmax(probs, dim=-1)\n",
        "        batch_processor.output_ids.copy_(next_tokens)\n",
        "\n",
        "    def _run_generation_loop(self):\n",
        "        batch_processor = None\n",
        "        try:\n",
        "            paged_attention_cache = PagedAttentionCache(\n",
        "                self.model.config,\n",
        "                self.generation_config,\n",
        "                self.model.device,\n",
        "                self.model.dtype,\n",
        "                tp_size=getattr(self.model, \"tp_size\"),\n",
        "            )\n",
        "\n",
        "            scheduler = None\n",
        "            if hasattr(self.generation_config, \"scheduler\"):\n",
        "                scheduler = SCHEDULER_MAPPING.get(self.generation_config.scheduler)\n",
        "                if scheduler is None:\n",
        "                    #logger.warning(f\"Scheduler '{scheduler}' not found. Defaulting to FIFO.\")\n",
        "                    scheduler = FIFOScheduler\n",
        "            else:\n",
        "                # Default to fifo\n",
        "                scheduler = FIFOScheduler\n",
        "\n",
        "            batch_processor = ContinuousBatchProcessor(\n",
        "                paged_attention_cache,\n",
        "                self.model.config,\n",
        "                self.generation_config,\n",
        "                self.input_queue,\n",
        "                self.output_queue,\n",
        "                self.stop_event,\n",
        "                self.model.device,\n",
        "                self.model.dtype,\n",
        "                scheduler(paged_attention_cache, self.manual_eviction),\n",
        "                self.streaming,\n",
        "                self.manual_eviction,\n",
        "            )\n",
        "            self.batch_processor = batch_processor\n",
        "            is_first = True\n",
        "\n",
        "            if self.profile:\n",
        "                tracing_schedule = schedule(skip_first=2, warmup=3, active=200, repeat=100, wait=1)\n",
        "                trace_handler = tensorboard_trace_handler(\n",
        "                    dir_name=\"/fsx/arthur/transformers\", use_gzip=True, worker_name=\"paged_compile\"\n",
        "                )\n",
        "                activities = [\n",
        "                    torch.profiler.ProfilerActivity.CPU,\n",
        "                    torch.profiler.ProfilerActivity.CUDA,\n",
        "                ]\n",
        "                with profile(\n",
        "                    activities=activities,\n",
        "                    schedule=tracing_schedule,\n",
        "                    on_trace_ready=trace_handler,\n",
        "                    record_shapes=False,\n",
        "                    with_stack=True,\n",
        "                ) as prof:\n",
        "                    while not self.stop_event.is_set() or batch_processor.has_pending_requests():\n",
        "                        self._inner_generation_loop(batch_processor, is_first)\n",
        "                        if is_first:\n",
        "                            is_first = False\n",
        "                        prof.step()\n",
        "            else:\n",
        "                while not self.stop_event.is_set() or batch_processor.has_pending_requests():\n",
        "                    self._inner_generation_loop(batch_processor, is_first)\n",
        "                    if is_first:\n",
        "                        is_first = False\n",
        "\n",
        "        except Exception as e:\n",
        "            #logger.error(f\"Error in generation loop: {e}\", exc_info=True)\n",
        "            self._handle_critical_error(e, batch_processor)\n",
        "        finally:\n",
        "            #logger.info(\"Generation loop finished.\")\n",
        "            pass\n",
        "\n",
        "    @traced(span_name=\"generation_loop\")\n",
        "    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor, is_first: bool = False):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        batch_processor.prepare_next_batch()\n",
        "        if torch.cuda.is_available() and self.use_cuda_graph:\n",
        "            if is_first:\n",
        "                self.warmup(batch_processor)\n",
        "            elif hasattr(self, \"graph\"):\n",
        "                try:\n",
        "                    self._graph_replay()\n",
        "                except Exception as e:\n",
        "                    #logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n",
        "                    batch_processor.handle_batch_error(e)\n",
        "                    return\n",
        "            else:\n",
        "                self._generation_step(batch_processor)\n",
        "        else:\n",
        "            self._generation_step(batch_processor)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        batch_processor.update_batch()\n",
        "\n",
        "    @traced(span_name=\"graph_replay\")\n",
        "    def _graph_replay(self):\n",
        "        self.graph.replay()\n",
        "\n",
        "    @traced\n",
        "    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n",
        "        self.stop_event.set()\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                req_data = self.input_queue.get_nowait()\n",
        "                if batch_processor is not None:\n",
        "                    batch_processor._handle_request_error(error, req_data)\n",
        "        except queue.Empty:\n",
        "            pass\n",
        "\n",
        "        if batch_processor is not None:\n",
        "            batch_processor.fail_all_requests(error)\n",
        "\n",
        "    @traced\n",
        "    def evict_request_from_cache(self, request_id: str):\n",
        "        \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n",
        "        if not self.manual_eviction:\n",
        "            raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n",
        "        if self.batch_processor is not None:\n",
        "            self.batch_processor.scheduler.finish_request(request_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LogitsProcessorList, TemperatureLogitsWarper, TopKLogitsWarper\n",
        "\n",
        "logits_warpers = LogitsProcessorList([\n",
        "    TemperatureLogitsWarper(temperature=0.7),\n",
        "    TopKLogitsWarper(top_k=50)\n",
        "])"
      ],
      "metadata": {
        "id": "rzXRIlM2GM9S"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_CACHE_NAMES = [\n",
        "    \"past_key_values\",  # default\n",
        "    \"cache_params\",  # mamba-based models\n",
        "    \"state\",  # rwkv\n",
        "    \"mems\",  # xlnet\n",
        "    \"past_buckets_states\",  # reformer\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class GenerateDecoderOnlyOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerateEncoderDecoderOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n",
        "    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n",
        "    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerateBeamDecoderOnlyOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    sequences_scores: Optional[torch.FloatTensor] = None\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    beam_indices: Optional[torch.LongTensor] = None\n",
        "    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerateBeamEncoderDecoderOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    sequences_scores: Optional[torch.FloatTensor] = None\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    beam_indices: Optional[torch.LongTensor] = None\n",
        "    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n",
        "    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n",
        "    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]\n",
        "GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]\n",
        "GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]"
      ],
      "metadata": {
        "id": "Q2y7POQPrQj_"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "0IkdyMHdkVCc"
      },
      "outputs": [],
      "source": [
        "class ContinuousMixin:\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        generation_config: Optional[GenerationConfig] = None,\n",
        "        logits_processor: Optional[LogitsProcessorList] = None,\n",
        "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
        "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n",
        "        synced_gpus: Optional[bool] = None,\n",
        "        assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
        "        streamer: Optional[\"BaseStreamer\"] = None,\n",
        "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
        "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
        "        use_model_defaults: Optional[bool] = None,\n",
        "        custom_generate: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "\n",
        "        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n",
        "        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n",
        "\n",
        "        generation_config, model_kwargs = self._prepare_generation_config(\n",
        "            generation_config, use_model_defaults, **kwargs\n",
        "        )\n",
        "        self._validate_model_kwargs(model_kwargs.copy())\n",
        "        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
        "\n",
        "        if synced_gpus is None:\n",
        "            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n",
        "\n",
        "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
        "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
        "\n",
        "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
        "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
        "        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n",
        "\n",
        "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n",
        "            inputs, generation_config.bos_token_id, model_kwargs\n",
        "        )\n",
        "        batch_size = inputs_tensor.shape[0]\n",
        "\n",
        "        device = inputs_tensor.device\n",
        "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
        "\n",
        "        if kwargs_has_attention_mask:\n",
        "            if model_input_name == \"input_ids\" and len(model_kwargs[\"attention_mask\"].shape) > 2:\n",
        "                raise ValueError(\"`attention_mask` passed to `generate` must be 2D.\")\n",
        "\n",
        "        input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
        "\n",
        "        input_ids_length = input_ids.shape[1]\n",
        "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
        "        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n",
        "        generation_config = self._prepare_generated_length(\n",
        "            generation_config=generation_config,\n",
        "            has_default_max_length=has_default_max_length,\n",
        "            has_default_min_length=has_default_min_length,\n",
        "            model_input_name=model_input_name,\n",
        "            inputs_tensor=inputs_tensor,\n",
        "            input_ids_length=input_ids_length,\n",
        "        )\n",
        "\n",
        "        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n",
        "            model_kwargs[\"logits_to_keep\"] = 1\n",
        "\n",
        "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
        "\n",
        "        max_cache_length = generation_config.max_length - 1\n",
        "\n",
        "        self._prepare_cache_for_generation(\n",
        "            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n",
        "        )\n",
        "\n",
        "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
        "\n",
        "        prepared_logits_processor = self._get_logits_processor(\n",
        "            generation_config=generation_config,\n",
        "            input_ids_seq_length=input_ids_length,\n",
        "            encoder_input_ids=inputs_tensor,\n",
        "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "            logits_processor=logits_processor,\n",
        "            device=inputs_tensor.device,\n",
        "            model_kwargs=model_kwargs,\n",
        "            negative_prompt_ids=negative_prompt_ids,\n",
        "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
        "        )\n",
        "        prepared_stopping_criteria = self._get_stopping_criteria(\n",
        "            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n",
        "        )\n",
        "\n",
        "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
        "\n",
        "        if generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n",
        "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
        "                input_ids=input_ids,\n",
        "                expand_size=generation_config.num_return_sequences,\n",
        "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "\n",
        "            result = self._sample(\n",
        "                input_ids,\n",
        "                logits_processor=prepared_logits_processor,\n",
        "                stopping_criteria=prepared_stopping_criteria,\n",
        "                generation_config=generation_config,\n",
        "                synced_gpus=synced_gpus,\n",
        "                streamer=streamer,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "299249F6Ww9U"
      },
      "outputs": [],
      "source": [
        "class GenerationMixin(ContinuousMixin):\n",
        "\n",
        "    def _prepare_cache_for_generation(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        model_kwargs: dict,\n",
        "        assistant_model: \"PreTrainedModel\",\n",
        "        batch_size: int,\n",
        "        max_cache_length: int,\n",
        "        device: torch.device,\n",
        "    ) -> bool:\n",
        "        is_hybrid_cache = any(class_name in self.__class__.__name__.lower() for class_name in [\"mamba\", \"falconh1\"])\n",
        "        cache_name = \"past_key_values\" if not is_hybrid_cache else \"cache_params\"\n",
        "\n",
        "        requires_cross_attention_cache = (\n",
        "            self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n",
        "        )\n",
        "\n",
        "        user_defined_cache = model_kwargs.get(cache_name)\n",
        "        if user_defined_cache is not None:\n",
        "            if generation_config.cache_implementation is not None:\n",
        "                raise ValueError(\n",
        "                    f\"Passing both `cache_implementation` (used to initialize certain caches) and `{cache_name}` (a \"\n",
        "                    \"Cache object) is unsupported. Please use only one of the two.\"\n",
        "                )\n",
        "            if isinstance(user_defined_cache, tuple) and self._supports_default_dynamic_cache():\n",
        "                model_kwargs[cache_name] = (\n",
        "                    DynamicCache.from_legacy_cache(user_defined_cache)\n",
        "                    if not requires_cross_attention_cache\n",
        "                    else EncoderDecoderCache.from_legacy_cache(user_defined_cache)\n",
        "                )\n",
        "            return\n",
        "\n",
        "        if generation_config.use_cache is False:\n",
        "            return\n",
        "\n",
        "        if not self._supports_default_dynamic_cache():\n",
        "            if generation_config.cache_implementation is not None:\n",
        "                warnings.warn(\n",
        "                    \"This model does not support `Cache` instances, it only supports the legacy cache format (tuple \"\n",
        "                    f\"of tuples). `cache_implementation` (set to {generation_config.cache_implementation}) will be \"\n",
        "                    \"ignored.\",\n",
        "                    UserWarning,\n",
        "                )\n",
        "            return\n",
        "\n",
        "        if assistant_model is not None and generation_config.cache_implementation is not None:\n",
        "            #logger.warning_once(\n",
        "                #\"An assistant model is provided, using a dynamic cache instead of a cache of type=\"\n",
        "                #f\"'{generation_config.cache_implementation}'.\"\n",
        "            #)\n",
        "            generation_config.cache_implementation = None\n",
        "\n",
        "        generation_config.cache_implementation = generation_config.cache_implementation or getattr(\n",
        "            self.config.get_text_config(), \"cache_implementation\", None\n",
        "        )\n",
        "        if generation_config.cache_implementation is not None:\n",
        "            if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n",
        "                if generation_config.cache_implementation == \"static\" and not self._supports_static_cache:\n",
        "                    raise ValueError(\n",
        "                        \"This model does not support `cache_implementation='static'`. Please check the following \"\n",
        "                        \"issue: https://github.com/huggingface/transformers/issues/28981\"\n",
        "                    )\n",
        "                model_kwargs[cache_name] = self._get_cache(\n",
        "                    cache_implementation=generation_config.cache_implementation,\n",
        "                    batch_size=max(generation_config.num_beams, generation_config.num_return_sequences) * batch_size,\n",
        "                    max_cache_len=max_cache_length,\n",
        "                    device=device,\n",
        "                    model_kwargs=model_kwargs,\n",
        "                )\n",
        "            elif generation_config.cache_implementation == \"quantized\":\n",
        "                if self.config.is_encoder_decoder or not self._supports_default_dynamic_cache():\n",
        "                    raise ValueError(\n",
        "                        \"This model does not support the quantized cache. If you want your model to support quantized \"\n",
        "                        \"cache, please open an issue and tag @zucchini-nlp.\"\n",
        "                    )\n",
        "\n",
        "                cache_config = (\n",
        "                    generation_config.cache_config\n",
        "                    if generation_config.cache_config is not None\n",
        "                    else QuantizedCacheConfig()\n",
        "                )\n",
        "                cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]\n",
        "\n",
        "                if cache_config.backend == \"quanto\" and not is_optimum_quanto_available():\n",
        "                    raise ImportError(\n",
        "                        \"You need to install optimum-quanto in order to use KV cache quantization with optimum-quanto backend. \"\n",
        "                        \"Please install it via  with `pip install optimum-quanto`\"\n",
        "                    )\n",
        "                elif cache_config.backend == \"HQQ\" and not is_hqq_available():\n",
        "                    raise ImportError(\n",
        "                        \"You need to install `HQQ` in order to use KV cache quantization with HQQ backend. \"\n",
        "                        \"Please install it via  with `pip install hqq`\"\n",
        "                    )\n",
        "\n",
        "                model_kwargs[cache_name] = cache_class(cache_config)\n",
        "            elif generation_config.cache_implementation == \"offloaded\":\n",
        "                model_kwargs[cache_name] = OffloadedCache()\n",
        "            elif generation_config.cache_implementation == \"dynamic\":\n",
        "                model_kwargs[cache_name] = DynamicCache()\n",
        "\n",
        "        else:\n",
        "            model_kwargs[cache_name] = (\n",
        "                DynamicCache()\n",
        "                if not requires_cross_attention_cache\n",
        "                else EncoderDecoderCache(DynamicCache(), DynamicCache())\n",
        "            )\n",
        "\n",
        "    def _validate_generated_length(self, generation_config, input_ids_length, has_default_max_length):\n",
        "        if has_default_max_length and generation_config.max_new_tokens is None and generation_config.max_length == 20:\n",
        "            # 20 is the default max_length of the generation config\n",
        "            warnings.warn(\n",
        "                f\"Using the model-agnostic default `max_length` (={generation_config.max_length}) to control the \"\n",
        "                \"generation length. We recommend setting `max_new_tokens` to control the maximum length of the \"\n",
        "                \"generation.\",\n",
        "                UserWarning,\n",
        "            )\n",
        "        if input_ids_length >= generation_config.max_length:\n",
        "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
        "            raise ValueError(\n",
        "                f\"Input length of {input_ids_string} is {input_ids_length}, but `max_length` is set to\"\n",
        "                f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n",
        "                \" increasing `max_length` or, better yet, setting `max_new_tokens`.\"\n",
        "            )\n",
        "\n",
        "        # 2. Min length warnings due to unfeasible parameter combinations\n",
        "        min_length_error_suffix = (\n",
        "            \" Generation will stop at the defined maximum length. You should decrease the minimum length and/or \"\n",
        "            \"increase the maximum length.\"\n",
        "        )\n",
        "        if has_default_max_length:\n",
        "            min_length_error_suffix += (\n",
        "                f\" Note that `max_length` is set to {generation_config.max_length}, its default value.\"\n",
        "            )\n",
        "        if generation_config.min_length is not None and generation_config.min_length > generation_config.max_length:\n",
        "            warnings.warn(\n",
        "                f\"Unfeasible length constraints: `min_length` ({generation_config.min_length}) is larger than\"\n",
        "                f\" the maximum possible length ({generation_config.max_length}).\" + min_length_error_suffix,\n",
        "                UserWarning,\n",
        "            )\n",
        "        if generation_config.min_new_tokens is not None:\n",
        "            min_length = generation_config.min_new_tokens + input_ids_length\n",
        "            if min_length > generation_config.max_length:\n",
        "                warnings.warn(\n",
        "                    f\"Unfeasible length constraints: `min_new_tokens` ({generation_config.min_new_tokens}), when \"\n",
        "                    f\"added to the prompt length ({input_ids_length}), is larger than\"\n",
        "                    f\" the maximum possible length ({generation_config.max_length}).\" + min_length_error_suffix,\n",
        "                    UserWarning,\n",
        "                )\n",
        "\n",
        "    def _supports_logits_to_keep(self) -> bool:\n",
        "        return \"logits_to_keep\" in set(inspect.signature(self.forward).parameters.keys())\n",
        "\n",
        "    def _prepare_generated_length(\n",
        "        self,\n",
        "        generation_config,\n",
        "        has_default_max_length,\n",
        "        has_default_min_length,\n",
        "        model_input_name,\n",
        "        input_ids_length,\n",
        "        inputs_tensor,\n",
        "    ):\n",
        "\n",
        "        if generation_config.max_new_tokens is not None:\n",
        "            #if not has_default_max_length and generation_config.max_length is not None:\n",
        "                #logger.warning(\n",
        "                    #f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
        "                    #f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
        "                    #\"Please refer to the documentation for more information. \"\n",
        "                    #\"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n",
        "                #)\n",
        "            generation_config.max_length = generation_config.max_new_tokens + input_ids_length\n",
        "\n",
        "        elif (\n",
        "            model_input_name == \"inputs_embeds\"\n",
        "            and input_ids_length != inputs_tensor.shape[1]\n",
        "            and not self.config.is_encoder_decoder\n",
        "        ):\n",
        "            generation_config.max_length -= inputs_tensor.shape[1]\n",
        "        elif has_default_max_length:  # by default let's always generate 20 new tokens\n",
        "            if generation_config.max_length == GenerationConfig().max_length:\n",
        "                generation_config.max_length = generation_config.max_length + input_ids_length\n",
        "                max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n",
        "                if max_position_embeddings is not None:\n",
        "                    generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n",
        "\n",
        "        # same for min length\n",
        "        if generation_config.min_new_tokens is not None:\n",
        "            #if not has_default_min_length:\n",
        "                #logger.warning(\n",
        "                    #f\"Both `min_new_tokens` (={generation_config.min_new_tokens}) and `min_length`(=\"\n",
        "                    #f\"{generation_config.min_length}) seem to have been set. `min_new_tokens` will take precedence. \"\n",
        "                    #\"Please refer to the documentation for more information. \"\n",
        "                    #\"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n",
        "                #)\n",
        "            generation_config.min_length = generation_config.min_new_tokens + input_ids_length\n",
        "\n",
        "        elif (\n",
        "            model_input_name == \"inputs_embeds\"\n",
        "            and input_ids_length != inputs_tensor.shape[1]\n",
        "            and not self.config.is_encoder_decoder\n",
        "        ):\n",
        "            generation_config.min_length = max(generation_config.min_length - inputs_tensor.shape[1], 0)\n",
        "\n",
        "        return generation_config\n",
        "\n",
        "\n",
        "    def _validate_assistant(self, assistant_model, tokenizer, assistant_tokenizer):\n",
        "        if assistant_model is None:\n",
        "            return\n",
        "\n",
        "        if self.config.is_encoder_decoder and not assistant_model.config.is_encoder_decoder:\n",
        "            attributes_to_check = [\"encoder_attention_heads\", \"encoder_ffn_dim\", \"encoder_layers\"]\n",
        "            attributes_to_check = [attr for attr in dir(assistant_model.config) if attr in attributes_to_check]\n",
        "            are_equal = all(\n",
        "                getattr(self.config, attr) == getattr(assistant_model.config, attr) for attr in attributes_to_check\n",
        "            )\n",
        "            if not are_equal:\n",
        "                raise ValueError(\n",
        "                    \"The main model and the assistant don't have compatible encoder-dependent input shapes. \"\n",
        "                    \"Ensure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.\"\n",
        "                )\n",
        "\n",
        "        doc_reference = (\n",
        "            \"(see https://huggingface.co/docs/transformers/en/generation_strategies#universal-assisted-decoding)\"\n",
        "        )\n",
        "        if self.config.get_text_config().vocab_size == assistant_model.config.get_text_config().vocab_size:\n",
        "            if assistant_tokenizer is not None:\n",
        "                raise ValueError(\n",
        "                    f\"`assistant_tokenizer` is not required when the main and assistant models use the same tokenizer. Please omit `assistant_tokenizer` from `generate()` {doc_reference}.\"\n",
        "                )\n",
        "        else:\n",
        "            if tokenizer is None or assistant_tokenizer is None:\n",
        "                raise ValueError(\n",
        "                    f\"The main and assistant moedels have different tokenizers. Please provide `tokenizer` and `assistant_tokenizer` to `generate()` {doc_reference}.\"\n",
        "                )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # 1. Handle BC:\n",
        "        model_inputs = {}\n",
        "        model_inputs[\"cache_position\"] = cache_position\n",
        "\n",
        "        # 2. Generic cache-dependent input preparation\n",
        "        if past_key_values is not None:\n",
        "            model_inputs[\"past_key_values\"] = past_key_values\n",
        "            inputs_embeds, input_ids = self._cache_dependant_input_preparation(\n",
        "                input_ids, inputs_embeds, cache_position\n",
        "            )\n",
        "\n",
        "        # 3. Prepare base model inputs\n",
        "        input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step for every prompt.\n",
        "        if not self.config.is_encoder_decoder:\n",
        "            if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n",
        "                model_inputs[input_ids_key] = None\n",
        "                model_inputs[\"inputs_embeds\"] = inputs_embeds\n",
        "            else:\n",
        "                # `clone` calls in this function ensure a consistent stride. See #32227\n",
        "                model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n",
        "                model_inputs[\"inputs_embeds\"] = None\n",
        "        else:\n",
        "            model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n",
        "\n",
        "        # 4. Create missing `position_ids` on the fly\n",
        "        encoder_attention_mask = attention_mask if self.config.is_encoder_decoder else None\n",
        "        attention_mask = (\n",
        "            kwargs.pop(\"decoder_attention_mask\", None) if self.config.is_encoder_decoder else attention_mask\n",
        "        )\n",
        "        attention_mask_key = \"decoder_attention_mask\" if self.config.is_encoder_decoder else \"attention_mask\"\n",
        "        position_ids_key = \"decoder_position_ids\" if self.config.is_encoder_decoder else \"position_ids\"\n",
        "        if (\n",
        "            attention_mask is not None\n",
        "            and kwargs.get(position_ids_key) is None\n",
        "            and position_ids_key in set(inspect.signature(self.forward).parameters.keys())\n",
        "        ):\n",
        "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "            kwargs[position_ids_key] = position_ids  # placed in kwargs for further processing (see below)\n",
        "\n",
        "        # 5. Slice model inputs if it's an input that should have the same length as `input_ids`\n",
        "        for model_input_name in [\"position_ids\", \"token_type_ids\", \"decoder_position_ids\"]:\n",
        "            model_input = kwargs.get(model_input_name)\n",
        "            if model_input is not None:\n",
        "                if past_key_values is not None:\n",
        "                    current_input_length = (\n",
        "                        model_inputs[\"inputs_embeds\"].shape[1]\n",
        "                        if model_inputs.get(\"inputs_embeds\") is not None\n",
        "                        else model_inputs[input_ids_key].shape[1]\n",
        "                    )\n",
        "                    model_input = model_input[:, -current_input_length:]\n",
        "                    model_input = model_input.clone(memory_format=torch.contiguous_format)\n",
        "                model_inputs[model_input_name] = model_input\n",
        "\n",
        "        # 6. Create 4D attention mask is we are using a compilable cache (important for performant compiled forward\n",
        "        # pass)\n",
        "        if (\n",
        "            isinstance(past_key_values, Cache)\n",
        "            and past_key_values.is_compileable\n",
        "            and attention_mask is not None\n",
        "            and attention_mask.ndim == 2\n",
        "        ):\n",
        "            if not self.config.is_encoder_decoder and model_inputs[\"inputs_embeds\"] is not None:\n",
        "                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n",
        "            else:\n",
        "                batch_size, sequence_length = model_inputs[input_ids_key].shape[:2]\n",
        "\n",
        "            base_model = getattr(self, self.base_model_prefix, self)\n",
        "            decoder = base_model.get_decoder() if hasattr(base_model, \"get_decoder\") else None\n",
        "            causal_mask_creation_function = getattr(\n",
        "                base_model, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n",
        "            )\n",
        "            if causal_mask_creation_function is None and decoder is not None:  # it may be in the decoder\n",
        "                causal_mask_creation_function = getattr(\n",
        "                    decoder, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n",
        "                )\n",
        "\n",
        "            # If it's not defined, it means the model uses the new general mask API\n",
        "            if causal_mask_creation_function is None:  # can't be found\n",
        "                token_type_ids = getattr(model_input, \"token_type_ids\", None)\n",
        "                position_ids = getattr(model_input, position_ids_key, None)\n",
        "                # Some models may overwrite the general one\n",
        "                causal_mask_creation_function = getattr(self, \"create_masks_for_generate\", create_masks_for_generate)\n",
        "                attention_mask = causal_mask_creation_function(\n",
        "                    config=self.config,\n",
        "                    # we only need batch size, seq_length and dtype here - we don't care about the values of the embeddings\n",
        "                    input_embeds=torch.empty((batch_size, sequence_length), dtype=self.dtype),\n",
        "                    attention_mask=attention_mask,\n",
        "                    cache_position=cache_position,\n",
        "                    past_key_values=past_key_values,\n",
        "                    position_ids=position_ids,\n",
        "                    token_type_ids=token_type_ids,\n",
        "                )\n",
        "            else:\n",
        "                attention_mask = causal_mask_creation_function(\n",
        "                    attention_mask,\n",
        "                    sequence_length=sequence_length,\n",
        "                    target_length=past_key_values.get_max_cache_shape(),\n",
        "                    dtype=self.dtype,\n",
        "                    cache_position=cache_position,\n",
        "                    batch_size=batch_size,\n",
        "                    config=self.config,\n",
        "                    past_key_values=past_key_values,\n",
        "                )\n",
        "        if attention_mask is not None:\n",
        "            model_inputs[attention_mask_key] = attention_mask\n",
        "\n",
        "        if encoder_attention_mask is not None:\n",
        "            model_inputs[\"attention_mask\"] = encoder_attention_mask\n",
        "\n",
        "        # 7. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n",
        "        for key, value in kwargs.items():\n",
        "            if key not in model_inputs:\n",
        "                model_inputs[key] = value\n",
        "\n",
        "        # 8. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n",
        "        model_inputs.pop(\"labels\", None)\n",
        "        return model_inputs\n",
        "\n",
        "    def _get_logits_processor(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        input_ids_seq_length: Optional[int] = None,\n",
        "        encoder_input_ids: torch.LongTensor = None,\n",
        "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n",
        "        logits_processor: Optional[LogitsProcessorList] = None,\n",
        "        device: Optional[str] = None,\n",
        "        model_kwargs: Optional[dict[str, Any]] = None,\n",
        "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
        "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> LogitsProcessorList:\n",
        "        processors = LogitsProcessorList()\n",
        "        if logits_processor is None:\n",
        "            logits_processor = []\n",
        "\n",
        "        if generation_config.guidance_scale is not None and generation_config.guidance_scale != 1:\n",
        "            processors.append(\n",
        "                UnbatchedClassifierFreeGuidanceLogitsProcessor(\n",
        "                    generation_config.guidance_scale,\n",
        "                    self,\n",
        "                    unconditional_ids=negative_prompt_ids,\n",
        "                    unconditional_attention_mask=negative_prompt_attention_mask,\n",
        "                    use_cache=generation_config.use_cache,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.sequence_bias is not None:\n",
        "            processors.append(SequenceBiasLogitsProcessor(sequence_bias=generation_config.sequence_bias))\n",
        "\n",
        "        if generation_config.diversity_penalty is not None and generation_config.diversity_penalty > 0.0:\n",
        "            processors.append(\n",
        "                HammingDiversityLogitsProcessor(\n",
        "                    diversity_penalty=generation_config.diversity_penalty,\n",
        "                    num_beams=generation_config.num_beams,\n",
        "                    num_beam_groups=generation_config.num_beam_groups,\n",
        "                )\n",
        "            )\n",
        "        if (\n",
        "            generation_config.encoder_repetition_penalty is not None\n",
        "            and generation_config.encoder_repetition_penalty != 1.0\n",
        "        ):\n",
        "            if len(encoder_input_ids.shape) == 2:\n",
        "                processors.append(\n",
        "                    EncoderRepetitionPenaltyLogitsProcessor(\n",
        "                        penalty=generation_config.encoder_repetition_penalty,\n",
        "                        encoder_input_ids=encoder_input_ids,\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                warnings.warn(\n",
        "                    \"Passing `encoder_repetition_penalty` requires some form of `input_ids` to be passed to \"\n",
        "                    \"`generate`, ignoring the argument.\",\n",
        "                    UserWarning,\n",
        "                )\n",
        "        if generation_config.repetition_penalty is not None and generation_config.repetition_penalty != 1.0:\n",
        "            processors.append(RepetitionPenaltyLogitsProcessor(penalty=generation_config.repetition_penalty))\n",
        "        if generation_config.no_repeat_ngram_size is not None and generation_config.no_repeat_ngram_size > 0:\n",
        "            processors.append(NoRepeatNGramLogitsProcessor(generation_config.no_repeat_ngram_size))\n",
        "        if (\n",
        "            generation_config.encoder_no_repeat_ngram_size is not None\n",
        "            and generation_config.encoder_no_repeat_ngram_size > 0\n",
        "        ):\n",
        "            if len(encoder_input_ids.shape) == 2:\n",
        "                processors.append(\n",
        "                    EncoderNoRepeatNGramLogitsProcessor(\n",
        "                        generation_config.encoder_no_repeat_ngram_size,\n",
        "                        encoder_input_ids,\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                warnings.warn(\n",
        "                    \"Passing `encoder_no_repeat_ngram_size` requires some form of `input_ids` to be passed to \"\n",
        "                    \"`generate`, ignoring the argument.\",\n",
        "                    UserWarning,\n",
        "                )\n",
        "        if generation_config.bad_words_ids is not None:\n",
        "            processors.append(\n",
        "                NoBadWordsLogitsProcessor(\n",
        "                    generation_config.bad_words_ids,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                )\n",
        "            )\n",
        "        if (\n",
        "            generation_config.min_length is not None\n",
        "            and getattr(generation_config, \"_eos_token_tensor\", None) is not None\n",
        "            and generation_config.min_length > 0\n",
        "        ):\n",
        "            processors.append(\n",
        "                MinLengthLogitsProcessor(\n",
        "                    generation_config.min_length,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if (\n",
        "            generation_config.min_new_tokens is not None\n",
        "            and getattr(generation_config, \"_eos_token_tensor\", None) is not None\n",
        "            and generation_config.min_new_tokens > 0\n",
        "        ):\n",
        "            processors.append(\n",
        "                MinNewTokensLengthLogitsProcessor(\n",
        "                    input_ids_seq_length,\n",
        "                    generation_config.min_new_tokens,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if prefix_allowed_tokens_fn is not None:\n",
        "            processors.append(\n",
        "                PrefixConstrainedLogitsProcessor(\n",
        "                    prefix_allowed_tokens_fn,\n",
        "                    generation_config.num_beams // generation_config.num_beam_groups,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.forced_bos_token_id is not None:\n",
        "            processors.append(\n",
        "                ForcedBOSTokenLogitsProcessor(\n",
        "                    generation_config.forced_bos_token_id,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.forced_eos_token_id is not None:\n",
        "            processors.append(\n",
        "                ForcedEOSTokenLogitsProcessor(\n",
        "                    generation_config.max_length,\n",
        "                    generation_config.forced_eos_token_id,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.remove_invalid_values is True:\n",
        "            processors.append(InfNanRemoveLogitsProcessor())\n",
        "        if generation_config.exponential_decay_length_penalty is not None:\n",
        "            processors.append(\n",
        "                ExponentialDecayLengthPenalty(\n",
        "                    generation_config.exponential_decay_length_penalty,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                    input_ids_seq_length,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.suppress_tokens is not None:\n",
        "            processors.append(\n",
        "                SuppressTokensLogitsProcessor(\n",
        "                    generation_config.suppress_tokens,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.begin_suppress_tokens is not None:\n",
        "            begin_index = input_ids_seq_length\n",
        "            begin_index = (\n",
        "                begin_index\n",
        "                if (input_ids_seq_length > 1 or generation_config.forced_bos_token_id is None)\n",
        "                else begin_index + 1\n",
        "            )\n",
        "            processors.append(\n",
        "                SuppressTokensAtBeginLogitsProcessor(\n",
        "                    generation_config.begin_suppress_tokens,\n",
        "                    begin_index,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        processors = self._merge_criteria_processor_list(processors, logits_processor)\n",
        "\n",
        "        if generation_config.do_sample:\n",
        "            if generation_config.num_beams > 1:\n",
        "                if isinstance(generation_config._eos_token_tensor, list):\n",
        "                    min_tokens_to_keep = len(generation_config._eos_token_tensor) + 1\n",
        "                elif isinstance(generation_config._eos_token_tensor, torch.Tensor):\n",
        "                    min_tokens_to_keep = generation_config._eos_token_tensor.shape[0] + 1\n",
        "                else:\n",
        "                    min_tokens_to_keep = 2\n",
        "            else:\n",
        "                min_tokens_to_keep = 1\n",
        "\n",
        "            if generation_config.temperature is not None and generation_config.temperature != 1.0:\n",
        "                processors.append(TemperatureLogitsWarper(generation_config.temperature))\n",
        "            if generation_config.top_k is not None and generation_config.top_k != 0:\n",
        "                processors.append(\n",
        "                    TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.top_p is not None and generation_config.top_p < 1.0:\n",
        "                processors.append(\n",
        "                    TopPLogitsWarper(top_p=generation_config.top_p, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.min_p is not None:\n",
        "                # Applied after temperature scaling (see https://github.com/ggerganov/llama.cpp/pull/3841#issuecomment-2073826084)\n",
        "                processors.append(\n",
        "                    MinPLogitsWarper(min_p=generation_config.min_p, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.typical_p is not None and generation_config.typical_p < 1.0:\n",
        "                processors.append(\n",
        "                    TypicalLogitsWarper(mass=generation_config.typical_p, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.epsilon_cutoff is not None and 0.0 < generation_config.epsilon_cutoff < 1.0:\n",
        "                processors.append(\n",
        "                    EpsilonLogitsWarper(\n",
        "                        epsilon=generation_config.epsilon_cutoff, min_tokens_to_keep=min_tokens_to_keep\n",
        "                    )\n",
        "                )\n",
        "            if generation_config.eta_cutoff is not None and 0.0 < generation_config.eta_cutoff < 1.0:\n",
        "                processors.append(\n",
        "                    EtaLogitsWarper(\n",
        "                        epsilon=generation_config.eta_cutoff, min_tokens_to_keep=min_tokens_to_keep, device=device\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        if generation_config.watermarking_config is not None:\n",
        "            processors.append(\n",
        "                generation_config.watermarking_config.construct_processor(\n",
        "                    self.config.get_text_config().vocab_size, device\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if generation_config.renormalize_logits is True:\n",
        "            processors.append(LogitNormalization())\n",
        "        return processors\n",
        "\n",
        "    def _merge_criteria_processor_list(\n",
        "        self,\n",
        "        default_list: Union[LogitsProcessorList, StoppingCriteriaList],\n",
        "        custom_list: Union[LogitsProcessorList, StoppingCriteriaList],\n",
        "    ) -> Union[LogitsProcessorList, StoppingCriteriaList]:\n",
        "        if len(custom_list) == 0:\n",
        "            return default_list\n",
        "\n",
        "        final_list = type(default_list)()\n",
        "        for default in default_list:\n",
        "            using_custom = False\n",
        "            for custom in custom_list:\n",
        "                if type(custom) is type(default):\n",
        "                    object_type = \"stopping criteria\" if isinstance(custom, StoppingCriteria) else \"logits processor\"\n",
        "                    #logger.warning_once(\n",
        "                        #f\"A custom {object_type} of type {type(custom)} has been passed to `.generate()`, but it \"\n",
        "                        #f\"was also created in `.generate()`, given its parameterization. The custom {type(custom)} \"\n",
        "                        #f\"will take precedence. Please check the docstring of {type(custom)} to see related \"\n",
        "                        #\"`.generate()` flags.\"\n",
        "                    #)\n",
        "                    final_list.append(custom)\n",
        "                    using_custom = True\n",
        "                    break\n",
        "            if not using_custom:\n",
        "                final_list.append(default)\n",
        "\n",
        "        for custom in custom_list:\n",
        "            if custom not in final_list:\n",
        "                final_list.append(custom)\n",
        "        return final_list\n",
        "\n",
        "    def _get_stopping_criteria(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        stopping_criteria: Optional[StoppingCriteriaList],\n",
        "        tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n",
        "        **kwargs,\n",
        "    ) -> StoppingCriteriaList:\n",
        "        criteria = StoppingCriteriaList()\n",
        "        if generation_config.max_length is not None:\n",
        "            max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n",
        "            criteria.append(\n",
        "                MaxLengthCriteria(\n",
        "                    max_length=generation_config.max_length,\n",
        "                    max_position_embeddings=max_position_embeddings,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.max_time is not None:\n",
        "            criteria.append(MaxTimeCriteria(max_time=generation_config.max_time))\n",
        "        if generation_config.stop_strings is not None:\n",
        "            if tokenizer is None:\n",
        "                raise ValueError(\n",
        "                    \"There are one or more stop strings, either in the arguments to `generate` or in the \"\n",
        "                    \"model's generation config, but we could not locate a tokenizer. When generating with \"\n",
        "                    \"stop strings, you must pass the model's tokenizer to the `tokenizer` argument of `generate`.\"\n",
        "                )\n",
        "            criteria.append(StopStringCriteria(stop_strings=generation_config.stop_strings, tokenizer=tokenizer))\n",
        "        if generation_config._eos_token_tensor is not None:\n",
        "            criteria.append(EosTokenCriteria(eos_token_id=generation_config._eos_token_tensor))\n",
        "        if (\n",
        "            generation_config.is_assistant\n",
        "            and generation_config.assistant_confidence_threshold is not None\n",
        "            and generation_config.assistant_confidence_threshold > 0\n",
        "        ):\n",
        "            criteria.append(\n",
        "                ConfidenceCriteria(assistant_confidence_threshold=generation_config.assistant_confidence_threshold)\n",
        "            )\n",
        "        criteria = self._merge_criteria_processor_list(criteria, stopping_criteria)\n",
        "        return criteria\n",
        "\n",
        "    @staticmethod\n",
        "    def _expand_inputs_for_generation(\n",
        "        expand_size: int = 1,\n",
        "        is_encoder_decoder: bool = False,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        **model_kwargs,\n",
        "    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n",
        "        if expand_size == 1:\n",
        "            return input_ids, model_kwargs\n",
        "\n",
        "        def _expand_dict_for_generation(dict_to_expand):\n",
        "            for key in dict_to_expand:\n",
        "                if (\n",
        "                    key != \"cache_position\"\n",
        "                    and dict_to_expand[key] is not None\n",
        "                    and isinstance(dict_to_expand[key], torch.Tensor)\n",
        "                ):\n",
        "                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n",
        "            return dict_to_expand\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
        "\n",
        "        model_kwargs = _expand_dict_for_generation(model_kwargs)\n",
        "\n",
        "        if is_encoder_decoder:\n",
        "            if model_kwargs.get(\"encoder_outputs\") is None:\n",
        "                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n",
        "            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n",
        "\n",
        "        return input_ids, model_kwargs\n",
        "\n",
        "    def _sample(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        logits_processor: LogitsProcessorList,\n",
        "        stopping_criteria: StoppingCriteriaList,\n",
        "        generation_config: GenerationConfig,\n",
        "        synced_gpus: bool,\n",
        "        streamer: Optional[\"BaseStreamer\"],\n",
        "        **model_kwargs,\n",
        "    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n",
        "        pad_token_id = generation_config._pad_token_tensor\n",
        "        output_attentions = generation_config.output_attentions\n",
        "        output_hidden_states = generation_config.output_hidden_states\n",
        "        output_scores = generation_config.output_scores\n",
        "        output_logits = generation_config.output_logits\n",
        "        return_dict_in_generate = generation_config.return_dict_in_generate\n",
        "        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n",
        "        do_sample = generation_config.do_sample\n",
        "\n",
        "        scores = () if (return_dict_in_generate and output_scores) else None\n",
        "        raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
        "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
        "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
        "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
        "\n",
        "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
        "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
        "            encoder_hidden_states = (\n",
        "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
        "            )\n",
        "\n",
        "        batch_size, cur_len = input_ids.shape[:2]\n",
        "        this_peer_finished = False\n",
        "        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
        "        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n",
        "\n",
        "        model_forward = self.__call__\n",
        "        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n",
        "        if compile_forward:\n",
        "            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
        "            if self.config._attn_implementation == \"flash_attention_2\" and getattr(\n",
        "                model_kwargs.get(\"past_key_values\"), \"is_compileable\", False\n",
        "            ):\n",
        "                if generation_config.compile_config is None:\n",
        "                    generation_config.compile_config = CompileConfig(fullgraph=False)\n",
        "                elif generation_config.compile_config.fullgraph:\n",
        "                    #logger.warning_once(\n",
        "                        #\"When using Flash Attention 2 and a static cache, you cannot use the option `CompileConfig(fullgraph=True)` as \"\n",
        "                        #\"FA2 introduces graph breaks. We overrode the option with `fullgraph=False`.\"\n",
        "                    #)\n",
        "                    generation_config.compile_config.fullgraph = False\n",
        "            model_forward = self.get_compiled_call(generation_config.compile_config)\n",
        "\n",
        "        if generation_config.prefill_chunk_size is not None:\n",
        "            model_kwargs = self._prefill_chunking(input_ids, generation_config, **model_kwargs)\n",
        "            is_prefill = False\n",
        "        else:\n",
        "            is_prefill = True\n",
        "\n",
        "        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "\n",
        "            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n",
        "            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
        "\n",
        "            if is_prefill:\n",
        "                outputs = self(**model_inputs, return_dict=True)\n",
        "                is_prefill = False\n",
        "            else:\n",
        "                outputs = model_forward(**model_inputs, return_dict=True)\n",
        "\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(\n",
        "                outputs,\n",
        "                model_kwargs,\n",
        "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "            )\n",
        "            if synced_gpus and this_peer_finished:\n",
        "                continue\n",
        "\n",
        "            next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)\n",
        "\n",
        "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
        "\n",
        "            if return_dict_in_generate:\n",
        "                if output_scores:\n",
        "                    scores += (next_token_scores,)\n",
        "                if output_logits:\n",
        "                    raw_logits += (next_token_logits,)\n",
        "                if output_attentions:\n",
        "                    decoder_attentions += (\n",
        "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
        "                    )\n",
        "                    if self.config.is_encoder_decoder:\n",
        "                        cross_attentions += (outputs.cross_attentions,)\n",
        "\n",
        "                if output_hidden_states:\n",
        "                    decoder_hidden_states += (\n",
        "                        (outputs.decoder_hidden_states,)\n",
        "                        if self.config.is_encoder_decoder\n",
        "                        else (outputs.hidden_states,)\n",
        "                    )\n",
        "\n",
        "            if do_sample:\n",
        "                probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
        "                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "            else:\n",
        "                next_tokens = torch.argmax(next_token_scores, dim=-1)\n",
        "\n",
        "            if has_eos_stopping_criteria:\n",
        "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "            if streamer is not None:\n",
        "                streamer.put(next_tokens.cpu())\n",
        "\n",
        "            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
        "            this_peer_finished = unfinished_sequences.max() == 0\n",
        "            cur_len += 1\n",
        "\n",
        "            del outputs\n",
        "\n",
        "        if streamer is not None:\n",
        "            streamer.end()\n",
        "\n",
        "        if return_dict_in_generate:\n",
        "            if self.config.is_encoder_decoder:\n",
        "                return GenerateEncoderDecoderOutput(\n",
        "                    sequences=input_ids,\n",
        "                    scores=scores,\n",
        "                    logits=raw_logits,\n",
        "                    encoder_attentions=encoder_attentions,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    decoder_attentions=decoder_attentions,\n",
        "                    cross_attentions=cross_attentions,\n",
        "                    decoder_hidden_states=decoder_hidden_states,\n",
        "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
        "                )\n",
        "            else:\n",
        "                return GenerateDecoderOnlyOutput(\n",
        "                    sequences=input_ids,\n",
        "                    scores=scores,\n",
        "                    logits=raw_logits,\n",
        "                    attentions=decoder_attentions,\n",
        "                    hidden_states=decoder_hidden_states,\n",
        "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
        "                )\n",
        "        else:\n",
        "            return input_ids\n",
        "\n",
        "    def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n",
        "        if \"cache_position\" in model_kwargs and model_kwargs[\"cache_position\"]:\n",
        "            return model_kwargs\n",
        "        if \"inputs_embeds\" in model_kwargs and not self.config.is_encoder_decoder:\n",
        "            cache_position = torch.ones_like(model_kwargs[\"inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n",
        "        elif \"decoder_inputs_embeds\" in model_kwargs and self.config.is_encoder_decoder:\n",
        "            cache_position = (\n",
        "                torch.ones_like(model_kwargs[\"decoder_inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n",
        "            )\n",
        "        else:\n",
        "            cache_position = torch.ones(seq_length, dtype=torch.int64, device=device).cumsum(0) - 1\n",
        "\n",
        "        past_length = 0\n",
        "        if model_kwargs.get(\"past_key_values\") is not None:\n",
        "            cache = model_kwargs[\"past_key_values\"]\n",
        "            past_length = 0\n",
        "            if not isinstance(cache, Cache):\n",
        "                past_length = cache[0][0].shape[2]\n",
        "            elif hasattr(cache, \"get_seq_length\") and cache.get_seq_length() is not None:\n",
        "                past_length = cache.get_seq_length()\n",
        "\n",
        "            cache_position = cache_position[past_length:]\n",
        "\n",
        "        model_kwargs[\"cache_position\"] = cache_position\n",
        "        return model_kwargs\n",
        "\n",
        "    def _valid_auto_compile_criteria(self, model_kwargs: dict, generation_config: GenerationConfig) -> bool:\n",
        "        if generation_config.disable_compile:\n",
        "            return False\n",
        "\n",
        "        valid_hardware = self.device.type == \"cuda\" or bool(\n",
        "            generation_config.compile_config is not None and generation_config.compile_config._compile_all_devices\n",
        "        )\n",
        "        using_compilable_cache = (\n",
        "            isinstance(model_kwargs.get(\"past_key_values\"), Cache) and model_kwargs[\"past_key_values\"].is_compileable\n",
        "        )\n",
        "        can_compile = valid_hardware and using_compilable_cache and self._supports_static_cache\n",
        "\n",
        "        if getattr(self, \"hf_quantizer\", None) is not None:\n",
        "            can_compile &= self.hf_quantizer.is_compileable\n",
        "\n",
        "        if hasattr(self, \"hf_device_map\"):\n",
        "            all_model_devices = set(self.hf_device_map.values())\n",
        "            has_cpu_offload = \"cpu\" in all_model_devices and len(all_model_devices) > 1\n",
        "            can_compile &= not has_cpu_offload\n",
        "            has_disk_offload = \"disk\" in all_model_devices\n",
        "            can_compile &= not has_disk_offload\n",
        "\n",
        "        #if generation_config.compile_config is not None and not can_compile:\n",
        "            #logger.warning_once(\n",
        "                #\"You have set `compile_config`, but we are unable to meet the criteria for compilation. Compilation \"\n",
        "                #\"will be skipped.\"\n",
        "            #)\n",
        "\n",
        "        return can_compile\n",
        "\n",
        "    def _has_unfinished_sequences(self, this_peer_finished: bool, synced_gpus: bool, device: torch.device) -> bool:\n",
        "        if synced_gpus:\n",
        "            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0, device=device)\n",
        "            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
        "            if this_peer_finished_flag.item() == 0.0:\n",
        "                return False\n",
        "        elif this_peer_finished:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _update_model_kwargs_for_generation(\n",
        "        self,\n",
        "        outputs: ModelOutput,\n",
        "        model_kwargs: dict[str, Any],\n",
        "        is_encoder_decoder: bool = False,\n",
        "        num_new_tokens: int = 1,\n",
        "    ) -> dict[str, Any]:\n",
        "        for possible_cache_name in ALL_CACHE_NAMES:\n",
        "            if possible_cache_name in outputs:\n",
        "                if possible_cache_name in (\"past_buckets_states\", \"mems\"):\n",
        "                    cache_name = \"past_key_values\"\n",
        "                else:\n",
        "                    cache_name = possible_cache_name\n",
        "                model_kwargs[cache_name] = getattr(outputs, possible_cache_name)\n",
        "                break\n",
        "\n",
        "        if \"token_type_ids\" in model_kwargs:\n",
        "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
        "            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        if not is_encoder_decoder:\n",
        "            if \"attention_mask\" in model_kwargs:\n",
        "                attention_mask = model_kwargs[\"attention_mask\"]\n",
        "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
        "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
        "                )\n",
        "        else:\n",
        "            if \"decoder_attention_mask\" in model_kwargs:\n",
        "                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n",
        "                model_kwargs[\"decoder_attention_mask\"] = torch.cat(\n",
        "                    [decoder_attention_mask, decoder_attention_mask.new_ones((decoder_attention_mask.shape[0], 1))],\n",
        "                    dim=-1,\n",
        "                )\n",
        "\n",
        "        if model_kwargs.get(\"use_cache\", True):\n",
        "            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n",
        "        else:\n",
        "            past_positions = model_kwargs.pop(\"cache_position\")\n",
        "            new_positions = torch.arange(\n",
        "                past_positions[-1] + 1, past_positions[-1] + num_new_tokens + 1, dtype=past_positions.dtype\n",
        "            ).to(past_positions.device)\n",
        "            model_kwargs[\"cache_position\"] = torch.cat((past_positions, new_positions))\n",
        "        return model_kwargs\n",
        "\n",
        "    def _prepare_special_tokens(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        kwargs_has_attention_mask: Optional[bool] = None,\n",
        "        device: Optional[Union[torch.device, str]] = None,\n",
        "    ):\n",
        "        def _tensor_or_none(token, device=None):\n",
        "            if token is None:\n",
        "                return token\n",
        "\n",
        "            device = device if device is not None else self.device\n",
        "            if isinstance(token, torch.Tensor):\n",
        "                return token.to(device)\n",
        "            return torch.tensor(token, device=device, dtype=torch.long)\n",
        "\n",
        "        bos_token_tensor = _tensor_or_none(generation_config.bos_token_id, device=device)\n",
        "        eos_token_tensor = _tensor_or_none(generation_config.eos_token_id, device=device)\n",
        "        pad_token_tensor = _tensor_or_none(generation_config.pad_token_id, device=device)\n",
        "        decoder_start_token_tensor = _tensor_or_none(generation_config.decoder_start_token_id, device=device)\n",
        "\n",
        "        if self.config.is_encoder_decoder:\n",
        "            decoder_start_token_tensor = (\n",
        "                decoder_start_token_tensor if decoder_start_token_tensor is not None else bos_token_tensor\n",
        "            )\n",
        "\n",
        "        if eos_token_tensor is not None and eos_token_tensor.ndim == 0:\n",
        "            eos_token_tensor = eos_token_tensor.unsqueeze(0)\n",
        "\n",
        "        if pad_token_tensor is None and eos_token_tensor is not None:\n",
        "            #if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n",
        "                #logger.warning(\n",
        "                    #\"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n",
        "                    #\"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
        "                #)\n",
        "            pad_token_tensor = eos_token_tensor[0]\n",
        "            #logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{pad_token_tensor} for open-end generation.\")\n",
        "\n",
        "        if self.config.is_encoder_decoder and decoder_start_token_tensor is None:\n",
        "            raise ValueError(\n",
        "                \"`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\"\n",
        "            )\n",
        "        #if (\n",
        "            #eos_token_tensor is not None\n",
        "            #and isin_mps_friendly(elements=eos_token_tensor, test_elements=pad_token_tensor).any()\n",
        "        #):\n",
        "            #if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n",
        "                #logger.warning_once(\n",
        "                    #\"The attention mask is not set and cannot be inferred from input because pad token is same as \"\n",
        "                    #\"eos token. As a consequence, you may observe unexpected behavior. Please pass your input's \"\n",
        "                    #\"`attention_mask` to obtain reliable results.\"\n",
        "                #)\n",
        "        #if eos_token_tensor is not None and (\n",
        "            #torch.is_floating_point(eos_token_tensor) or (eos_token_tensor < 0).any()\n",
        "        #):\n",
        "            #logger.warning(\n",
        "                #f\"`eos_token_id` should consist of positive integers, but is {eos_token_tensor}. Your generation \"\n",
        "                #\"will not stop until the maximum length is reached. Depending on other flags, it may even crash.\"\n",
        "            #)\n",
        "\n",
        "        generation_config._bos_token_tensor = bos_token_tensor\n",
        "        generation_config._eos_token_tensor = eos_token_tensor\n",
        "        generation_config._pad_token_tensor = pad_token_tensor\n",
        "        generation_config._decoder_start_token_tensor = decoder_start_token_tensor\n",
        "\n",
        "\n",
        "    def _maybe_initialize_input_ids_for_generation(\n",
        "        self,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        bos_token_id: Optional[torch.Tensor] = None,\n",
        "        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n",
        "    ) -> torch.LongTensor:\n",
        "        if inputs is not None:\n",
        "            return inputs\n",
        "\n",
        "        encoder_outputs = model_kwargs.get(\"encoder_outputs\")\n",
        "        if self.config.is_encoder_decoder and encoder_outputs is not None:\n",
        "            shape = encoder_outputs.last_hidden_state.size()[:-1]\n",
        "            return torch.ones(shape, dtype=torch.long, device=self.device) * -100\n",
        "\n",
        "        batch_size = 1\n",
        "        for value in model_kwargs.values():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                batch_size = value.shape[0]\n",
        "                break\n",
        "\n",
        "        if \"inputs_embeds\" in model_kwargs:\n",
        "            return torch.ones((batch_size, 0), dtype=torch.long, device=self.device)\n",
        "\n",
        "        if bos_token_id is None:\n",
        "            raise ValueError(\"`bos_token_id` has to be defined when no `input_ids` are provided.\")\n",
        "\n",
        "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * bos_token_id\n",
        "\n",
        "    def _prepare_model_inputs(\n",
        "        self,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        bos_token_id: Optional[torch.Tensor] = None,\n",
        "        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n",
        "    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n",
        "        if (\n",
        "            self.config.is_encoder_decoder\n",
        "            and hasattr(self, \"encoder\")\n",
        "            and self.encoder.main_input_name != self.main_input_name\n",
        "        ):\n",
        "            input_name = self.encoder.main_input_name\n",
        "        else:\n",
        "            input_name = self.main_input_name\n",
        "\n",
        "        model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name}\n",
        "\n",
        "        inputs_kwarg = model_kwargs.pop(input_name, None)\n",
        "        if inputs_kwarg is not None and inputs is not None:\n",
        "            raise ValueError(\n",
        "                f\"`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. \"\n",
        "                f\"Make sure to either pass {inputs} or {input_name}=...\"\n",
        "            )\n",
        "        elif inputs_kwarg is not None:\n",
        "            inputs = inputs_kwarg\n",
        "\n",
        "        if input_name == \"input_ids\" and \"inputs_embeds\" in model_kwargs:\n",
        "            if model_kwargs[\"inputs_embeds\"] is None:\n",
        "                model_kwargs.pop(\"inputs_embeds\")\n",
        "            elif not self.config.is_encoder_decoder:\n",
        "                has_inputs_embeds_forwarding = \"inputs_embeds\" in set(\n",
        "                    inspect.signature(self.prepare_inputs_for_generation).parameters.keys()\n",
        "                )\n",
        "                if not has_inputs_embeds_forwarding:\n",
        "                    raise ValueError(\n",
        "                        f\"You passed `inputs_embeds` to `.generate()`, but the model class {self.__class__.__name__} \"\n",
        "                        \"doesn't have its forwarding implemented. See the GPT2 implementation for an example \"\n",
        "                        \"(https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!\"\n",
        "                    )\n",
        "\n",
        "                model_kwargs[\"input_ids\"] = self._maybe_initialize_input_ids_for_generation(\n",
        "                    inputs, bos_token_id, model_kwargs=model_kwargs\n",
        "                )\n",
        "                inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\"\n",
        "            else:\n",
        "                if inputs is not None:\n",
        "                    raise ValueError(\"You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.\")\n",
        "                inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\"\n",
        "\n",
        "        # 4. if `inputs` is still None, try to create `input_ids` from BOS token\n",
        "        inputs = self._maybe_initialize_input_ids_for_generation(inputs, bos_token_id, model_kwargs)\n",
        "        return inputs, input_name, model_kwargs\n",
        "\n",
        "    def _prepare_generation_config(\n",
        "        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: dict\n",
        "    ) -> tuple[GenerationConfig, dict]:\n",
        "        using_model_generation_config = False\n",
        "        if generation_config is None:\n",
        "            if (\n",
        "                self.generation_config._from_model_config  # 1)\n",
        "                and self.generation_config._original_object_hash == hash(self.generation_config)  # 2)\n",
        "                and len(self.config._get_non_default_generation_parameters()) > 0  # 3)\n",
        "            ):\n",
        "                new_generation_config = GenerationConfig.from_model_config(self.config)\n",
        "                if new_generation_config != self.generation_config:  # 4)\n",
        "                    warnings.warn(\n",
        "                        \"You have modified the pretrained model configuration to control generation. This is a\"\n",
        "                        \" deprecated strategy to control generation and will be removed in v5.\"\n",
        "                        \" Please use and modify the model generation configuration (see\"\n",
        "                        \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\",\n",
        "                        UserWarning,\n",
        "                    )\n",
        "                    self.generation_config = new_generation_config\n",
        "\n",
        "            generation_config = self.generation_config\n",
        "            using_model_generation_config = True\n",
        "\n",
        "        generation_config = copy.deepcopy(generation_config)\n",
        "\n",
        "        if not using_model_generation_config:\n",
        "            model_base_version = version.parse(version.parse(self.generation_config.transformers_version).base_version)\n",
        "            if use_model_defaults is True or (\n",
        "                use_model_defaults is None and model_base_version >= version.parse(\"4.50.0\")\n",
        "            ):\n",
        "                modified_values = {}\n",
        "                global_default_generation_config = GenerationConfig()\n",
        "                model_generation_config = self.generation_config\n",
        "                for key, model_gen_config_value in model_generation_config.__dict__.items():\n",
        "                    if key.startswith(\"_\") or key == \"transformers_version\":  # metadata\n",
        "                        continue\n",
        "                    global_default_value = getattr(global_default_generation_config, key, None)\n",
        "                    custom_gen_config_value = getattr(generation_config, key, None)\n",
        "                    if (\n",
        "                        custom_gen_config_value == global_default_value\n",
        "                        and model_gen_config_value != global_default_value\n",
        "                    ):\n",
        "                        modified_values[key] = model_gen_config_value\n",
        "                        setattr(generation_config, key, model_gen_config_value)\n",
        "                if generation_config.temperature == 0.0:\n",
        "                    generation_config.do_sample = False\n",
        "                #if use_model_defaults is None and len(modified_values) > 0:\n",
        "                    #logger.warning_once(\n",
        "                        #f\"`generation_config` default values have been modified to match model-specific defaults: \"\n",
        "                        #f\"{modified_values}. If this is not desired, please set these values explicitly.\"\n",
        "                    #)\n",
        "            else:\n",
        "                if generation_config.bos_token_id is None:\n",
        "                    generation_config.bos_token_id = self.generation_config.bos_token_id\n",
        "                if generation_config.eos_token_id is None:\n",
        "                    generation_config.eos_token_id = self.generation_config.eos_token_id\n",
        "                if generation_config.pad_token_id is None:\n",
        "                    generation_config.pad_token_id = self.generation_config.pad_token_id\n",
        "                if generation_config.decoder_start_token_id is None:\n",
        "                    generation_config.decoder_start_token_id = self.generation_config.decoder_start_token_id\n",
        "\n",
        "        model_kwargs = generation_config.update(**kwargs)\n",
        "\n",
        "        return generation_config, model_kwargs\n",
        "\n",
        "    def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n",
        "        if self.config.is_encoder_decoder:\n",
        "            for key in [\"decoder_input_ids\"]:\n",
        "                model_kwargs.pop(key, None)\n",
        "\n",
        "        unused_model_args = []\n",
        "        model_args = set(inspect.signature(self.prepare_inputs_for_generation).parameters)\n",
        "        if \"kwargs\" in model_args or \"model_kwargs\" in model_args:\n",
        "            model_args |= set(inspect.signature(self.forward).parameters)\n",
        "\n",
        "        if self.config.is_encoder_decoder:\n",
        "            base_model = getattr(self, self.base_model_prefix, None)\n",
        "\n",
        "            encoder = getattr(self, \"encoder\", None)\n",
        "            if encoder is None and base_model is not None:\n",
        "                encoder = getattr(base_model, \"encoder\", None)\n",
        "\n",
        "            if encoder is not None:\n",
        "                encoder_model_args = set(inspect.signature(encoder.forward).parameters)\n",
        "                model_args |= encoder_model_args\n",
        "\n",
        "            decoder = getattr(self, \"decoder\", None)\n",
        "            if decoder is None and base_model is not None:\n",
        "                decoder = getattr(base_model, \"decoder\", None)\n",
        "\n",
        "            if decoder is not None:\n",
        "                decoder_model_args = set(inspect.signature(decoder.forward).parameters)\n",
        "                model_args |= {f\"decoder_{x}\" for x in decoder_model_args}\n",
        "\n",
        "        for key, value in model_kwargs.items():\n",
        "            if value is not None and key not in model_args:\n",
        "                unused_model_args.append(key)\n",
        "\n",
        "        if unused_model_args:\n",
        "            raise ValueError(\n",
        "                f\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\n",
        "                \" generate arguments will also show up in this list)\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "Y-DjBWScVEmv"
      },
      "outputs": [],
      "source": [
        "class CustomQwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
        "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = CustomQwen2Model(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
        "        **kwargs,\n",
        "    ) -> CausalLMOutputWithPast:\n",
        "        outputs: BaseModelOutputWithPast = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            cache_position=cache_position,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
        "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            print(\"1\")\n",
        "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cached_file(\n",
        "    path_or_repo_id: Union[str, os.PathLike],\n",
        "    filename: str,\n",
        "    **kwargs,\n",
        ") -> Optional[str]:\n",
        "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
        "    file = file[0] if file is not None else file\n",
        "    return file\n",
        "\n",
        "def cached_files(\n",
        "    path_or_repo_id: Union[str, os.PathLike],\n",
        "    filenames: list[str],\n",
        "    cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "    force_download: bool = False,\n",
        "    resume_download: Optional[bool] = None,\n",
        "    proxies: Optional[dict[str, str]] = None,\n",
        "    token: Optional[Union[bool, str]] = None,\n",
        "    revision: Optional[str] = None,\n",
        "    local_files_only: bool = False,\n",
        "    subfolder: str = \"\",\n",
        "    repo_type: Optional[str] = None,\n",
        "    user_agent: Optional[Union[str, dict[str, str]]] = None,\n",
        "    _raise_exceptions_for_gated_repo: bool = True,\n",
        "    _raise_exceptions_for_missing_entries: bool = True,\n",
        "    _raise_exceptions_for_connection_errors: bool = True,\n",
        "    _commit_hash: Optional[str] = None,\n",
        "    **deprecated_kwargs,\n",
        ") -> Optional[str]:\n",
        "    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n",
        "    if is_offline_mode() and not local_files_only:\n",
        "        #logger.info(\"Offline mode: forcing local_files_only=True\")\n",
        "        local_files_only = True\n",
        "    if subfolder is None:\n",
        "        subfolder = \"\"\n",
        "\n",
        "    full_filenames = [os.path.join(subfolder, file) for file in filenames]\n",
        "\n",
        "    path_or_repo_id = str(path_or_repo_id)\n",
        "    existing_files = []\n",
        "    for filename in full_filenames:\n",
        "        if os.path.isdir(path_or_repo_id):\n",
        "            resolved_file = os.path.join(path_or_repo_id, filename)\n",
        "            if not os.path.isfile(resolved_file):\n",
        "                if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, \"config.json\"):\n",
        "                    revision_ = \"main\" if revision is None else revision\n",
        "                    raise OSError(\n",
        "                        f\"{path_or_repo_id} does not appear to have a file named {filename}. Checkout \"\n",
        "                        f\"'https://huggingface.co/{path_or_repo_id}/tree/{revision_}' for available files.\"\n",
        "                    )\n",
        "                else:\n",
        "                    return None\n",
        "            existing_files.append(resolved_file)\n",
        "\n",
        "    if len(existing_files) == len(full_filenames):\n",
        "        return existing_files\n",
        "    if isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    existing_files = []\n",
        "    file_counter = 0\n",
        "    if _commit_hash is not None and not force_download:\n",
        "        for filename in full_filenames:\n",
        "            resolved_file = try_to_load_from_cache(\n",
        "                path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type\n",
        "            )\n",
        "            if resolved_file is not None:\n",
        "                if resolved_file is not _CACHED_NO_EXIST:\n",
        "                    file_counter += 1\n",
        "                    existing_files.append(resolved_file)\n",
        "                elif not _raise_exceptions_for_missing_entries:\n",
        "                    file_counter += 1\n",
        "                else:\n",
        "                    raise OSError(f\"Could not locate {filename} inside {path_or_repo_id}.\")\n",
        "\n",
        "    if file_counter == len(full_filenames):\n",
        "        return existing_files if len(existing_files) > 0 else None"
      ],
      "metadata": {
        "id": "UbV2rN-IiLAT"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC6s4FOmcrgs"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def traced(\n",
        "    func=None,\n",
        "    *,\n",
        "    span_name=None,\n",
        "    standalone=False,\n",
        "    additional_attributes: Optional[list[tuple[str, str, Union[Any, Callable[[Any], Any]]]]] = None,\n",
        "):\n",
        "    def decorator(func):\n",
        "        if not _has_opentelemetry:\n",
        "            return func\n",
        "\n",
        "        import functools\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            instance = args[0] if args and (hasattr(func, \"__self__\") and func.__self__ is not None) else None\n",
        "            is_method = instance is not None\n",
        "\n",
        "            if is_method and hasattr(instance, \"tracer\"):\n",
        "                tracer = instance.tracer\n",
        "            else:\n",
        "                tracer = get_tracer(f\"transformers.{func.__module__}.{func.__name__}\")\n",
        "\n",
        "            name = span_name or func.__name__\n",
        "            span_fn = tracer.start_span if standalone else tracer.start_as_current_span\n",
        "            with span_fn(name) as span:\n",
        "                span.set_attribute(\"function.name\", func.__name__)\n",
        "                span.set_attribute(\"function.module\", func.__module__)\n",
        "                span.set_attribute(\"function.is_method\", is_method)\n",
        "\n",
        "                if args:\n",
        "                    for i, arg in enumerate(args):\n",
        "                        if isinstance(arg, (str, int, float, bool)) or arg is None:\n",
        "                            span.set_attribute(f\"args.{i}\", str(arg))\n",
        "                        else:\n",
        "                            span.set_attribute(f\"args.{i}\", str(type(arg)))\n",
        "                if kwargs:\n",
        "                    for key, value in kwargs.items():\n",
        "                        if isinstance(value, (str, int, float, bool)) or value is None:\n",
        "                            span.set_attribute(f\"kwargs.{key}\", str(value))\n",
        "                        else:\n",
        "                            span.set_attribute(f\"kwargs.{key}\", str(type(value)))\n",
        "\n",
        "                if additional_attributes and is_method:\n",
        "                    for attr_config in additional_attributes:\n",
        "                        instance_attribute_name, span_attribute_key, value_or_transform_function = attr_config\n",
        "                        if hasattr(instance, instance_attribute_name):\n",
        "                            attribute_value = getattr(instance, instance_attribute_name)\n",
        "                            if callable(value_or_transform_function):\n",
        "                                transformed_value = value_or_transform_function(attribute_value)\n",
        "                            else:\n",
        "                                transformed_value = value_or_transform_function\n",
        "                            span.set_attribute(span_attribute_key, transformed_value)\n",
        "\n",
        "                try:\n",
        "                    result = func(*args, **kwargs)\n",
        "                    return result\n",
        "                except Exception as e:\n",
        "                    span.set_status(Status(StatusCode.ERROR))\n",
        "                    span.record_exception(e)\n",
        "                    raise\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    if func is None:\n",
        "        return decorator\n",
        "    return decorator(func)"
      ],
      "metadata": {
        "id": "feWKxQrReCRU"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "Ij91P1vQZfu4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def is_peft_available():\n",
        "    return _peft_available\n",
        "\n",
        "class PushToHubMixin:\n",
        "    pass\n",
        "\n",
        "def is_remote_url(url_or_filename):\n",
        "    parsed = urlparse(url_or_filename)\n",
        "    return parsed.scheme in (\"http\", \"https\")\n",
        "\n",
        "def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False):\n",
        "    assert not (check_is_tpu and check_is_gpu), \"The check_is_tpu and check_is_gpu cannot both be true.\"\n",
        "\n",
        "    if not _torch_xla_available:\n",
        "        return False\n",
        "\n",
        "    import torch_xla\n",
        "\n",
        "    if check_is_gpu:\n",
        "        return torch_xla.runtime.device_type() in [\"GPU\", \"CUDA\"]\n",
        "    elif check_is_tpu:\n",
        "        return torch_xla.runtime.device_type() == \"TPU\"\n",
        "\n",
        "    return True\n",
        "\n",
        "def is_torch_sdpa_available():\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "    elif _torch_version == \"N/A\":\n",
        "        return False\n",
        "\n",
        "    # NOTE: MLU is OK with non-contiguous inputs.\n",
        "    if is_torch_mlu_available():\n",
        "        return True\n",
        "    # NOTE: NPU can use SDPA in Transformers with torch>=2.1.0.\n",
        "    if is_torch_npu_available():\n",
        "        return True\n",
        "    # NOTE: We require torch>=2.1.1 to avoid a numerical issue in SDPA with non-contiguous inputs: https://github.com/pytorch/pytorch/issues/112577\n",
        "    return version.parse(_torch_version) >= version.parse(\"2.1.1\")\n",
        "\n",
        "class Action(ExplicitEnum):\n",
        "    NONE = \"none\"\n",
        "    NOTIFY = \"notify\"\n",
        "    NOTIFY_ALWAYS = \"notify_always\"\n",
        "    RAISE = \"raise\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YHXOB4yxt0j"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "HDy8-19HyQz6"
      },
      "outputs": [],
      "source": [
        "class AutoHfQuantizer:\n",
        "    @classmethod\n",
        "    def from_config(cls, quantization_config: Union[QuantizationConfigMixin, dict], **kwargs):\n",
        "        # Convert it to a QuantizationConfig if the q_config is a dict\n",
        "        if isinstance(quantization_config, dict):\n",
        "            quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n",
        "\n",
        "        quant_method = quantization_config.quant_method\n",
        "\n",
        "        # Again, we need a special care for bnb as we have a single quantization config\n",
        "        # class for both 4-bit and 8-bit quantization\n",
        "        if quant_method == QuantizationMethod.BITS_AND_BYTES:\n",
        "            if quantization_config.load_in_8bit:\n",
        "                quant_method += \"_8bit\"\n",
        "            else:\n",
        "                quant_method += \"_4bit\"\n",
        "\n",
        "        if quant_method not in AUTO_QUANTIZER_MAPPING.keys():\n",
        "            raise ValueError(\n",
        "                f\"Unknown quantization type, got {quant_method} - supported types are:\"\n",
        "                f\" {list(AUTO_QUANTIZER_MAPPING.keys())}\"\n",
        "            )\n",
        "\n",
        "        target_cls = AUTO_QUANTIZER_MAPPING[quant_method]\n",
        "        return target_cls(quantization_config, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n",
        "        quantization_config = AutoQuantizationConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
        "        return cls.from_config(quantization_config)\n",
        "\n",
        "    @classmethod\n",
        "    def merge_quantization_configs(\n",
        "        cls,\n",
        "        quantization_config: Union[dict, QuantizationConfigMixin],\n",
        "        quantization_config_from_args: Optional[QuantizationConfigMixin],\n",
        "    ):\n",
        "        if quantization_config_from_args is not None:\n",
        "            warning_msg = (\n",
        "                \"You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading\"\n",
        "                \" already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\"\n",
        "            )\n",
        "        else:\n",
        "            warning_msg = \"\"\n",
        "\n",
        "        if isinstance(quantization_config, dict):\n",
        "            # Convert the config based on the type of quantization_config_from_args (e.g., AutoRoundConfig), which takes priority before automatic configuration dispatch.\n",
        "            if isinstance(quantization_config_from_args, AutoRoundConfig):\n",
        "                quantization_config = AutoRoundConfig.from_dict(quantization_config)\n",
        "            else:\n",
        "                quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n",
        "\n",
        "        if (\n",
        "            isinstance(\n",
        "                quantization_config, (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig)\n",
        "            )\n",
        "            and quantization_config_from_args is not None\n",
        "        ):\n",
        "            # special case for GPTQ / AWQ / FbgemmFp8 config collision\n",
        "            loading_attr_dict = quantization_config_from_args.get_loading_attributes()\n",
        "            for attr, val in loading_attr_dict.items():\n",
        "                setattr(quantization_config, attr, val)\n",
        "\n",
        "            warning_msg += f\"However, loading attributes (e.g. {list(loading_attr_dict.keys())}) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\"\n",
        "\n",
        "        if warning_msg != \"\":\n",
        "            warnings.warn(warning_msg)\n",
        "\n",
        "        return quantization_config\n",
        "\n",
        "    @staticmethod\n",
        "    def supports_quant_method(quantization_config_dict):\n",
        "        quant_method = quantization_config_dict.get(\"quant_method\", None)\n",
        "        if quantization_config_dict.get(\"load_in_8bit\", False) or quantization_config_dict.get(\"load_in_4bit\", False):\n",
        "            suffix = \"_4bit\" if quantization_config_dict.get(\"load_in_4bit\", False) else \"_8bit\"\n",
        "            quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n",
        "        elif quant_method is None:\n",
        "            raise ValueError(\n",
        "                \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n",
        "            )\n",
        "\n",
        "        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING.keys():\n",
        "            logger.warning(\n",
        "                f\"Unknown quantization type, got {quant_method} - supported types are:\"\n",
        "                f\" {list(AUTO_QUANTIZER_MAPPING.keys())}. Hence, we will skip the quantization. \"\n",
        "                \"To remove the warning, you can delete the quantization_config attribute in config.json\"\n",
        "            )\n",
        "            return False\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "rzbk5FBu6CWA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n",
        "    if tp_plan is None:\n",
        "        return\n",
        "\n",
        "    generic_keys = {re.sub(r\"\\d+\", \"*\", key) for key in expected_keys}\n",
        "    unsharded_layers = set(generic_keys)\n",
        "    unused_rules = tp_plan\n",
        "\n",
        "    for key in generic_keys:\n",
        "        param_name = key.rsplit(\".\", 1)[0] if \".\" in key else key\n",
        "        generic_param_name = re.sub(r\"\\d+\", \"*\", param_name)\n",
        "\n",
        "        if generic_param_name in tp_plan:\n",
        "            unused_rules.pop(generic_param_name)\n",
        "            unsharded_layers.discard(key)\n",
        "        elif \".\" in generic_param_name and (parent_param_name := generic_param_name.rsplit(\".\", 1)[0]) in tp_plan:\n",
        "            unused_rules.pop(parent_param_name)\n",
        "            unsharded_layers.discard(key)\n",
        "        else:\n",
        "            pass  # we couldn't find the rule for this parameter, so it's not sharded\n",
        "\n",
        "    if len(unused_rules) > 0:\n",
        "        logger.warning(f\"The following TP rules were not applied on any of the layers: {unused_rules}\")\n",
        "    if len(unsharded_layers) > 0:\n",
        "        logger.warning(f\"The following layers were not sharded: {', '.join(unsharded_layers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "JmOIHtMGVdJX"
      },
      "outputs": [],
      "source": [
        "\n",
        "    def _get_key_renaming_mapping(\n",
        "        self,\n",
        "        checkpoint_keys: list[str],\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        loading_base_model_from_task_state_dict: bool = False,\n",
        "        loading_task_model_from_base_state_dict: bool = False,\n",
        "    ):\n",
        "        prefix = self.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "\n",
        "        renamed_keys = {}\n",
        "        key_renaming_mapping = {}\n",
        "        for key in checkpoint_keys:\n",
        "            # Class specific rename\n",
        "            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n",
        "\n",
        "            # Optionally map the key according to `key_mapping`\n",
        "            if key_mapping is not None:\n",
        "                for pattern, replacement in key_mapping.items():\n",
        "                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n",
        "                    # Early exit of the loop\n",
        "                    if n_replace > 0:\n",
        "                        has_changed = True\n",
        "                        break\n",
        "\n",
        "            # In this case, we need to add the prefix to the keys, to match them to the expected keys\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                new_key = \".\".join([prefix, new_key])\n",
        "            # In this case we need to remove the prefix from the key to match them to the expected keys, and use\n",
        "            # only the keys starting with the prefix\n",
        "            elif loading_base_model_from_task_state_dict:\n",
        "                if not new_key.startswith(_prefix):\n",
        "                    continue\n",
        "                new_key = new_key[len(_prefix) :]\n",
        "\n",
        "            key_renaming_mapping[key] = new_key\n",
        "\n",
        "            # track gamma/beta rename for logging\n",
        "            if has_changed:\n",
        "                if key.endswith(\"LayerNorm.gamma\"):\n",
        "                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n",
        "                elif key.endswith(\"LayerNorm.beta\"):\n",
        "                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n",
        "\n",
        "        if renamed_keys:\n",
        "            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n",
        "            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n",
        "            for old_key, new_key in renamed_keys.values():\n",
        "                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n",
        "            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n",
        "            #logger.info_once(warning_msg)\n",
        "\n",
        "        return key_renaming_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 実行"
      ],
      "metadata": {
        "id": "rbfuxQLF2L5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYIZ9EuIW1Ua",
        "outputId": "ef7c3ecb-dd9a-4670-e53d-24cea06e1495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected torch version: 2.8.0+cu126\n",
            "Detected torch version: 2.8.0+cu126\n",
            "Detected torch version: 2.8.0+cu126\n",
            "こんにちは、ゲストさん\n",
            "ログイン\n",
            "IDでもっと\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 独自モデルとして読み込む\n",
        "config = Qwen2Config.from_pretrained(\"/content/drive/MyDrive/reconstructed_model\")\n",
        "config.use_flash_attn = True\n",
        "model = CustomQwen2ForCausalLM.from_pretrained(\n",
        "    \"/content/drive/MyDrive/reconstructed_model\",\n",
        "    config=config,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ").eval()\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "except Exception as e:\n",
        "    print(\"compile失敗:\", e)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/reconstructed_model\")\n",
        "\n",
        "inputs = tokenizer(\"こんにちは、\", return_tensors=\"pt\")\n",
        "inputs = {\n",
        "    k: (v.to(model.device).half() if k != \"input_ids\" else v.to(model.device))\n",
        "    for k, v in inputs.items()\n",
        "}\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,\n",
        "        #do_sample=False,          # サンプリングせずgreedy → 高速\n",
        "        use_cache=False,           # キャッシュ活用 → 重要\n",
        "        #num_beams=1,              # Beam search無効 → 高速\n",
        "        #early_stopping=True,      # 終了判定を早める\n",
        "        #pad_token_id=tokenizer.eos_token_id  # 明示的にパディングトークン指定\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OujfLwNeDzD8",
        "pqrV4C7BkgxJ",
        "BC6s4FOmcrgs",
        "7YHXOB4yxt0j"
      ],
      "provenance": [],
      "mount_file_id": "1LVMEW3jDswGM-bu60LxdTSf6N8pfWmk8",
      "authorship_tag": "ABX9TyPjjgTlssw8cF7q7GIEwrDW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}