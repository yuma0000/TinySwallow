これはnnabla_funcクラスです
import nnabla as nn
import nnabla.function as F

def attention(state_dict, layer_count, hidden_states, cos, sin):
    input_shape = hidden_states.shape[:-1]
    hidden_shape = (*input_shape, -1, self.head_dim)

    query = state_dict[f"layers.{layer_count}".self_attn.q_proj.weight]
    key = state_dict[f"layers.{layer_count}".self_attn.k_proj.weight]
    value = state_dict[f"layers.{layer_count}".self_attn.v_proj.weight]

    query_state = query @ hidden_states + state_dict[f"model.layers{layer_count}.self_attn.q_proj.bias"]
    key_state = key @ hidden_states + state_dict[f"model.layers{layer_count}.self_attn.k_proj.bias"]
    value_state = value @ hidden_states + state_dict[f"model.layers{layer_count}.self_attn.v_proj.bias"]

    query_state = query_state.transpose(1, 2)
    key_state = key_state.transpose(1, 2)
    value_state = value_state.transpose(1, 2)

    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  

    if past_key_value is not None:  
        cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  
        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  

    if hasattr(module, "num_key_value_groups"):  
        key = repeat_kv(key, module.num_key_value_groups)  
        value = repeat_kv(value, module.num_key_value_groups)  

    if attention_mask is not None and attention_mask.ndim == 4:  
        attention_mask = attention_mask[:, :, :, : key.shape[-2]]  

    query = query.contiguous()  
    key = key.contiguous()  
    value = value.contiguous()  

    if is_causal is None:  
        is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)  

    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):  
        is_causal = is_causal.item()  

    origin_dtype = query.dtype

    def maybe_upcast(t):  
        return t.float() if t.dtype in (torch.float16, torch.bfloat16) else t  

    query = maybe_upcast(query)  
    key = maybe_upcast(key)  
    value = maybe_upcast(value)  

    if query.size(1) != key.size(1):  
        qh, kh = query.size(1), key.size(1)  
        assert qh % kh == 0, "GQA: query_heads must be divisible by key_heads"  
        repeat_factor = qh // kh  
        key = key.repeat_interleave(repeat_factor, dim=1)  
        value = value.repeat_interleave(repeat_factor, dim=1)  

    if scale is None:  
        scale = 1.0 / (key.size(-1) ** 0.5)  

    attn_weights = torch.matmul(query, key.transpose(-2, -1)) * scale  

    if is_causal:  
        q_len, k_len = attn_weights.size(-2), attn_weights.size(-1)  
        causal_mask = torch.tril(torch.ones(q_len, k_len, device=attn_weights.device, dtype=torch.bool))  
        attn_weights = attn_weights.masked_fill(~causal_mask.view(1, 1, q_len, k_len), float('-inf'))  

    if attn_mask is not None:  
        if attn_mask.dtype == torch.bool:  
            attn_weights = attn_weights.masked_fill(~attn_mask, float('-inf'))  
        else:  
            attn_weights = attn_weights + attn_mask  

    attn_probs = safe_softmax(attn_weights, dim=-1)  

    if dropout_p > 0.0 and torch.is_grad_enabled():  
        attn_probs = dropout(attn_probs, p=dropout_p, training=True)  

    out = torch.matmul(attn_probs, value)  

    attn_output = out.to(origin_dtype)  

    attn_output = attn_output.transpose(1, 2).contiguous()  

    attn_output = attn_output.reshape(*input_shape, -1).contiguous()  
    attn_output = self.o_proj(attn_output)  
    return attn_output

def rotate_half(x):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

def rms_norm(weight, hidden_states, eps):
    input_dtype = hidden_states.dtype #入力
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
    hidden_states = hidden_states * torch.rsqrt(variance + eps)
    return weight * hidden_states.to(input_dtype)

def mlp(self, state_dict, x):
    #gate, up, down, act
    gate_proj = state_dict[f"gate"]
    up_proj = state_dict[f"up"]
    down_proj = state_dict[f"down"]
    act_fn = state_dict[f"act"]
    return down_proj(act_fn(gate_proj(x)) * up_proj(x))

def decoder_layer(state_dict, max_layer, layer_count, hidden_states):
    cos, sin = rotary_emb(hidden_states) #rotary_embの埋め込み

    if max_layer < layer_count:
        return None
    for i in layer_count:
        residual = hidden_states
        hidden_states = rms_norm(state_dict[f"model.layers.{layer_count}.input_layernorm.weight"], hidden_states)
        hidden_states = attention(
            state_dict=state_dict,
            layer_count,
            hidden_states=hidden_states,
            cos,
            sin
        )
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = Rms_Norm(state_dict[f"model.layers.{layer_count}.post_attention_layernorm.weight"], hidden_states)
        hidden_states = mlp(state_dict, hidden_states)
        hidden_states = residual + hidden_states
    return hidden_states

def rotary_emb(x):
    inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
    position_ids_expanded = position_ids[:, None, :].float()

    device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
    with torch.autocast(device_type=device_type, enabled=False):
        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        emb = torch.cat((freqs, freqs), dim=-1)
        cos = emb.cos() * self.attention_scaling
        sin = emb.sin() * self.attention_scaling

    cos = cos.to(dtype=x.dtype)
    sin = sin.to(dtype=x.dtype)
    return cos, sin

def Im_head(state_dict, x):
    #linear
    weight = state_dict["im_head.weight"]
    bias = state_dict["im_head.bias"]
    F.affine(x, weight, bias)
