これはnnabla_funcクラスです
import nnabla as nn
import nnabla.function as F

def Attention(query, key, value, query_states, key_states, value_states, cos, sin):
    query_states, key_states, value_states (1, 2)を入れ替える

    cos, sin = position_embeddings  
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  

    if past_key_value is not None:  
        cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  
        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  

    if hasattr(module, "num_key_value_groups"):  
        key = repeat_kv(key, module.num_key_value_groups)  
        value = repeat_kv(value, module.num_key_value_groups)  

    if attention_mask is not None and attention_mask.ndim == 4:  
        attention_mask = attention_mask[:, :, :, : key.shape[-2]]  

    query = query.contiguous()  
    key = key.contiguous()  
    value = value.contiguous()  

    if is_causal is None:  
        is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)  

    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):  
        is_causal = is_causal.item()  

    origin_dtype = query.dtype

    def maybe_upcast(t):  
        return t.float() if t.dtype in (torch.float16, torch.bfloat16) else t  

    query = maybe_upcast(query)  
    key = maybe_upcast(key)  
    value = maybe_upcast(value)  

    if query.size(1) != key.size(1):  
        qh, kh = query.size(1), key.size(1)  
        assert qh % kh == 0, "GQA: query_heads must be divisible by key_heads"  
        repeat_factor = qh // kh  
        key = key.repeat_interleave(repeat_factor, dim=1)  
        value = value.repeat_interleave(repeat_factor, dim=1)  

    if scale is None:  
        scale = 1.0 / (key.size(-1) ** 0.5)  

    attn_weights = torch.matmul(query, key.transpose(-2, -1)) * scale  

    if is_causal:  
        q_len, k_len = attn_weights.size(-2), attn_weights.size(-1)  
        causal_mask = torch.tril(torch.ones(q_len, k_len, device=attn_weights.device, dtype=torch.bool))  
        attn_weights = attn_weights.masked_fill(~causal_mask.view(1, 1, q_len, k_len), float('-inf'))  

    if attn_mask is not None:  
        if attn_mask.dtype == torch.bool:  
            attn_weights = attn_weights.masked_fill(~attn_mask, float('-inf'))  
        else:  
            attn_weights = attn_weights + attn_mask  

    attn_probs = safe_softmax(attn_weights, dim=-1)  

    if dropout_p > 0.0 and torch.is_grad_enabled():  
        attn_probs = dropout(attn_probs, p=dropout_p, training=True)  

    out = torch.matmul(attn_probs, value)  

    attn_output = out.to(origin_dtype)  

    attn_output = attn_output.transpose(1, 2).contiguous()  

    attn_output = attn_output.reshape(*input_shape, -1).contiguous()  
    attn_output = self.o_proj(attn_output)  
    return attn_output

def rotate_half(x):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

def Rms_Norm(weight, hidden_states, eps):
    input_dtype = hidden_states.dtype #入力
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
    hidden_states = hidden_states * torch.rsqrt(variance + eps)
    return weight * hidden_states.to(input_dtype)

def Mlp(self, state_dict, x):
    #gate, up, down, act
    gate_proj = state_dict[f"gate"]
    up_proj = state_dict[f"up"]
    down_proj = state_dict[f"down"]
    act_fn = state_dict[f"act"]
    return down_proj(act_fn(gate_proj(x)) * up_proj(x))

class Decoder_layer():
    #transformersのデコーダー
    def __init__(self, state_dict, max_layer):
        self.state_dict = state_dict

    def __call__(self, layer_count, hidden_states):
        query = state[f"layers.{layer_count}".self_attn.q_proj.weight]
        key = state[f"layers.{layer_count}".self_attn.k_proj.weight]
        value = state[f"layers.{layer_count}".self_attn.v_proj.weight]

        residual = hidden_states
        hidden_states = Rms_Norm(state_dict[f"input_layernorm"], hidden_states)
        hidden_states = Attention(
            query,
            key, 
            value,
            hidden_states=hidden_states,
            position_ids=position_ids,
            position_embeddings=position_embeddings,
        )
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = Rms_Norm(state_dict[f"post_attention_layernorm"], hidden_states)
        hidden_states = Mlp(state_dict, hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states

class Rotary_emb():
    def __init__(self, state_dict):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        self.cos = cos.to(dtype=x.dtype)
        self.sin = sin.to(dtype=x.dtype)

    def __call__(self, q, k):
        apply_rotary_pos_emb(q, k, cos, sin)

def Im_head(state_dict, x):
    #linear
