これはnnabla_funcクラスです

import numpy as np
import nnabla as nn
import nnabla.function as F

def embed_tokens(x, state_dict):
    weight = state_dict[f"model.embed_tokens.weight"]
    return F.embed(x, weight)

def repeat_kv(x, num_repeat):
    if num_repeat == 1:
        return x
    B, H, L, D = x.shape
    x = F.reshape(x, (B, H, 1, L, D))
    x = F.repeat(x, repeats=num_repeat, axis=2)
    x = F.reshape(x, (B, H * num_repeat, L, D))
    return x

def attention(
    state_dict,
    layer_count,
    hidden_states,
    head_dim,
    cache,
    attention_mask,
    scale,
    dropout_p,
    cos,
    sin
):
    input_shape = hidden_states.shape[:-1]
    hidden_shape = (*input_shape, -1, head_dim)

    query = state_dict[f"model.layers.{layer_count}.self_attn.q_proj.weight"]
    key = state_dict[f"model.layers.{layer_count}.self_attn.k_proj.weight"]
    value = state_dict[f"model.layers.{layer_count}.self_attn.v_proj.weight"]

    query_state = F.affine(hidden_states, query, state_dict[f"model.layers.{layer_count}.self_attn.q_proj.bias"])
    key_state = F.affine(hidden_states, key, state_dict[f"model.layers.{layer_count}.self_attn.k_proj.bias"])
    value_state = F.affine(hidden_states, value, state_dict[f"model.layers.{layer_count}.self_attn.v_proj.bias"])


    query_state = F.reshape(query_state, hidden_shape)
    key_state = F.reshape(key_state, hidden_shape)
    value_state = F.reshape(value_state, hidden_shape)

    query_state = F.transpose(query_state, (0, 2, 1, 3))
    key_state = F.transpose(key_state, (0, 2, 1, 3))
    value_state = F.transpose(value_state, (0, 2, 1, 3))

    query_state, key_state = apply_rotary_pos_emb(query_state, key_state, cos, sin)

    key_state, value_state = cache.kv_states.add(key_state, value_state)

    num_repeat = query_state.shape[1] // key_state.shape[1]
    key_state = repeat_kv(key_state, num_repeat)
    value_state = repeat_kv(value_state, num_repeat)

    if attention_mask is not None and len(attention_mask.shape) == 4:
        seq_len = key_state.shape[-2]
        attention_mask = F.slice(
            attention_mask,
            start=(0, 0, 0, 0),
            stop=(attention_mask.shape[0],
            attention_mask.shape[1],
            attention_mask.shape[2],
            seq_len),
            step=(1, 1, 1, 1)
        )

    origin_dtype = query_state.dtype

    def maybe_upcast(x):
        if x.dtype in (np.float16, np.bfloat16):
            return F.cast(x, np.float32)
        return x

    query_state = maybe_upcast(query_state)  
    key_state = maybe_upcast(key_state)  
    value_state = maybe_upcast(value_state)  

    if scale is None:
        scale = 1.0 / (key_state.shape[-1] ** 0.5)

    attn_weights = F.batch_matmul(query_state, key_state, transpose_a=False, transpose_b=True) * scale

    def causal_mask(q_len, k_len):
        mask = np.tril(np.ones((q_len, k_len), dtype=np.bool_))
        return nn.Variable.from_numpy_array(mask)

    #is_causal
    q_len, k_len = attn_weights.shape[-2], attn_weights.shape[-1]
    mask = causal_mask(q_len, k_len)
    mask = F.reshape(mask, (1,1,q_len,k_len))
    attn_weights = F.where(mask, attn_weights, F.constant(-1e9, attn_weights.shape))

    attn_mask = attention_mask

    if attn_mask is not None:
        if attn_mask.dtype == np.bool_:
            attn_weights = F.where(attn_mask, attn_weights, F.constant(-1e9, attn_weights.shape))
        else:
            attn_weights = attn_weights + attn_mask

    attn_probs = F.softmax(attn_weights, axis=-1)

    if dropout_p > 0.0:
        attn_probs = F.dropout(attn_probs, p=dropout_p)

    out = F.batch_matmul(attn_probs, value_state)
    out = F.cast(out, origin_dtype)
    out = F.transpose(out, (0, 2, 1, 3))
    out = F.reshape(out, (*input_shape, -1))
    attn_output = F.affine(out, state_dict[f"model.layers.{layer_count}.self_attn.o_proj.weight"])
    return attn_output

def _slice_last(x, start, stop):
    starts = (0,) * (x.ndim - 1) + (start,)
    stops = (0,) * (x.ndim - 1) + (stop,)
    return F.slice(x, start=starts, stop=stops)

def rotate_half(x):
    hd = x.shape[-1]
    half = hd // 2
    x1 = _slice_last(x, 0, half)
    x2 = _slice_last(x, half, hd)
    return F.concatenate(F.neg(x2), x1, axis=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, expand_dim=1):
    cos = F.expand_dims(cos, axis=expand_dim)
    sin = F.expand_dims(sin, axis=expand_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

def rms_norm(weight, hidden_states, eps):
    input_dtype = hidden_states.dtype #入力
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
    hidden_states = hidden_states * F.rsqrt(variance + eps)
    return weight * F.cast(hidden_states, input_dtype)

def mlp(state_dict, layer_count, x):
    #gate, up, down, act
    gate_w = state_dict[f"model.layers.{layer_count}.mlp.gate_proj.weight"]
    up_w = state_dict[f"model.layers.{layer_count}.mlp.up_proj.weight"]
    down_w = state_dict[f"model.layers.{layer_count}.mlp.down_proj.weight"]

    gate = F.affine(x, gate_w)
    act = F.mul2(gate, F.sigmoid(gate))
    up = F.affine(x, up_w)
    act_up = F.mul2(act, up)
    down = F.affine(act_up, down_w)
    return down

def decoder_layer(state_dict, max_layer, position_ids, hidden_states):
    cos, sin = rotary_emb(hidden_states, position_ids) #rotary_embの埋め込み

    for i in range(max_layer):
        residual = hidden_states
        hidden_states = rms_norm(state_dict[f"model.layers.{i}.input_layernorm.weight"], hidden_states)
        hidden_states = attention(
            state_dict=state_dict,
            layer_count=i,
            hidden_states=hidden_states,
            head_dim,
            cache,
            attention_mask,
            scale,
            dropout_p,
            cos,
            sin
        )
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = rms_norm(state_dict[f"model.layers.{i}.post_attention_layernorm.weight"], hidden_states)
        hidden_states = mlp(state_dict, i, hidden_states)
        hidden_states = residual + hidden_states
    return hidden_states

def rotary_emb(hidden_states, position_ids, config):
    B, L, H = hidden_states.shape
    if position_ids is None:
        pos_ids = np.arange(L, dtype=np.int32)
        pos_ids = np.tile(pos_ids, (B, 1))
        position_ids = nn.Variable.from_numpy_array(pos_ids)
    head_dim = getattr(config, "head_dim", None) or config.hidden_size // config.num_attention_heads
    partial_rotary_factor = getattr(config, "partial_rotary_factor", 1.0)
    dim = int(head_dim * partial_rotary_factor)

    base = getattr(config, "rope_theta", 10000.0)
    inv_freq = 1.0 / (base ** (np.arange(0, dim, 2) / dim))
    inv_freq = inv_freq.astype(np.float32)
    inv_freq_nn = nn.Variable.from_numpy_array(inv_freq)
    attention_factor = 1.0

    pos = F.reshape(position_ids, (position_ids.shape[0], 1, position_ids.shape[1]))
    freqs = F.batch_matmul(pos, F.reshape(inv_freq_nn, (1, inv_freq_nn.shape[0], 1)))
    freqs = F.transpose(freqs, (0, 2, 1))
    emb = F.concatenate(freqs, freqs, axis=-1)

    cos = F.cos(emb) * attention_factor
    sin = F.sin(emb) * attention_factor

    return cos, sin

def lm_head(state_dict, x):
    #linear
    weight = state_dict[f"model.embed_tokens.weight"]
    return F.affine(x, weight)
