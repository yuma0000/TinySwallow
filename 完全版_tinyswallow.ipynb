{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuma0000/TinySwallow/blob/main/%E5%AE%8C%E5%85%A8%E7%89%88_tinyswallow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC6s4FOmcrgs"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnudQPPV43xC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import inspect\n",
        "import importlib.util\n",
        "import re\n",
        "import json\n",
        "import huggingface_hub\n",
        "import numpy as np\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import functools\n",
        "import packaging\n",
        "\n",
        "from torch import nn\n",
        "from typing import Any, Union, Callable, Tuple, Optional, TypeVar, ContextManager\n",
        "from packaging import version\n",
        "from dataclasses import dataclass, fields, field, is_dataclass\n",
        "from collections.abc import Iterable, Iterator, MutableMapping\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from functools import wraps, lru_cache\n",
        "from abc import abstractmethod\n",
        "import unicodedata\n",
        "import regex as re\n",
        "from urllib.parse import urlparse\n",
        "from contextlib import ExitStack\n",
        "\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def init_on_device(device: \"torch.device\", include_buffers: bool = False):\n",
        "    if include_buffers:\n",
        "        with device:\n",
        "            yield\n",
        "        return\n",
        "\n",
        "    old_register_parameter = nn.Module.register_parameter\n",
        "    if include_buffers:\n",
        "        old_register_buffer = nn.Module.register_buffer\n",
        "\n",
        "    def register_empty_parameter(module, name, param):\n",
        "        old_register_parameter(module, name, param)\n",
        "        if param is not None:\n",
        "            param_cls = type(module._parameters[name])\n",
        "            kwargs = module._parameters[name].__dict__\n",
        "            kwargs[\"requires_grad\"] = param.requires_grad\n",
        "            module._parameters[name] = param_cls(module._parameters[name].to(device), **kwargs)\n",
        "\n",
        "    def register_empty_buffer(module, name, buffer, persistent=True):\n",
        "        old_register_buffer(module, name, buffer, persistent=persistent)\n",
        "        if buffer is not None:\n",
        "            module._buffers[name] = module._buffers[name].to(device)\n",
        "\n",
        "    if include_buffers:\n",
        "        tensor_constructors_to_patch = {\n",
        "            torch_function_name: getattr(torch, torch_function_name)\n",
        "            for torch_function_name in [\"empty\", \"zeros\", \"ones\", \"full\"]\n",
        "        }\n",
        "    else:\n",
        "        tensor_constructors_to_patch = {}\n",
        "\n",
        "    def patch_tensor_constructor(fn):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            kwargs[\"device\"] = device\n",
        "            return fn(*args, **kwargs)\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    try:\n",
        "        nn.Module.register_parameter = register_empty_parameter\n",
        "        if include_buffers:\n",
        "            nn.Module.register_buffer = register_empty_buffer\n",
        "        for torch_function_name in tensor_constructors_to_patch.keys():\n",
        "            setattr(torch, torch_function_name, patch_tensor_constructor(getattr(torch, torch_function_name)))\n",
        "        yield\n",
        "    finally:\n",
        "        nn.Module.register_parameter = old_register_parameter\n",
        "        if include_buffers:\n",
        "            nn.Module.register_buffer = old_register_buffer\n",
        "        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items():\n",
        "            setattr(torch, torch_function_name, old_torch_function)\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def init_empty_weights(include_buffers: bool = False):\n",
        "    with init_on_device(torch.device(\"meta\"), include_buffers=include_buffers) as f:\n",
        "        yield f\n",
        "\n",
        "__version__ = \"4.54.0.dev0\"\n",
        "\n",
        "def is_deepspeed_zero3_enabled():\n",
        "    if _hf_deepspeed_config_weak_ref is not None and _hf_deepspeed_config_weak_ref() is not None:\n",
        "        return _hf_deepspeed_config_weak_ref().is_zero3()\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def is_fsdp_managed_module(module: nn.Module) -> bool:\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "\n",
        "    import torch\n",
        "\n",
        "    if not torch.distributed.is_available():\n",
        "        return False\n",
        "\n",
        "    import torch.distributed.fsdp\n",
        "\n",
        "    return isinstance(module, torch.distributed.fsdp.FullyShardedDataParallel) or getattr(\n",
        "        module, \"_is_fsdp_managed_module\", False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agA2kVceADvd"
      },
      "outputs": [],
      "source": [
        "class EosTokenCriteria():\n",
        "    def __init__(self, eos_token_id: Union[int, list[int], torch.Tensor]):\n",
        "        if not isinstance(eos_token_id, torch.Tensor):\n",
        "            if isinstance(eos_token_id, int):\n",
        "                eos_token_id = [eos_token_id]\n",
        "            eos_token_id = torch.tensor(eos_token_id)\n",
        "        self.eos_token_id = eos_token_id\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        self.eos_token_id = self.eos_token_id.to(input_ids.device)\n",
        "        is_done = isin_mps_friendly(input_ids[:, -1], self.eos_token_id)\n",
        "        return is_done\n",
        "\n",
        "class MaxLengthCriteria():\n",
        "    def __init__(self, max_length: int, max_position_embeddings: Optional[int] = None):\n",
        "        self.max_length = max_length\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        cur_len = input_ids.shape[1]\n",
        "        is_done = cur_len >= self.max_length\n",
        "        return torch.full((input_ids.shape[0],), is_done, device=input_ids.device, dtype=torch.bool)\n",
        "\n",
        "class StoppingCriteriaList(list):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        is_done = torch.full((input_ids.shape[0],), False, device=input_ids.device, dtype=torch.bool)\n",
        "        for criteria in self:\n",
        "            is_done = is_done | criteria(input_ids, scores, **kwargs)\n",
        "        return is_done\n",
        "\n",
        "    @property\n",
        "    def max_length(self) -> Optional[int]:\n",
        "        for stopping_criterium in self:\n",
        "            if isinstance(stopping_criterium, MaxLengthCriteria):\n",
        "                return stopping_criterium.max_length\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKEATT6k7DnK"
      },
      "outputs": [],
      "source": [
        "class LogitsProcessorList(list):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
        "        for processor in self:\n",
        "            function_args = inspect.signature(processor.__call__).parameters\n",
        "            if len(function_args) > 2:\n",
        "                scores = processor(input_ids, scores, **kwargs)\n",
        "            else:\n",
        "                scores = processor(input_ids, scores)\n",
        "\n",
        "        return scores\n",
        "\n",
        "class RepetitionPenaltyLogitsProcessor():\n",
        "    def __init__(self, penalty: float, prompt_ignore_length: Optional[int] = None):\n",
        "        self.penalty = penalty\n",
        "        self.prompt_ignore_length = prompt_ignore_length\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        if self.prompt_ignore_length:\n",
        "            input_ids = input_ids[:, self.prompt_ignore_length :]\n",
        "\n",
        "        score = torch.gather(scores, 1, input_ids)\n",
        "\n",
        "        score = torch.where(score < 0, score * self.penalty, score / self.penalty)\n",
        "\n",
        "        scores_processed = scores.scatter(1, input_ids, score)\n",
        "        return scores_processed\n",
        "\n",
        "class TopKLogitsWarper():\n",
        "    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
        "        self.top_k = max(top_k, min_tokens_to_keep)\n",
        "        self.filter_value = filter_value\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        top_k = min(self.top_k, scores.size(-1))\n",
        "        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n",
        "        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
        "        return scores_processed\n",
        "\n",
        "class TemperatureLogitsWarper():\n",
        "    def __init__(self, temperature: float):\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        scores_processed = scores / self.temperature\n",
        "        return scores_processed\n",
        "\n",
        "class TopPLogitsWarper():\n",
        "    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
        "        top_p = float(top_p)\n",
        "        self.top_p = top_p\n",
        "        self.filter_value = filter_value\n",
        "        self.min_tokens_to_keep = min_tokens_to_keep\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        sorted_logits, sorted_indices = torch.sort(scores, descending=False)\n",
        "        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)\n",
        "        sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
        "        return scores_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG2B9hBw6dAQ"
      },
      "outputs": [],
      "source": [
        "def find_tied_parameters(model: \"nn.Module\", **kwargs):\n",
        "    all_named_parameters = dict(model.named_parameters(remove_duplicate=False))\n",
        "    no_duplicate_named_parameters = dict(model.named_parameters(remove_duplicate=True))\n",
        "    tied_param_names = set(all_named_parameters.keys()) - set(no_duplicate_named_parameters.keys())\n",
        "    tied_param_groups = {}\n",
        "    for tied_param_name in tied_param_names:\n",
        "        tied_param = all_named_parameters[tied_param_name]\n",
        "        for param_name, param in no_duplicate_named_parameters.items():\n",
        "            if param is tied_param:\n",
        "                if param_name not in tied_param_groups:\n",
        "                    tied_param_groups[param_name] = []\n",
        "                tied_param_groups[param_name].append(tied_param_name)\n",
        "\n",
        "    return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YHXOB4yxt0j"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KAkcvf3o07Z",
        "outputId": "7411f9e2-c803-43e7-f6ea-4da51639c37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected tokenizers version: 0.21.2\n"
          ]
        }
      ],
      "source": [
        "def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[tuple[bool, str], bool]:\n",
        "    package_exists = importlib.util.find_spec(pkg_name) is not None\n",
        "    package_version = \"N/A\"\n",
        "    if package_exists:\n",
        "        package_version = importlib.metadata.version(pkg_name)\n",
        "        print(f\"Detected {pkg_name} version: {package_version}\")\n",
        "    if return_version:\n",
        "        return package_exists, package_version\n",
        "    else:\n",
        "        return package_exists\n",
        "\n",
        "_tokenizers_available = _is_package_available(\"tokenizers\")\n",
        "\n",
        "def is_tokenizers_available():\n",
        "    return _tokenizers_available\n",
        "\n",
        "def is_torch_fx_proxy(x):\n",
        "    return False\n",
        "\n",
        "def _is_torch(x):\n",
        "    import torch\n",
        "\n",
        "    return isinstance(x, torch.Tensor)\n",
        "\n",
        "def is_torch_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_tf_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_flax_available():\n",
        "    return False\n",
        "\n",
        "def is_jax_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_numpy_array(x):\n",
        "    return _is_numpy(x)\n",
        "\n",
        "def is_mlx_array(x):\n",
        "    return False\n",
        "\n",
        "def _is_numpy(x):\n",
        "    return isinstance(x, np.ndarray)\n",
        "\n",
        "def infer_framework_from_repr(x):\n",
        "    representation = str(type(x))\n",
        "    if representation.startswith(\"<class 'torch.\"):\n",
        "        return \"pt\"\n",
        "    elif representation.startswith(\"<class 'tensorflow.\"):\n",
        "        return \"tf\"\n",
        "    elif representation.startswith(\"<class 'jax\"):\n",
        "        return \"jax\"\n",
        "    elif representation.startswith(\"<class 'numpy.\"):\n",
        "        return \"np\"\n",
        "    elif representation.startswith(\"<class 'mlx.\"):\n",
        "        return \"mlx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s3djMBmyr8h",
        "outputId": "3261878f-3902-4f2c-cca8-6bea6510ae76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected torch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n",
        "    if not _is_package_available(\"torch\"):\n",
        "        return False\n",
        "\n",
        "    if accept_dev:\n",
        "        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n",
        "            library_version\n",
        "        )\n",
        "    else:\n",
        "        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n",
        "\n",
        "_torch_distributed_available = torch.distributed.is_available()\n",
        "\n",
        "if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n",
        "    from torch.distributed.tensor import DTensor, Placement, Replicate, Shard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-3ixmlD9DDz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TensorParallelLayer:\n",
        "    use_dtensor = True\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh): ...\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh): ...\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        if self.use_dtensor:\n",
        "            distribute_module(\n",
        "                module,\n",
        "                device_mesh,\n",
        "                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n",
        "                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n",
        "            )\n",
        "\n",
        "class ColwiseParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "        use_dtensor=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Replicate(),)\n",
        "        self.output_layouts = (output_layouts or Shard(-1),)\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=False)\n",
        "        return input_tensor\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        if param_type == \"bias\":\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n",
        "            shard = [Shard(-1)]\n",
        "        else:\n",
        "            shard = [Shard(-2)]\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2)\n",
        "\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(\n",
        "                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n",
        "            )\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        if outputs.placements != output_layouts:\n",
        "            outputs = outputs.redistribute(placements=output_layouts, async_op=False)\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "class RowwiseParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "        use_dtensor=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Shard(-1),)\n",
        "        self.output_layouts = (output_layouts or Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        if param_type != \"bias\":\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n",
        "            shard = [Shard(-1)]\n",
        "        else:\n",
        "            shard = [Replicate()]\n",
        "            parameter = param[:]\n",
        "\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(\n",
        "                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n",
        "            )\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
        "            mod._bias = mod.bias\n",
        "            mod.bias = None\n",
        "\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        if outputs.placements != output_layouts:\n",
        "            outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n",
        "        if hasattr(mod, \"_bias\"):\n",
        "            outputs += mod._bias\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        module._distribute_module_applied = True\n",
        "        if self.use_dtensor:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                self.desired_input_layouts: tuple[Placement, ...] = (Shard(-1),)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                self.desired_input_layouts = (Replicate(),)\n",
        "            elif isinstance(module, nn.Parameter):\n",
        "                self.desired_input_layouts = (Shard(-1),)\n",
        "            else:\n",
        "                raise NotImplementedError(\"RowwiseParallel currently only support nn.Linear and nn.Embedding!\")\n",
        "\n",
        "            distribute_module(\n",
        "                module,\n",
        "                device_mesh,\n",
        "                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n",
        "                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n",
        "            )\n",
        "\n",
        "class IsolatedParallel(TensorParallelLayer):\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh=None):\n",
        "        input_tensor = inputs[0]\n",
        "        if isinstance(input_tensor, DTensor):\n",
        "            input_tensor = input_tensor.to_local()\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh=None):\n",
        "        return outputs\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        distribute_module(\n",
        "            module,\n",
        "            device_mesh,\n",
        "            partial(self._prepare_input_fn, None, None),\n",
        "            partial(self._prepare_output_fn, None, None),\n",
        "        )\n",
        "\n",
        "class GatherParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Replicate(),)\n",
        "        self.output_layouts = output_layouts\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        if inputs and isinstance(inputs[0], DTensor):\n",
        "            inputs = inputs[0].to_local()\n",
        "        return inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        torch.distributed.all_reduce(outputs[0], op=torch.distributed.ReduceOp.SUM, async_op=False)\n",
        "        return outputs\n",
        "\n",
        "class PackedRowwiseParallel(RowwiseParallel):\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -1)\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-1)], run_check=False)\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "class SequenceParallel(TensorParallelLayer):\n",
        "    def __init__(self, *, sequence_dim: int = 1, use_local_output: bool = False, use_dtensor=False):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (Replicate(),)\n",
        "        self.desired_input_layouts = (Shard(1),)\n",
        "        self.output_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = True\n",
        "        self.sequence_sharding = (Shard(sequence_dim),)\n",
        "        self.use_local_output = use_local_output\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        outputs = outputs.redistribute(\n",
        "            placements=(Replicate(),), async_op=True\n",
        "        )\n",
        "        return outputs.to_local()\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        parameter = param[...]\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(parameter, device_mesh, [Replicate()], run_check=False)\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "class ReplicateParallel(TensorParallelLayer):\n",
        "    def __init__(self, *, use_dtensor=True, use_local_output=True):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (Replicate(),)\n",
        "        self.output_layouts = (Replicate(),)\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        param = param[...].to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            param = param.contiguous()\n",
        "        param = DTensor.from_local(param, device_mesh, [Replicate()], run_check=False)\n",
        "        return param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iD0At3RRAJ_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij91P1vQZfu4",
        "outputId": "65d77ab6-4150-4d13-e649-8b4f7353f300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected safetensors version: 0.5.3\n",
            "Detected peft version: 0.16.0\n"
          ]
        }
      ],
      "source": [
        "CONFIG_NAME = \"config.json\"\n",
        "SAFE_WEIGHTS_NAME = \"model.safetensors\"\n",
        "GENERATION_CONFIG_NAME = \"generation_config.json\"\n",
        "\n",
        "ENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\n",
        "_safetensors_available = _is_package_available(\"safetensors\")\n",
        "_peft_available = _is_package_available(\"peft\")\n",
        "_torch_xla_available = False\n",
        "\n",
        "SpecificPretrainedConfigType = TypeVar(\"SpecificPretrainedConfigType\", bound=\"PretrainedConfig\")\n",
        "\n",
        "def is_safetensors_available():\n",
        "    return _safetensors_available\n",
        "\n",
        "def is_peft_available():\n",
        "    return _peft_available\n",
        "\n",
        "def is_remote_url(url_or_filename):\n",
        "    parsed = urlparse(url_or_filename)\n",
        "    return parsed.scheme in (\"http\", \"https\")\n",
        "\n",
        "def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False):\n",
        "    assert not (check_is_tpu and check_is_gpu), \"The check_is_tpu and check_is_gpu cannot both be true.\"\n",
        "\n",
        "    if not _torch_xla_available:\n",
        "        return False\n",
        "\n",
        "    import torch_xla\n",
        "\n",
        "    if check_is_gpu:\n",
        "        return torch_xla.runtime.device_type() in [\"GPU\", \"CUDA\"]\n",
        "    elif check_is_tpu:\n",
        "        return torch_xla.runtime.device_type() == \"TPU\"\n",
        "\n",
        "    return True\n",
        "\n",
        "def is_torch_sdpa_available():\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "    elif _torch_version == \"N/A\":\n",
        "        return False\n",
        "\n",
        "    if is_torch_mlu_available():\n",
        "        return True\n",
        "    if is_torch_npu_available():\n",
        "        return True\n",
        "    return version.parse(_torch_version) >= version.parse(\"2.1.1\")\n",
        "\n",
        "def deprecate_kwarg(\n",
        "    old_name: str,\n",
        "    version: str,\n",
        "    new_name: Optional[str] = None,\n",
        "    warn_if_greater_or_equal_version: bool = False,\n",
        "    raise_if_greater_or_equal_version: bool = False,\n",
        "    raise_if_both_names: bool = False,\n",
        "    additional_message: Optional[str] = None,\n",
        "):\n",
        "    deprecated_version = packaging.version.parse(version)\n",
        "    current_version = packaging.version.parse(__version__)\n",
        "    is_greater_or_equal_version = current_version >= deprecated_version\n",
        "\n",
        "    if is_greater_or_equal_version:\n",
        "        version_message = f\"and removed starting from version {version}\"\n",
        "    else:\n",
        "        version_message = f\"and will be removed in version {version}\"\n",
        "\n",
        "    def wrapper(func):\n",
        "        sig = inspect.signature(func)\n",
        "        function_named_args = set(sig.parameters.keys())\n",
        "        is_instance_method = \"self\" in function_named_args\n",
        "        is_class_method = \"cls\" in function_named_args\n",
        "\n",
        "        @wraps(func)\n",
        "        def wrapped_func(*args, **kwargs):\n",
        "            func_name = func.__name__\n",
        "            if is_instance_method:\n",
        "                func_name = f\"{args[0].__class__.__name__}.{func_name}\"\n",
        "            elif is_class_method:\n",
        "                func_name = f\"{args[0].__name__}.{func_name}\"\n",
        "\n",
        "            minimum_action = Action.NONE\n",
        "            message = None\n",
        "\n",
        "            if old_name in kwargs and new_name in kwargs:\n",
        "                minimum_action = Action.RAISE if raise_if_both_names else Action.NOTIFY_ALWAYS\n",
        "                message = f\"Both `{old_name}` and `{new_name}` are set for `{func_name}`. Using `{new_name}={kwargs[new_name]}` and ignoring deprecated `{old_name}={kwargs[old_name]}`.\"\n",
        "                kwargs.pop(old_name)\n",
        "\n",
        "            elif old_name in kwargs and new_name is not None and new_name not in kwargs:\n",
        "                minimum_action = Action.NOTIFY\n",
        "                message = f\"`{old_name}` is deprecated {version_message} for `{func_name}`. Use `{new_name}` instead.\"\n",
        "                kwargs[new_name] = kwargs.pop(old_name)\n",
        "\n",
        "            elif old_name in kwargs:\n",
        "                minimum_action = Action.NOTIFY\n",
        "                message = f\"`{old_name}` is deprecated {version_message} for `{func_name}`.\"\n",
        "\n",
        "            if message is not None and additional_message is not None:\n",
        "                message = f\"{message} {additional_message}\"\n",
        "\n",
        "            if is_greater_or_equal_version:\n",
        "                if raise_if_greater_or_equal_version and minimum_action != Action.NONE:\n",
        "                    minimum_action = Action.RAISE\n",
        "\n",
        "                elif not warn_if_greater_or_equal_version and minimum_action == Action.NOTIFY:\n",
        "                    minimum_action = Action.NONE\n",
        "\n",
        "            return func(*args, **kwargs)\n",
        "\n",
        "        return wrapped_func\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "class ContextManagers:\n",
        "    def __init__(self, context_managers: list[ContextManager]):\n",
        "        self.context_managers = context_managers\n",
        "        self.stack = ExitStack()\n",
        "\n",
        "    def __enter__(self):\n",
        "        for context_manager in self.context_managers:\n",
        "            self.stack.enter_context(context_manager)\n",
        "\n",
        "    def __exit__(self, *args, **kwargs):\n",
        "        self.stack.__exit__(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj3ZBcGc1DoS"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PretrainedConfig():\n",
        "    model_type: str = \"\"\n",
        "    base_config_key: str = \"\"\n",
        "    sub_configs: dict[str, type[\"PretrainedConfig\"]] = {}\n",
        "    has_no_defaults_at_init: bool = False\n",
        "    attribute_map: dict[str, str] = {}\n",
        "    base_model_tp_plan: Optional[dict[str, Any]] = None\n",
        "    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n",
        "    _auto_class: Optional[str] = None\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        output_hidden_states: bool = False,\n",
        "        output_attentions: bool = False,\n",
        "        return_dict: bool = True,\n",
        "        torchscript: bool = False,\n",
        "        torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n",
        "        pruned_heads: Optional[dict[int, list[int]]] = None,\n",
        "        tie_word_embeddings: bool = True,\n",
        "        chunk_size_feed_forward: int = 0,\n",
        "        is_encoder_decoder: bool = False,\n",
        "        is_decoder: bool = False,\n",
        "        cross_attention_hidden_size: Optional[int] = None,\n",
        "        add_cross_attention: bool = False,\n",
        "        tie_encoder_decoder: bool = False,\n",
        "        architectures: Optional[list[str]] = None,\n",
        "        finetuning_task: Optional[str] = None,\n",
        "        id2label: Optional[dict[int, str]] = None,\n",
        "        label2id: Optional[dict[str, int]] = None,\n",
        "        num_labels: Optional[int] = None,\n",
        "        task_specific_params: Optional[dict[str, Any]] = None,\n",
        "        problem_type: Optional[str] = None,\n",
        "        tokenizer_class: Optional[str] = None,\n",
        "        prefix: Optional[str] = None,\n",
        "        bos_token_id: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        sep_token_id: Optional[int] = None,\n",
        "        decoder_start_token_id: Optional[int] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        if torch_dtype is not None and isinstance(torch_dtype, str) and is_torch_available():\n",
        "            import torch\n",
        "\n",
        "            torch_dtype = getattr(torch, torch_dtype)\n",
        "\n",
        "        self.return_dict = return_dict\n",
        "        self.output_hidden_states = output_hidden_states\n",
        "        self.torchscript = torchscript\n",
        "        self.torch_dtype = torch_dtype\n",
        "        self._output_attentions = output_attentions\n",
        "\n",
        "        self.pruned_heads = pruned_heads if pruned_heads is not None else {}\n",
        "        self.tie_word_embeddings = tie_word_embeddings\n",
        "        self.chunk_size_feed_forward = chunk_size_feed_forward\n",
        "\n",
        "        self.is_encoder_decoder = is_encoder_decoder\n",
        "        self.is_decoder = is_decoder\n",
        "        self.cross_attention_hidden_size = cross_attention_hidden_size\n",
        "        self.add_cross_attention = add_cross_attention\n",
        "        self.tie_encoder_decoder = tie_encoder_decoder\n",
        "\n",
        "        self.architectures = architectures\n",
        "        self.finetuning_task = finetuning_task\n",
        "        self.id2label = id2label\n",
        "        self.label2id = label2id\n",
        "        self.task_specific_params = task_specific_params\n",
        "        self.problem_type = problem_type\n",
        "\n",
        "        if self.id2label is None:\n",
        "            self._create_id_label_maps(num_labels if num_labels is not None else 2)\n",
        "        else:\n",
        "            self.id2label = {int(key): value for key, value in self.id2label.items()}\n",
        "\n",
        "        self.tokenizer_class = tokenizer_class\n",
        "        self.prefix = prefix\n",
        "        self.bos_token_id = bos_token_id\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.sep_token_id = sep_token_id\n",
        "        self.decoder_start_token_id = decoder_start_token_id\n",
        "\n",
        "        for parameter_name, default_value in self._get_global_generation_defaults().items():\n",
        "            setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n",
        "\n",
        "        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n",
        "        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        self._attn_implementation_internal = kwargs.pop(\"attn_implementation\", None)\n",
        "        self._attn_implementation_autoset = False\n",
        "\n",
        "        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "        self.tf_legacy_loss = kwargs.pop(\"tf_legacy_loss\", False)\n",
        "        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls: type[SpecificPretrainedConfigType],\n",
        "        pretrained_model_name_or_path: Union[str, os.PathLike],\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        **kwargs,\n",
        "    ) -> SpecificPretrainedConfigType:\n",
        "        kwargs[\"cache_dir\"] = cache_dir\n",
        "        kwargs[\"force_download\"] = force_download\n",
        "        kwargs[\"local_files_only\"] = local_files_only\n",
        "        kwargs[\"revision\"] = revision\n",
        "\n",
        "        cls._set_token_in_kwargs(kwargs, token)\n",
        "\n",
        "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "        if cls.base_config_key and cls.base_config_key in config_dict:\n",
        "            config_dict = config_dict[cls.base_config_key]\n",
        "\n",
        "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
        "            for v in config_dict.values():\n",
        "                if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n",
        "                    config_dict = v\n",
        "\n",
        "        return cls.from_dict(config_dict, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_token_in_kwargs(kwargs, token=None):\n",
        "        if token is None:\n",
        "            token = kwargs.pop(\"token\", None)\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "\n",
        "        if token is not None:\n",
        "            kwargs[\"token\"] = token\n",
        "\n",
        "    @classmethod\n",
        "    def _get_config_dict(\n",
        "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
        "    ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
        "        force_download = kwargs.pop(\"force_download\", False)\n",
        "        resume_download = kwargs.pop(\"resume_download\", None)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        token = kwargs.pop(\"token\", None)\n",
        "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
        "        revision = kwargs.pop(\"revision\", None)\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        gguf_file = kwargs.get(\"gguf_file\", None)\n",
        "\n",
        "        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n",
        "        if from_pipeline is not None:\n",
        "            user_agent[\"using_pipeline\"] = from_pipeline\n",
        "\n",
        "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
        "\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "        if os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n",
        "            resolved_config_file = pretrained_model_name_or_path\n",
        "            is_local = True\n",
        "        elif is_remote_url(pretrained_model_name_or_path):\n",
        "            configuration_file = pretrained_model_name_or_path if gguf_file is None else gguf_file\n",
        "            resolved_config_file = download_url(pretrained_model_name_or_path)\n",
        "        else:\n",
        "            configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME) if gguf_file is None else gguf_file\n",
        "\n",
        "            resolved_config_file = cached_file(\n",
        "                pretrained_model_name_or_path,\n",
        "                configuration_file,\n",
        "                cache_dir=cache_dir,\n",
        "                force_download=force_download,\n",
        "                proxies=proxies,\n",
        "                resume_download=resume_download,\n",
        "                local_files_only=local_files_only,\n",
        "                token=token,\n",
        "                user_agent=user_agent,\n",
        "                revision=revision,\n",
        "                subfolder=subfolder,\n",
        "                _commit_hash=commit_hash,\n",
        "            )\n",
        "            if resolved_config_file is None:\n",
        "                return None, kwargs\n",
        "            commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "\n",
        "        if gguf_file:\n",
        "            config_dict = load_gguf_checkpoint(resolved_config_file, return_tensors=False)[\"config\"]\n",
        "        else:\n",
        "            config_dict = cls._dict_from_json_file(resolved_config_file)\n",
        "\n",
        "        config_dict[\"_commit_hash\"] = commit_hash\n",
        "        if \"model_type\" not in config_dict and is_timm_config_dict(config_dict):\n",
        "            config_dict[\"model_type\"] = \"timm_wrapper\"\n",
        "\n",
        "        return config_dict, kwargs\n",
        "\n",
        "    @classmethod\n",
        "    def get_config_dict(\n",
        "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
        "    ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "        cls._set_token_in_kwargs(kwargs)\n",
        "\n",
        "        original_kwargs = copy.deepcopy(kwargs)\n",
        "        config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "        if config_dict is None:\n",
        "            return {}, kwargs\n",
        "        if \"_commit_hash\" in config_dict:\n",
        "            original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        if \"configuration_files\" in config_dict:\n",
        "            configuration_file = get_configuration_file(config_dict[\"configuration_files\"])\n",
        "            config_dict, kwargs = cls._get_config_dict(\n",
        "                pretrained_model_name_or_path, _configuration_file=configuration_file, **original_kwargs\n",
        "            )\n",
        "\n",
        "        return config_dict, kwargs\n",
        "\n",
        "    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n",
        "        decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n",
        "        encoder_possible_text_config_names = (\"text_encoder\",)\n",
        "        if decoder:\n",
        "            possible_text_config_names = decoder_possible_text_config_names\n",
        "        else:\n",
        "            possible_text_config_names = encoder_possible_text_config_names + decoder_possible_text_config_names\n",
        "\n",
        "        valid_text_config_names = []\n",
        "        for text_config_name in possible_text_config_names:\n",
        "            if hasattr(self, text_config_name):\n",
        "                text_config = getattr(self, text_config_name, None)\n",
        "                if text_config is not None:\n",
        "                    valid_text_config_names += [text_config_name]\n",
        "\n",
        "        if len(valid_text_config_names) == 1:\n",
        "            config_to_return = getattr(self, valid_text_config_names[0])\n",
        "        else:\n",
        "            config_to_return = self\n",
        "        return config_to_return\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_global_generation_defaults() -> dict[str, Any]:\n",
        "        return {\n",
        "            \"max_length\": 20,\n",
        "            \"min_length\": 0,\n",
        "            \"do_sample\": False,\n",
        "            \"early_stopping\": False,\n",
        "            \"num_beams\": 1,\n",
        "            \"num_beam_groups\": 1,\n",
        "            \"diversity_penalty\": 0.0,\n",
        "            \"temperature\": 1.0,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 1.0,\n",
        "            \"typical_p\": 1.0,\n",
        "            \"repetition_penalty\": 1.0,\n",
        "            \"length_penalty\": 1.0,\n",
        "            \"no_repeat_ngram_size\": 0,\n",
        "            \"encoder_no_repeat_ngram_size\": 0,\n",
        "            \"bad_words_ids\": None,\n",
        "            \"num_return_sequences\": 1,\n",
        "            \"output_scores\": False,\n",
        "            \"return_dict_in_generate\": False,\n",
        "            \"forced_bos_token_id\": None,\n",
        "            \"forced_eos_token_id\": None,\n",
        "            \"remove_invalid_values\": False,\n",
        "            \"exponential_decay_length_penalty\": None,\n",
        "            \"suppress_tokens\": None,\n",
        "            \"begin_suppress_tokens\": None,\n",
        "            }\n",
        "\n",
        "    def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n",
        "        if hasattr(self, \"quantization_config\"):\n",
        "            _ = d.pop(\"_pre_quantization_dtype\", None)\n",
        "\n",
        "        if \"_auto_class\" in d:\n",
        "            del d[\"_auto_class\"]\n",
        "        if \"_output_attentions\" in d:\n",
        "            d[\"output_attentions\"] = d.pop(\"_output_attentions\")\n",
        "        if \"_commit_hash\" in d:\n",
        "            del d[\"_commit_hash\"]\n",
        "        if \"_attn_implementation_internal\" in d:\n",
        "            del d[\"_attn_implementation_internal\"]\n",
        "        if \"_attn_implementation_autoset\" in d:\n",
        "            del d[\"_attn_implementation_autoset\"]\n",
        "        if \"base_model_tp_plan\" in d:\n",
        "            del d[\"base_model_tp_plan\"]\n",
        "        if \"base_model_pp_plan\" in d:\n",
        "            del d[\"base_model_pp_plan\"]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self._remove_keys_not_serialized(value)\n",
        "\n",
        "\n",
        "    def _create_id_label_maps(self, num_labels: int):\n",
        "        self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n",
        "        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n",
        "\n",
        "    @classmethod\n",
        "    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
        "        with open(json_file, encoding=\"utf-8\") as reader:\n",
        "            text = reader.read()\n",
        "        return json.loads(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(\n",
        "        cls: type[SpecificPretrainedConfigType], config_dict: dict[str, Any], **kwargs\n",
        "    ) -> SpecificPretrainedConfigType:\n",
        "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
        "        kwargs.pop(\"_from_auto\", None)\n",
        "        kwargs.pop(\"_from_pipeline\", None)\n",
        "        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n",
        "            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n",
        "\n",
        "        config = cls(**config_dict)\n",
        "\n",
        "        if hasattr(config, \"pruned_heads\"):\n",
        "            config.pruned_heads = {int(key): value for key, value in config.pruned_heads.items()}\n",
        "\n",
        "        if \"num_labels\" in kwargs and \"id2label\" in kwargs:\n",
        "            num_labels = kwargs[\"num_labels\"]\n",
        "            id2label = kwargs[\"id2label\"] if kwargs[\"id2label\"] is not None else []\n",
        "        to_remove = []\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(config, key):\n",
        "                current_attr = getattr(config, key)\n",
        "                if isinstance(current_attr, PretrainedConfig) and isinstance(value, dict):\n",
        "                    value = current_attr.__class__(**value)\n",
        "                setattr(config, key, value)\n",
        "                if key != \"torch_dtype\":\n",
        "                    to_remove.append(key)\n",
        "        for key in to_remove:\n",
        "            kwargs.pop(key, None)\n",
        "\n",
        "        if return_unused_kwargs:\n",
        "            return config, kwargs\n",
        "        else:\n",
        "            return config\n",
        "\n",
        "    def to_dict(self) -> dict[str, Any]:\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        if hasattr(self.__class__, \"model_type\"):\n",
        "            output[\"model_type\"] = self.__class__.model_type\n",
        "\n",
        "        output[\"transformers_version\"] = __version__\n",
        "\n",
        "        for key, value in output.items():\n",
        "            if isinstance(value, PretrainedConfig):\n",
        "                value = value.to_dict()\n",
        "                del value[\"transformers_version\"]\n",
        "\n",
        "            output[key] = value\n",
        "\n",
        "        self._remove_keys_not_serialized(output)\n",
        "\n",
        "        if hasattr(self, \"quantization_config\"):\n",
        "            output[\"quantization_config\"] = (\n",
        "                self.quantization_config.to_dict()\n",
        "                if not isinstance(self.quantization_config, dict)\n",
        "                else self.quantization_config\n",
        "            )\n",
        "        self.dict_torch_dtype_to_str(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n",
        "        if d.get(\"torch_dtype\", None) is not None:\n",
        "            if isinstance(d[\"torch_dtype\"], dict):\n",
        "                d[\"torch_dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"torch_dtype\"].items()}\n",
        "            elif not isinstance(d[\"torch_dtype\"], str):\n",
        "                d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self.dict_torch_dtype_to_str(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5AqaGacSvRU"
      },
      "outputs": [],
      "source": [
        "class PreTrainedTokenizer():\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        self.tokens_trie = Trie()\n",
        "\n",
        "        if not hasattr(self, \"_added_tokens_decoder\"):\n",
        "            self._added_tokens_decoder = {}\n",
        "\n",
        "        self._added_tokens_decoder.update(kwargs.pop(\"added_tokens_decoder\", {}))\n",
        "        self._added_tokens_encoder: dict[str, int] = {k.content: v for v, k in self._added_tokens_decoder.items()}\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self._add_tokens(\n",
        "            [token for token in self.all_special_tokens_extended if token not in self._added_tokens_encoder],\n",
        "            special_tokens=True,\n",
        "        )\n",
        "\n",
        "        self._decode_use_source_tokenizer = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTKZ8pDnQQkL"
      },
      "outputs": [],
      "source": [
        "class ModelOutput(OrderedDict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        is_modeloutput_subclass = self.__class__ != ModelOutput\n",
        "\n",
        "    def __getitem__(self, k):\n",
        "        if isinstance(k, str):\n",
        "            inner_dict = dict(self.items())\n",
        "            return inner_dict[k]\n",
        "        else:\n",
        "            return self.to_tuple()[k]\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if name in self.keys() and value is not None:\n",
        "            super().__setitem__(name, value)\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        super().__setitem__(key, value)\n",
        "        super().__setattr__(key, value)\n",
        "\n",
        "    def __reduce__(self):\n",
        "        if not is_dataclass(self):\n",
        "            return super().__reduce__()\n",
        "        callable, _args, *remaining = super().__reduce__()\n",
        "        args = tuple(getattr(self, field.name) for field in fields(self))\n",
        "        return callable, args, *remaining\n",
        "\n",
        "    def to_tuple(self) -> tuple[Any]:\n",
        "        return tuple(self[k] for k in self.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyA21IWgn2gJ",
        "outputId": "47c2d2af-e8ef-4e57-b2a0-8132266e4eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected torch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n",
        "    if not _is_package_available(\"torch\"):\n",
        "        return False\n",
        "\n",
        "    if accept_dev:\n",
        "        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n",
        "            library_version\n",
        "        )\n",
        "    else:\n",
        "        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n",
        "\n",
        "_is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)\n",
        "\n",
        "def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n",
        "    return kv_idx <= q_idx\n",
        "\n",
        "def prepare_padding_mask(\n",
        "    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int, _slice: bool = True\n",
        ") -> Optional[torch.Tensor]:\n",
        "    local_padding_mask = attention_mask\n",
        "    if attention_mask is not None:\n",
        "        if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
        "            local_padding_mask = torch.nn.functional.pad(attention_mask, (0, padding_length))\n",
        "        if _slice:\n",
        "            mask_indices = torch.arange(kv_length, device=local_padding_mask.device)\n",
        "            mask_indices += kv_offset\n",
        "            local_padding_mask = local_padding_mask[:, mask_indices]\n",
        "    return local_padding_mask\n",
        "\n",
        "_torch_available = False\n",
        "\n",
        "def is_torch_available():\n",
        "    return _torch_available\n",
        "\n",
        "def is_torchdynamo_compiling():\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "\n",
        "    import torch\n",
        "\n",
        "    return torch.compiler.is_compiling()\n",
        "\n",
        "def _ignore_causal_mask_sdpa(\n",
        "    padding_mask: Optional[torch.Tensor],\n",
        "    query_length: int,\n",
        "    kv_length: int,\n",
        "    kv_offset: int,\n",
        "    local_attention_size: Optional[int] = None,\n",
        ") -> bool:\n",
        "    is_tracing = torch.jit.is_tracing() or isinstance(padding_mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n",
        "    if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n",
        "        mask_indices = torch.arange(kv_length, device=padding_mask.device)\n",
        "        mask_indices += kv_offset\n",
        "        padding_mask = padding_mask[:, mask_indices]\n",
        "\n",
        "    if (\n",
        "        not is_tracing\n",
        "        and (query_length == 1 or kv_length == query_length)\n",
        "        and (local_attention_size is None or kv_length < local_attention_size)\n",
        "        and (padding_mask is None or padding_mask.all())\n",
        "    ):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def sdpa_mask_recent_torch(\n",
        "    batch_size: int,\n",
        "    cache_position: torch.Tensor,\n",
        "    kv_length: int,\n",
        "    kv_offset: int = 0,\n",
        "    mask_function: Callable = causal_mask_function,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    local_size: Optional[int] = None,\n",
        "    allow_is_causal_skip: bool = True,\n",
        "    **kwargs,\n",
        ") -> Optional[torch.Tensor]:\n",
        "    q_length = cache_position.shape[0]\n",
        "    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n",
        "\n",
        "    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n",
        "        return None\n",
        "\n",
        "    kv_arange = torch.arange(kv_length, device=cache_position.device)\n",
        "    kv_arange += kv_offset\n",
        "\n",
        "    if padding_mask is not None:\n",
        "        mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n",
        "\n",
        "    batch_arange = torch.arange(batch_size, device=cache_position.device)\n",
        "    head_arange = torch.arange(1, device=cache_position.device)\n",
        "    with TransformGetItemToIndex():\n",
        "        causal_mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n",
        "\n",
        "    return causal_mask\n",
        "\n",
        "sdpa_mask = sdpa_mask_recent_torch if _is_torch_greater_or_equal_than_2_6 else sdpa_mask_older_torch\n",
        "\n",
        "def _preprocess_mask_arguments(\n",
        "    config: PretrainedConfig,\n",
        "    input_embeds: torch.Tensor,\n",
        "    attention_mask: Optional[Union[torch.Tensor, torch.Tensor]],\n",
        "    cache_position: torch.Tensor,\n",
        "    past_key_values: Optional[Cache],\n",
        "    position_ids: Optional[torch.Tensor],\n",
        "    layer_idx: Optional[int],\n",
        ") -> tuple[bool, Optional[Union[torch.Tensor, torch.Tensor]], int, int]:\n",
        "\n",
        "    if isinstance(attention_mask, (torch.Tensor, torch.Tensor)) and len(attention_mask.shape) == 4:\n",
        "        return True, attention_mask, None, None, None\n",
        "\n",
        "    if config._attn_implementation not in AttentionMaskInterface._global_mapping:\n",
        "        return True, None, None, None, None\n",
        "\n",
        "    if attention_mask is not None and attention_mask.ndim == 2:\n",
        "        attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n",
        "\n",
        "    if past_key_values is not None:\n",
        "        kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n",
        "    else:\n",
        "        kv_length, kv_offset = input_embeds.shape[1], 0\n",
        "\n",
        "    packed_sequence_mask = None\n",
        "    if position_ids is not None and attention_mask is None and past_key_values is None:\n",
        "        batch_size = input_embeds.shape[0]\n",
        "        if batch_size != position_ids.shape[0]:\n",
        "            position_ids = position_ids.expand(batch_size, -1)\n",
        "        packed_sequence_mask = find_packed_sequence_indices(position_ids)\n",
        "\n",
        "    return False, attention_mask, packed_sequence_mask, kv_length, kv_offset\n",
        "\n",
        "def create_causal_mask(\n",
        "    config: PretrainedConfig,\n",
        "    input_embeds: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    cache_position: torch.Tensor,\n",
        "    past_key_values: Optional[Cache],\n",
        "    position_ids: Optional[torch.Tensor] = None,\n",
        "    or_mask_function: Optional[Callable] = None,\n",
        "    and_mask_function: Optional[Callable] = None,\n",
        ") -> Optional[Union[torch.Tensor, torch.Tensor]]:\n",
        "    if hasattr(past_key_values, \"is_sliding\") and False in past_key_values.is_sliding:\n",
        "        layer_idx = past_key_values.is_sliding.index(False)\n",
        "    else:\n",
        "        layer_idx = 0\n",
        "\n",
        "    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n",
        "        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n",
        "    )\n",
        "    if early_exit:\n",
        "        return attention_mask\n",
        "\n",
        "    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n",
        "    mask_factory_function = causal_mask_function\n",
        "    mask_interface = sdpa_mask\n",
        "\n",
        "    allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n",
        "\n",
        "    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n",
        "        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n",
        "        allow_is_causal_skip = False\n",
        "\n",
        "    if or_mask_function is not None:\n",
        "        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n",
        "        allow_is_causal_skip = False\n",
        "    if and_mask_function is not None:\n",
        "        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n",
        "        allow_is_causal_skip = False\n",
        "\n",
        "    causal_mask = mask_interface(\n",
        "        batch_size=batch_size,\n",
        "        cache_position=cache_position,\n",
        "        kv_length=kv_length,\n",
        "        kv_offset=kv_offset,\n",
        "        mask_function=mask_factory_function,\n",
        "        attention_mask=attention_mask,\n",
        "        allow_is_causal_skip=allow_is_causal_skip,\n",
        "        dtype=dtype,\n",
        "        config=config,\n",
        "    )\n",
        "    return causal_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdlQFOkoQ0El"
      },
      "outputs": [],
      "source": [
        "class GeneralInterface(MutableMapping):\n",
        "    _global_mapping = {}\n",
        "    def __init__(self):\n",
        "        self._local_mapping = {}\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if key in self._local_mapping:\n",
        "            return self._local_mapping[key]\n",
        "        return self._global_mapping[key]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self._local_mapping.update({key: value})\n",
        "\n",
        "    def __delitem__(self, key):\n",
        "        del self._local_mapping[key]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter({**self._global_mapping, **self._local_mapping})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._global_mapping.keys() | self._local_mapping.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def register(cls, key: str, value: Callable):\n",
        "        cls._global_mapping.update({key: value})\n",
        "\n",
        "    @classmethod\n",
        "    def valid_keys(cls) -> list[str]:\n",
        "        return list(cls._global_mapping.keys())\n",
        "\n",
        "@dataclass\n",
        "class CausalLMOutputWithPast(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: Optional[torch.FloatTensor] = None\n",
        "    past_key_values: Optional[Cache] = None\n",
        "    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "    attentions: Optional[tuple[torch.FloatTensor, ...]] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18p38Q9meo6q"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class AttentionMaskInterface(GeneralInterface):\n",
        "    _global_mapping = {\n",
        "        \"sdpa\": sdpa_mask,\n",
        "    }\n",
        "\"\"\"\n",
        "def _get_frameworks_and_test_func(x):\n",
        "    framework_to_test = {\n",
        "        \"pt\": is_torch_tensor,\n",
        "        \"tf\": is_tf_tensor,\n",
        "        \"jax\": is_jax_tensor,\n",
        "        \"np\": is_numpy_array,\n",
        "        \"mlx\": is_mlx_array,\n",
        "    }\n",
        "    preferred_framework = infer_framework_from_repr(x)\n",
        "    frameworks = [] if preferred_framework is None else [preferred_framework]\n",
        "    if preferred_framework != \"np\":\n",
        "        frameworks.append(\"np\")\n",
        "    frameworks.extend([f for f in framework_to_test if f not in [preferred_framework, \"np\"]])\n",
        "    return {f: framework_to_test[f] for f in frameworks}\n",
        "\n",
        "\n",
        "\n",
        "def is_tensor(x):\n",
        "    framework_to_test_func = _get_frameworks_and_test_func(x)\n",
        "    for test_func in framework_to_test_func.values():\n",
        "        if test_func(x):\n",
        "            return True\n",
        "\n",
        "    if is_torch_fx_proxy(x):\n",
        "        return True\n",
        "\n",
        "    if is_flax_available():\n",
        "        from jax.core import Tracer\n",
        "\n",
        "        if isinstance(x, Tracer):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\"\"\"\n",
        "@dataclass\n",
        "class BaseModelOutputWithPast(ModelOutput):\n",
        "    last_hidden_state: Optional[torch.FloatTensor] = None\n",
        "    past_key_values: Optional[Cache] = None\n",
        "    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "    attentions: Optional[tuple[torch.FloatTensor, ...]] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQSiZa52xyNo"
      },
      "outputs": [],
      "source": [
        "def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int) -> torch.Tensor:\n",
        "    if elements.device.type == \"mps\" and not is_torch_greater_or_equal_than_2_4:\n",
        "        test_elements = torch.tensor(test_elements)\n",
        "        if test_elements.ndim == 0:\n",
        "            test_elements = test_elements.unsqueeze(0)\n",
        "        return elements.tile(test_elements.shape[0], 1).eq(test_elements.unsqueeze(1)).sum(dim=0).bool().squeeze()\n",
        "    else:\n",
        "        return torch.isin(elements, test_elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZM7SGAS7fHB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmOIHtMGVdJX"
      },
      "outputs": [],
      "source": [
        "def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n",
        "    if variant is not None:\n",
        "        path, name = weights_name.rsplit(\".\", 1)\n",
        "        weights_name = f\"{path}.{variant}.{name}\"\n",
        "    return weights_name\n",
        "\n",
        "if is_safetensors_available():\n",
        "    from safetensors import safe_open\n",
        "    from safetensors.torch import load_file as safe_load_file\n",
        "    from safetensors.torch import save_file as safe_save_file\n",
        "\n",
        "if is_peft_available():\n",
        "    from transformers.utils import find_adapter_config_file\n",
        "\n",
        "_init_weights = True\n",
        "_is_ds_init_called = False\n",
        "_torch_distributed_available = torch.distributed.is_available()\n",
        "SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n",
        "\n",
        "_CAN_RECORD_REGISTRY = {}\n",
        "\n",
        "TORCH_INIT_FUNCTIONS = {\n",
        "    \"uniform_\": nn.init.uniform_,\n",
        "    \"normal_\": nn.init.normal_,\n",
        "    \"trunc_normal_\": nn.init.trunc_normal_,\n",
        "    \"constant_\": nn.init.constant_,\n",
        "    \"xavier_uniform_\": nn.init.xavier_uniform_,\n",
        "    \"xavier_normal_\": nn.init.xavier_normal_,\n",
        "    \"kaiming_uniform_\": nn.init.kaiming_uniform_,\n",
        "    \"kaiming_normal_\": nn.init.kaiming_normal_,\n",
        "    \"uniform\": nn.init.uniform,\n",
        "    \"normal\": nn.init.normal,\n",
        "    \"xavier_uniform\": nn.init.xavier_uniform,\n",
        "    \"xavier_normal\": nn.init.xavier_normal,\n",
        "    \"kaiming_uniform\": nn.init.kaiming_uniform,\n",
        "    \"kaiming_normal\": nn.init.kaiming_normal,\n",
        "}\n",
        "\n",
        "VLMS = [\n",
        "    \"aria\",\n",
        "    \"ayavision\",\n",
        "    \"colpali\",\n",
        "    \"emu3\",\n",
        "    \"fuyu\",\n",
        "    \"gotocr2\",\n",
        "    \"gemma3\",\n",
        "    \"internvl\",\n",
        "    \"llava\",\n",
        "    \"mistral3\",\n",
        "    \"mllama\",\n",
        "    \"paligemma\",\n",
        "    \"shieldgemma2\",\n",
        "    \"qwen2vl\",\n",
        "    \"qwen2_5_vl\",\n",
        "    \"videollava\",\n",
        "    \"vipllava\",\n",
        "]\n",
        "\n",
        "str_to_torch_dtype = {\n",
        "    \"BOOL\": torch.bool,\n",
        "    \"U8\": torch.uint8,\n",
        "    \"I8\": torch.int8,\n",
        "    \"I16\": torch.int16,\n",
        "    \"F16\": torch.float16,\n",
        "    \"BF16\": torch.bfloat16,\n",
        "    \"I32\": torch.int32,\n",
        "    \"F32\": torch.float32,\n",
        "    \"F64\": torch.float64,\n",
        "    \"I64\": torch.int64,\n",
        "    \"F8_E4M3\": torch.float8_e4m3fn,\n",
        "    \"F8_E5M2\": torch.float8_e5m2,\n",
        "}\n",
        "\n",
        "def restore_default_torch_dtype(func):\n",
        "    @wraps(func)\n",
        "    def _wrapper(*args, **kwargs):\n",
        "        old_dtype = torch.get_default_dtype()\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        finally:\n",
        "            torch.set_default_dtype(old_dtype)\n",
        "\n",
        "    return _wrapper\n",
        "\n",
        "def get_torch_context_manager_or_global_device():\n",
        "    device_in_context = torch.tensor([]).device\n",
        "    default_device = torch.get_default_device() if is_torch_greater_or_equal(\"2.3\") else torch.device(\"cpu\")\n",
        "    if device_in_context == default_device:\n",
        "        if default_device != torch.device(\"cpu\"):\n",
        "            return default_device\n",
        "        return None\n",
        "    return device_in_context\n",
        "\n",
        "def load_state_dict(\n",
        "    checkpoint_file: Union[str, os.PathLike],\n",
        "    is_quantized: bool = False,\n",
        "    map_location: Optional[Union[str, torch.device]] = \"cpu\",\n",
        "    weights_only: bool = True,\n",
        "):\n",
        "    if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n",
        "        with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
        "            metadata = f.metadata()\n",
        "\n",
        "            if metadata is not None and metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\", \"mlx\"]:\n",
        "                raise OSError(\n",
        "                    f\"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure \"\n",
        "                    \"you save your model with the `save_pretrained` method.\"\n",
        "                )\n",
        "            state_dict = {}\n",
        "            for k in f.keys():\n",
        "                if map_location == \"meta\":\n",
        "                    _slice = f.get_slice(k)\n",
        "                    k_dtype = _slice.get_dtype()\n",
        "                    if k_dtype in str_to_torch_dtype:\n",
        "                        dtype = str_to_torch_dtype[k_dtype]\n",
        "                    state_dict[k] = torch.empty(size=_slice.get_shape(), dtype=dtype, device=\"meta\")\n",
        "                else:\n",
        "                    state_dict[k] = f.get_tensor(k)\n",
        "            return state_dict\n",
        "\n",
        "    if weights_only:\n",
        "        check_torch_load_is_safe()\n",
        "    if map_location is None:\n",
        "        if (\n",
        "            (\n",
        "                is_deepspeed_zero3_enabled()\n",
        "                and torch.distributed.is_initialized()\n",
        "                and torch.distributed.get_rank() > 0\n",
        "            )\n",
        "            or (is_fsdp_enabled() and not is_local_dist_rank_0())\n",
        "        ) and not is_quantized:\n",
        "            map_location = \"meta\"\n",
        "        else:\n",
        "            map_location = \"cpu\"\n",
        "    extra_args = {}\n",
        "    if isinstance(checkpoint_file, str) and map_location != \"meta\" and is_zipfile(checkpoint_file):\n",
        "        extra_args = {\"mmap\": True}\n",
        "    return torch.load(\n",
        "        checkpoint_file,\n",
        "        map_location=map_location,\n",
        "        weights_only=weights_only,\n",
        "        **extra_args,\n",
        "    )\n",
        "\n",
        "    def _get_key_renaming_mapping(\n",
        "        self,\n",
        "        checkpoint_keys: list[str],\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        loading_base_model_from_task_state_dict: bool = False,\n",
        "        loading_task_model_from_base_state_dict: bool = False,\n",
        "    ):\n",
        "        prefix = self.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "\n",
        "        renamed_keys = {}\n",
        "        key_renaming_mapping = {}\n",
        "        for key in checkpoint_keys:\n",
        "            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n",
        "\n",
        "            if key_mapping is not None:\n",
        "                for pattern, replacement in key_mapping.items():\n",
        "                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n",
        "                    if n_replace > 0:\n",
        "                        has_changed = True\n",
        "                        break\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                new_key = \".\".join([prefix, new_key])\n",
        "            elif loading_base_model_from_task_state_dict:\n",
        "                if not new_key.startswith(_prefix):\n",
        "                    continue\n",
        "                new_key = new_key[len(_prefix) :]\n",
        "\n",
        "            key_renaming_mapping[key] = new_key\n",
        "\n",
        "            if has_changed:\n",
        "                if key.endswith(\"LayerNorm.gamma\"):\n",
        "                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n",
        "                elif key.endswith(\"LayerNorm.beta\"):\n",
        "                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n",
        "\n",
        "        if renamed_keys:\n",
        "            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n",
        "            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n",
        "            for old_key, new_key in renamed_keys.values():\n",
        "                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n",
        "            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n",
        "        return key_renaming_mapping\n",
        "\n",
        "def _find_missing_and_unexpected_keys(\n",
        "    cls,\n",
        "    model: \"PreTrainedModel\",\n",
        "    original_checkpoint_keys: list[str],\n",
        "    checkpoint_keys: list[str],\n",
        "    loading_base_model_from_task_state_dict: bool,\n",
        "    hf_quantizer: Optional[HfQuantizer],\n",
        "    device_map: dict,\n",
        ") -> tuple[list[str], list[str]]:\n",
        "    prefix = model.base_model_prefix\n",
        "\n",
        "    expected_keys = list(model.state_dict().keys())\n",
        "    if hf_quantizer is not None:\n",
        "        expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n",
        "\n",
        "    missing_keys = sorted(set(expected_keys) - set(checkpoint_keys))\n",
        "    unexpected_keys = set(checkpoint_keys) - set(expected_keys)\n",
        "\n",
        "    if loading_base_model_from_task_state_dict:\n",
        "        task_specific_keys = [k for k in original_checkpoint_keys if not k.startswith(f\"{prefix}.\")]\n",
        "        unexpected_keys.update(task_specific_keys)\n",
        "\n",
        "    model_buffers = {n for n, _ in model.named_buffers()}\n",
        "    unexpected_keys = sorted(unexpected_keys - model_buffers)\n",
        "    has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer in model_buffers)\n",
        "    if has_inv_freq_buffers:\n",
        "        unexpected_keys = [k for k in unexpected_keys if \"rotary_emb.inv_freq\" not in k]\n",
        "\n",
        "    tied_params = find_tied_parameters(model)\n",
        "    for group in tied_params:\n",
        "        missing_in_group = [k for k in missing_keys if k in group]\n",
        "        if len(missing_in_group) > 0 and len(missing_in_group) < len(group):\n",
        "            missing_keys = [k for k in missing_keys if k not in missing_in_group]\n",
        "\n",
        "    if hf_quantizer is not None:\n",
        "        missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n",
        "        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n",
        "    if cls._keys_to_ignore_on_load_missing is not None:\n",
        "        for pattern in cls._keys_to_ignore_on_load_missing:\n",
        "            missing_keys = [k for k in missing_keys if re.search(pattern, k) is None]\n",
        "\n",
        "    if cls._keys_to_ignore_on_load_unexpected is not None:\n",
        "        for pattern in cls._keys_to_ignore_on_load_unexpected:\n",
        "            unexpected_keys = [k for k in unexpected_keys if re.search(pattern, k) is None]\n",
        "\n",
        "    return missing_keys, unexpected_keys\n",
        "\n",
        "def _find_mismatched_keys(\n",
        "    model: \"PreTrainedModel\",\n",
        "    state_dict: Optional[dict],\n",
        "    checkpoint_files: Optional[list[str]],\n",
        "    ignore_mismatched_sizes: bool,\n",
        "    keys_to_rename_mapping: dict[str, str],\n",
        "    is_quantized: bool,\n",
        "    weights_only: bool,\n",
        ") -> tuple[list[str], list[tuple[int, int]]]:\n",
        "    if not ignore_mismatched_sizes:\n",
        "        return [], []\n",
        "\n",
        "    if state_dict is not None:\n",
        "        checkpoint_files = [\"\"]\n",
        "\n",
        "    model_state_dict = model.state_dict()\n",
        "    mismatched_keys = []\n",
        "    mismatched_shapes = []\n",
        "    for shard_file in checkpoint_files:\n",
        "        if shard_file != \"\":\n",
        "            state_dict = load_state_dict(\n",
        "                shard_file, is_quantized=is_quantized, map_location=\"meta\", weights_only=weights_only\n",
        "            )\n",
        "\n",
        "        new_state_dict = {keys_to_rename_mapping[k]: v for k, v in state_dict.items() if k in keys_to_rename_mapping}\n",
        "\n",
        "        for key in new_state_dict.keys():\n",
        "            if key in model_state_dict and new_state_dict[key].shape != model_state_dict[key].shape:\n",
        "                if not (\n",
        "                    is_quantized\n",
        "                    and new_state_dict[key].shape[-1] == 1\n",
        "                    and new_state_dict[key].numel() * 2 == model_state_dict[key].numel()\n",
        "                ):\n",
        "                    mismatched_keys.append(key)\n",
        "                    mismatched_shapes.append((new_state_dict[key].shape, model_state_dict[key].shape))\n",
        "\n",
        "    return mismatched_keys, mismatched_shapes\n",
        "\n",
        "def is_fsdp_enabled():\n",
        "    return (\n",
        "        torch.distributed.is_available()\n",
        "        and torch.distributed.is_initialized()\n",
        "        and strtobool(os.environ.get(\"ACCELERATE_USE_FSDP\", \"False\")) == 1\n",
        "        and strtobool(os.environ.get(\"FSDP_CPU_RAM_EFFICIENT_LOADING\", \"False\")) == 1\n",
        "    )\n",
        "\n",
        "def set_initialized_submodules(model, state_dict_keys):\n",
        "    state_dict_keys = set(state_dict_keys)\n",
        "    not_initialized_submodules = {}\n",
        "    for module_name, module in model.named_modules():\n",
        "        if module_name == \"\":\n",
        "            module_keys = set(module.state_dict())\n",
        "        else:\n",
        "            module_keys = {f\"{module_name}.{k}\" for k in module.state_dict()}\n",
        "        if module_keys.issubset(state_dict_keys):\n",
        "            module._is_hf_initialized = True\n",
        "        else:\n",
        "            not_initialized_submodules[module_name] = module\n",
        "    return not_initialized_submodules\n",
        "\n",
        "def load_shard_file(args):\n",
        "    (\n",
        "        shard_file,\n",
        "        state_dict,\n",
        "        disk_only_shard_files,\n",
        "        is_hqq_or_bnb,\n",
        "        is_quantized,\n",
        "        device_map,\n",
        "        hf_quantizer,\n",
        "        key_renaming_mapping,\n",
        "        weights_only,\n",
        "        model_to_load,\n",
        "        expected_keys,\n",
        "        reverse_key_renaming_mapping,\n",
        "        disk_offload_folder,\n",
        "        disk_offload_index,\n",
        "        cpu_offload_folder,\n",
        "        cpu_offload_index,\n",
        "        is_offloaded_safetensors,\n",
        "        keep_in_fp32_regex,\n",
        "        unexpected_keys,\n",
        "        device_mesh,\n",
        "    ) = args\n",
        "\n",
        "    if shard_file in disk_only_shard_files:\n",
        "        return [], disk_offload_index, cpu_offload_index\n",
        "\n",
        "    map_location = \"cpu\"\n",
        "    if (\n",
        "        shard_file.endswith(\".safetensors\")\n",
        "        and not is_hqq_or_bnb\n",
        "        and not (is_deepspeed_zero3_enabled() and not is_quantized)\n",
        "    ):\n",
        "        map_location = \"meta\"\n",
        "    elif (\n",
        "        device_map is not None\n",
        "        and hf_quantizer is not None\n",
        "        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n",
        "        and (\n",
        "            hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n",
        "            or isinstance(hf_quantizer.quantization_config.quant_type, Int4WeightOnlyConfig)\n",
        "        )\n",
        "    ):\n",
        "        map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n",
        "\n",
        "    if shard_file != \"\":\n",
        "        state_dict = load_state_dict(\n",
        "            shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only\n",
        "        )\n",
        "\n",
        "    state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n",
        "\n",
        "    error_msgs = []\n",
        "\n",
        "    if is_deepspeed_zero3_enabled() and not is_quantized:\n",
        "        error_msgs += _load_state_dict_into_zero3_model(model_to_load, state_dict)\n",
        "    elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n",
        "        disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n",
        "            model_to_load,\n",
        "            state_dict,\n",
        "            shard_file,\n",
        "            expected_keys,\n",
        "            reverse_key_renaming_mapping,\n",
        "            device_map=device_map,\n",
        "            disk_offload_folder=disk_offload_folder,\n",
        "            disk_offload_index=disk_offload_index,\n",
        "            cpu_offload_folder=cpu_offload_folder,\n",
        "            cpu_offload_index=cpu_offload_index,\n",
        "            hf_quantizer=hf_quantizer,\n",
        "            is_safetensors=is_offloaded_safetensors,\n",
        "            keep_in_fp32_regex=keep_in_fp32_regex,\n",
        "            unexpected_keys=unexpected_keys,\n",
        "            device_mesh=device_mesh,\n",
        "        )\n",
        "\n",
        "    return error_msgs, disk_offload_index, cpu_offload_index\n",
        "\n",
        "@torch.no_grad()\n",
        "def _load_state_dict_into_meta_model(\n",
        "    model: \"PreTrainedModel\",\n",
        "    state_dict: dict,\n",
        "    shard_file: str,\n",
        "    expected_keys: list[str],\n",
        "    reverse_renaming_mapping: dict[str, str],\n",
        "    device_map: Optional[dict] = None,\n",
        "    disk_offload_folder: Optional[str] = None,\n",
        "    disk_offload_index: Optional[dict] = None,\n",
        "    cpu_offload_folder: Optional[str] = None,\n",
        "    cpu_offload_index: Optional[dict] = None,\n",
        "    hf_quantizer: Optional[HfQuantizer] = None,\n",
        "    is_safetensors: bool = False,\n",
        "    keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "    unexpected_keys: Optional[list[str]] = None,\n",
        "    device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n",
        ") -> tuple[Optional[dict], Optional[dict]]:\n",
        "    tensor_device = \"cpu\"\n",
        "    if device_map is not None and device_map.get(\"\", None) is not None:\n",
        "        if device_map[\"\"] not in (\"cpu\", torch.device(\"cpu\")):\n",
        "            tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n",
        "    if device_map is not None:\n",
        "        device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n",
        "\n",
        "    is_quantized = hf_quantizer is not None\n",
        "    is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "        QuantizationMethod.HQQ,\n",
        "        QuantizationMethod.BITS_AND_BYTES,\n",
        "    }\n",
        "    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb\n",
        "    file_pointer = None\n",
        "    if is_meta_state_dict:\n",
        "        file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n",
        "\n",
        "    for param_name, empty_param in state_dict.items():\n",
        "        if param_name not in expected_keys:\n",
        "            continue\n",
        "\n",
        "        if is_meta_state_dict:\n",
        "            serialized_param_name = reverse_renaming_mapping[param_name]\n",
        "            param = file_pointer.get_slice(serialized_param_name)\n",
        "        else:\n",
        "            param = empty_param.to(tensor_device)\n",
        "\n",
        "        to_contiguous, casting_dtype = _infer_parameter_dtype(\n",
        "            model,\n",
        "            param_name,\n",
        "            empty_param,\n",
        "            keep_in_fp32_regex,\n",
        "            hf_quantizer,\n",
        "        )\n",
        "\n",
        "        if device_mesh is not None:\n",
        "            shard_and_distribute_module(\n",
        "                model,\n",
        "                param,\n",
        "                empty_param,\n",
        "                param_name,\n",
        "                casting_dtype,\n",
        "                to_contiguous,\n",
        "                device_mesh.get_local_rank(),\n",
        "                device_mesh,\n",
        "            )\n",
        "        else:\n",
        "            param = param[...]\n",
        "            if casting_dtype is not None:\n",
        "                param = param.to(casting_dtype)\n",
        "            if to_contiguous:\n",
        "                param = param.contiguous()\n",
        "\n",
        "            if device_map is None:\n",
        "                param_device = \"cpu\"\n",
        "            else:\n",
        "                module_layer = re.search(device_map_regex, param_name)\n",
        "                param_device = device_map[module_layer.group()]\n",
        "\n",
        "            if param_device == \"disk\":\n",
        "                if not is_safetensors:\n",
        "                    disk_offload_index = offload_weight(param, param_name, disk_offload_folder, disk_offload_index)\n",
        "            elif param_device == \"cpu\" and cpu_offload_index is not None:\n",
        "                cpu_offload_index = offload_weight(param, param_name, cpu_offload_folder, cpu_offload_index)\n",
        "            elif (\n",
        "                not is_quantized\n",
        "                or (not hf_quantizer.requires_parameters_quantization)\n",
        "                or (\n",
        "                    not hf_quantizer.check_quantized_param(\n",
        "                        model,\n",
        "                        param,\n",
        "                        param_name,\n",
        "                        state_dict,\n",
        "                        param_device=param_device,\n",
        "                        device_map=device_map,\n",
        "                    )\n",
        "                )\n",
        "            ):\n",
        "                if is_fsdp_enabled():\n",
        "                    param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n",
        "\n",
        "                _load_parameter_into_model(model, param_name, param.to(param_device))\n",
        "\n",
        "            else:\n",
        "                hf_quantizer.create_quantized_param(\n",
        "                    model, param, param_name, param_device, state_dict, unexpected_keys\n",
        "                )\n",
        "                if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n",
        "                    module, param_type = get_module_from_name(model, param_name)\n",
        "                    value = getattr(module, param_type)\n",
        "                    param_to = \"cpu\"\n",
        "                    if is_fsdp_enabled() and not is_local_dist_rank_0():\n",
        "                        param_to = \"meta\"\n",
        "                    val_kwargs = {}\n",
        "                    if hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\":\n",
        "                        val_kwargs[\"requires_grad\"] = False\n",
        "                    value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n",
        "                    setattr(module, param_type, value)\n",
        "\n",
        "    if file_pointer is not None:\n",
        "        file_pointer.__exit__(None, None, None)\n",
        "\n",
        "    return disk_offload_index, cpu_offload_index\n",
        "\n",
        "def _infer_parameter_dtype(\n",
        "    model: \"PreTrainedModel\",\n",
        "    param_name: str,\n",
        "    empty_param: torch.Tensor,\n",
        "    keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "    hf_quantizer: Optional[HfQuantizer] = None,\n",
        ") -> Union[bool, Optional[torch.dtype]]:\n",
        "    old_param = model.get_parameter_or_buffer(param_name)\n",
        "    is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n",
        "    casting_dtype = None\n",
        "    is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n",
        "    if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n",
        "        if keep_in_fp32_regex is not None and keep_in_fp32_regex.search(param_name):\n",
        "            casting_dtype = torch.float32\n",
        "        elif hf_quantizer is not None:\n",
        "            casting_dtype = model.config._pre_quantization_dtype\n",
        "        else:\n",
        "            casting_dtype = old_param.dtype\n",
        "    return old_param is not None and old_param.is_contiguous(), casting_dtype\n",
        "\n",
        "def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor: torch.Tensor):\n",
        "    module, param_type = get_module_from_name(model, param_name)\n",
        "    module.load_state_dict({param_type: tensor}, strict=False, assign=True)\n",
        "\n",
        "def get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n",
        "    return next(parameter.parameters()).device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvGuCtAt20f4"
      },
      "outputs": [],
      "source": [
        "def ForCausalLMLoss(\n",
        "    logits,\n",
        "    labels,\n",
        "    vocab_size: int,\n",
        "    num_items_in_batch: Optional[torch.Tensor] = None,\n",
        "    ignore_index: int = -100,\n",
        "    shift_labels: Optional[torch.Tensor] = None,\n",
        "    **kwargs,\n",
        ") -> torch.Tensor:\n",
        "    logits = logits.float()\n",
        "\n",
        "    if shift_labels is None:\n",
        "        labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    shift_labels = shift_labels.view(-1)\n",
        "    shift_labels = shift_labels.to(logits.device)\n",
        "    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n",
        "    return loss\n",
        "\n",
        "def ForMaskedLMLoss(\n",
        "    logits: torch.Tensor,\n",
        "    labels: torch.Tensor,\n",
        "    vocab_size: int,\n",
        "    num_items_in_batch: Optional[torch.Tensor] = None,\n",
        "    ignore_index: int = -100,\n",
        "    **kwargs,\n",
        "):\n",
        "    logits = logits.float()\n",
        "\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    labels = labels.view(-1)\n",
        "\n",
        "    labels = labels.to(logits.device)\n",
        "    loss = fixed_cross_entropy(logits, labels, num_items_in_batch, ignore_index, **kwargs)\n",
        "    return loss\n",
        "\n",
        "LOSS_MAPPING = {\n",
        "    \"ForCausalLM\": ForCausalLMLoss,\n",
        "    \"ForMaskedLM\": ForMaskedLMLoss,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8J6-Q65yDkx"
      },
      "outputs": [],
      "source": [
        "class PreTrainedModel(nn.Module):\n",
        "    _auto_class = None\n",
        "    _keep_in_fp32_modules = None\n",
        "    _keep_in_fp32_modules_strict = None\n",
        "    _keys_to_ignore_on_load_missing = None\n",
        "    _keys_to_ignore_on_load_unexpected = None\n",
        "    main_input_name = \"input_ids\"\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return get_parameter_device(self)\n",
        "\n",
        "    def _check_attn_implementation(cls, attn_implementation: Union[dict, str]) -> Union[dict, str]:\n",
        "        if isinstance(attn_implementation, str) and re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", attn_implementation):\n",
        "            repo_id, kernel_name = attn_implementation.split(\":\")\n",
        "            kernel_name = kernel_name.strip()\n",
        "            repo_id = repo_id.strip()\n",
        "\n",
        "            kernel = get_kernel(repo_id)\n",
        "            AttentionInterface.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n",
        "            attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n",
        "        return attn_implementation\n",
        "\n",
        "    def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n",
        "        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n",
        "\n",
        "        for key in self.config.sub_configs.keys():\n",
        "            sub_config = getattr(self.config, key)\n",
        "            curr_attn_implementation = (\n",
        "                requested_attn_implementation\n",
        "                if not isinstance(requested_attn_implementation, dict)\n",
        "                else requested_attn_implementation.get(key, None)\n",
        "            )\n",
        "            if (\n",
        "                sub_config is not None\n",
        "                and sub_config._attn_implementation_internal is None\n",
        "                and curr_attn_implementation is not None\n",
        "            ):\n",
        "                sub_config._attn_implementation_internal = curr_attn_implementation\n",
        "\n",
        "        if requested_attn_implementation == \"flash_attention_3\" and self._flash_attn_3_can_dispatch():\n",
        "            self.config._attn_implementation = \"flash_attention_3\"\n",
        "        if requested_attn_implementation == \"flash_attention_2\" and self._flash_attn_2_can_dispatch():\n",
        "            self.config._attn_implementation = \"flash_attention_2\"\n",
        "        elif requested_attn_implementation == \"flex_attention\" and self._flex_attn_can_dispatch():\n",
        "            self.config._attn_implementation = \"flex_attention\"\n",
        "        elif (\n",
        "            requested_attn_implementation in [None, \"sdpa\"]\n",
        "            and not is_torch_xla_available()\n",
        "            and self._sdpa_can_dispatch(hard_check_only=requested_attn_implementation is not None)\n",
        "        ):\n",
        "            self.config._attn_implementation = \"sdpa\"\n",
        "        elif requested_attn_implementation in AttentionInterface.valid_keys():\n",
        "            self.config._attn_implementation = requested_attn_implementation\n",
        "        elif isinstance(requested_attn_implementation, dict):\n",
        "            self.config._attn_implementation = requested_attn_implementation.get(\"\", None)\n",
        "        else:\n",
        "            self.config._attn_implementation = \"eager\"\n",
        "\n",
        "        self.config._attn_implementation_autoset = True\n",
        "\n",
        "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        if hasattr(config, \"_attn_implementation_internal\") and not getattr(\n",
        "            config, \"_attn_implementation_autoset\", False\n",
        "        ):\n",
        "            self.set_attention_implementation(self.config._attn_implementation_internal)\n",
        "\n",
        "        loss_type = self.__class__.__name__\n",
        "        if loss_type not in LOSS_MAPPING:\n",
        "            loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n",
        "            loss_type = re.findall(loss_groups, self.__class__.__name__)\n",
        "            if len(loss_type) > 0:\n",
        "                loss_type = loss_type[0]\n",
        "            else:\n",
        "                loss_type = None\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        self.name_or_path = config.name_or_path\n",
        "        self.warnings_issued = {}\n",
        "        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n",
        "        self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n",
        "        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n",
        "\n",
        "        self._no_split_modules = self._no_split_modules or []\n",
        "        _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs\n",
        "\n",
        "    @classmethod\n",
        "    @restore_default_torch_dtype\n",
        "    def from_pretrained(\n",
        "        cls: type[SpecificPreTrainedModelType],\n",
        "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
        "        *model_args,\n",
        "        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        ignore_mismatched_sizes: bool = False,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        use_safetensors: Optional[bool] = None,\n",
        "        weights_only: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> SpecificPreTrainedModelType:\n",
        "        state_dict = kwargs.pop(\"state_dict\", None)\n",
        "        from_tf = kwargs.pop(\"from_tf\", False)\n",
        "        from_flax = kwargs.pop(\"from_flax\", False)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
        "        device_map = kwargs.pop(\"device_map\", None)\n",
        "        offload_folder = kwargs.pop(\"offload_folder\", None)\n",
        "        offload_state_dict = kwargs.pop(\"offload_state_dict\", False)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "        variant = kwargs.pop(\"variant\", None)\n",
        "        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n",
        "        gguf_file = kwargs.pop(\"gguf_file\", None)\n",
        "        tp_size = kwargs.pop(\"tp_size\", None)\n",
        "        device_mesh = kwargs.pop(\"device_mesh\", None)\n",
        "        key_mapping = kwargs.pop(\"key_mapping\", None)\n",
        "        if key_mapping is None and any(\n",
        "            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n",
        "        ):\n",
        "            key_mapping = cls._checkpoint_conversion_mapping\n",
        "\n",
        "        _ = kwargs.pop(\"resume_download\", None)\n",
        "        _ = kwargs.pop(\"mirror\", None)\n",
        "        _ = kwargs.pop(\"_fast_init\", True)\n",
        "        _ = kwargs.pop(\"low_cpu_mem_usage\", None)\n",
        "\n",
        "        if commit_hash is None:\n",
        "            commit_hash = getattr(config, \"_commit_hash\", None)\n",
        "\n",
        "        if is_peft_available():\n",
        "            _adapter_model_path = adapter_kwargs.pop(\"_adapter_model_path\", None)\n",
        "\n",
        "            if _adapter_model_path is None:\n",
        "                _adapter_model_path = find_adapter_config_file(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    cache_dir=cache_dir,\n",
        "                    force_download=force_download,\n",
        "                    proxies=proxies,\n",
        "                    local_files_only=local_files_only,\n",
        "                    _commit_hash=commit_hash,\n",
        "                    **adapter_kwargs,\n",
        "                )\n",
        "            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):\n",
        "                with open(_adapter_model_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    _adapter_model_path = pretrained_model_name_or_path\n",
        "                    pretrained_model_name_or_path = json.load(f)[\"base_model_name_or_path\"]\n",
        "\n",
        "        if device_map is None and not is_deepspeed_zero3_enabled():\n",
        "            device_in_context = get_torch_context_manager_or_global_device()\n",
        "\n",
        "            device_map = device_in_context\n",
        "\n",
        "        from_pt = not (from_tf | from_flax)\n",
        "\n",
        "        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n",
        "\n",
        "        config = copy.deepcopy(config)\n",
        "\n",
        "        kwarg_attn_imp = kwargs.pop(\"attn_implementation\", None)\n",
        "        if kwarg_attn_imp is not None:\n",
        "            config._attn_implementation = kwarg_attn_imp\n",
        "\n",
        "        model_kwargs = kwargs\n",
        "\n",
        "\n",
        "        transformers_explicit_filename = getattr(config, \"transformers_weights\", None)\n",
        "\n",
        "        hf_quantizer = None\n",
        "\n",
        "        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n",
        "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "            subfolder=subfolder,\n",
        "            variant=variant,\n",
        "            gguf_file=gguf_file,\n",
        "            from_tf=from_tf,\n",
        "            from_flax=from_flax,\n",
        "            use_safetensors=use_safetensors,\n",
        "            cache_dir=cache_dir,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            local_files_only=local_files_only,\n",
        "            token=token,\n",
        "            user_agent=user_agent,\n",
        "            revision=revision,\n",
        "            commit_hash=commit_hash,\n",
        "            is_remote_code=cls._auto_class is not None,\n",
        "            transformers_explicit_filename=transformers_explicit_filename,\n",
        "        )\n",
        "\n",
        "        is_sharded = sharded_metadata is not None\n",
        "        is_quantized = hf_quantizer is not None\n",
        "        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None\n",
        "\n",
        "        if (\n",
        "            is_safetensors_available()\n",
        "            and is_from_file\n",
        "            and not is_sharded\n",
        "            and checkpoint_files[0].endswith(\".safetensors\")\n",
        "        ):\n",
        "            with safe_open(checkpoint_files[0], framework=\"pt\") as f:\n",
        "                metadata = f.metadata()\n",
        "\n",
        "        from_pt = not (from_tf | from_flax)\n",
        "        if from_pt:\n",
        "            config, torch_dtype, dtype_orig = _get_torch_dtype(\n",
        "                cls, torch_dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n",
        "        )\n",
        "\n",
        "        config.name_or_path = pretrained_model_name_or_path\n",
        "\n",
        "        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n",
        "\n",
        "        config = copy.deepcopy(config)\n",
        "        with ContextManagers(model_init_context):\n",
        "            model = cls(config, *model_args, **model_kwargs)\n",
        "\n",
        "        model.tie_weights()\n",
        "\n",
        "        config = model.config\n",
        "\n",
        "        keep_in_fp32_regex = None\n",
        "\n",
        "        if from_pt:\n",
        "            if dtype_orig is not None:\n",
        "                torch.set_default_dtype(dtype_orig)\n",
        "\n",
        "            (\n",
        "                model,\n",
        "                missing_keys,\n",
        "                unexpected_keys,\n",
        "                mismatched_keys,\n",
        "                offload_index,\n",
        "                error_msgs,\n",
        "            ) = cls._load_pretrained_model(\n",
        "                model,\n",
        "                state_dict,\n",
        "                checkpoint_files,\n",
        "                pretrained_model_name_or_path,\n",
        "                ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
        "                sharded_metadata=sharded_metadata,\n",
        "                device_map=device_map,\n",
        "                disk_offload_folder=offload_folder,\n",
        "                offload_state_dict=offload_state_dict,\n",
        "                dtype=torch_dtype,\n",
        "                hf_quantizer=hf_quantizer,\n",
        "                keep_in_fp32_regex=keep_in_fp32_regex,\n",
        "                device_mesh=device_mesh,\n",
        "                key_mapping=key_mapping,\n",
        "                weights_only=weights_only,\n",
        "            )\n",
        "\n",
        "        model._tp_size = tp_size\n",
        "        model._device_mesh = device_mesh\n",
        "\n",
        "        model.tie_weights()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        if model.can_generate() and pretrained_model_name_or_path is not None:\n",
        "            repo_loading_kwargs = {\n",
        "                \"cache_dir\": cache_dir,\n",
        "                \"force_download\": force_download,\n",
        "                \"proxies\": proxies,\n",
        "                \"local_files_only\": local_files_only,\n",
        "                \"token\": token,\n",
        "                \"revision\": revision,\n",
        "                \"subfolder\": subfolder,\n",
        "                **kwargs,\n",
        "            }\n",
        "\n",
        "            model.generation_config = GenerationConfig.from_pretrained(\n",
        "                pretrained_model_name_or_path,\n",
        "                    _from_auto=from_auto_class,\n",
        "                    _from_pipeline=from_pipeline,\n",
        "                    **repo_loading_kwargs,\n",
        "                )\n",
        "\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n",
        "\n",
        "        dtype_orig = torch.get_default_dtype()\n",
        "        torch.set_default_dtype(dtype)\n",
        "        return dtype_orig\n",
        "\n",
        "    @classmethod\n",
        "    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n",
        "        if is_deepspeed_zero3_enabled():\n",
        "            import deepspeed\n",
        "\n",
        "            init_contexts = [no_init_weights()]\n",
        "            if not is_quantized and not _is_ds_init_called:\n",
        "                init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n",
        "            elif is_quantized:\n",
        "                init_contexts.extend([init_empty_weights(), set_quantized_state()])\n",
        "        else:\n",
        "            init_contexts = [no_init_weights(), init_empty_weights()]\n",
        "\n",
        "        return init_contexts\n",
        "\n",
        "    def _sdpa_can_dispatch(self, hard_check_only: bool = False) -> bool:\n",
        "        if (\n",
        "            torch.version.hip is not None\n",
        "            and torch.cuda.device_count() > 1\n",
        "            and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n",
        "        ):\n",
        "            torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n",
        "        if not is_torch_sdpa_available() or not self._supports_sdpa or _is_bettertransformer:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    @classmethod\n",
        "    def can_generate(cls) -> bool:\n",
        "        if \"GenerationMixin\" in str(cls.__bases__):\n",
        "            return True\n",
        "        for base in cls.__bases__:\n",
        "            if not hasattr(base, \"can_generate\"):\n",
        "                continue\n",
        "            if \"PreTrainedModel\" not in str(base) and base.can_generate():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def post_init(self):\n",
        "        self.init_weights()\n",
        "        self._backward_compatibility_gradient_checkpointing()\n",
        "\n",
        "        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n",
        "            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n",
        "            unique_module_names = set()\n",
        "            for param in all_parameters:\n",
        "                unique_module_names.update(\n",
        "                    [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n",
        "                )\n",
        "\n",
        "        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n",
        "        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n",
        "        for name, module in self.named_children():\n",
        "            if plan := getattr(module, \"_tp_plan\", None):\n",
        "                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        pass\n",
        "\n",
        "    def init_weights(self):\n",
        "        if self.config.pruned_heads:\n",
        "            self.prune_heads(self.config.pruned_heads)\n",
        "\n",
        "        if _init_weights:\n",
        "            self.initialize_weights()\n",
        "\n",
        "            self.tie_weights()\n",
        "\n",
        "    def _backward_compatibility_gradient_checkpointing(self):\n",
        "        if self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\n",
        "            self.gradient_checkpointing_enable()\n",
        "            delattr(self.config, \"gradient_checkpointing\")\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n",
        "            output_embeddings = self.get_output_embeddings()\n",
        "            if output_embeddings is not None:\n",
        "                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
        "\n",
        "        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n",
        "            if hasattr(self, self.base_model_prefix):\n",
        "                self = getattr(self, self.base_model_prefix)\n",
        "            tied_weights = self._tie_encoder_decoder_weights(\n",
        "                self.encoder, self.decoder, self.base_model_prefix, \"encoder\"\n",
        "            )\n",
        "            self._dynamic_tied_weights_keys = tied_weights\n",
        "\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, \"_tie_weights\"):\n",
        "                module._tie_weights()\n",
        "\n",
        "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
        "        if self.config.torchscript:\n",
        "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
        "        else:\n",
        "            output_embeddings.weight = input_embeddings.weight\n",
        "\n",
        "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
        "            output_embeddings.bias.data = nn.functional.pad(\n",
        "                output_embeddings.bias.data,\n",
        "                (\n",
        "                    0,\n",
        "                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
        "                ),\n",
        "                \"constant\",\n",
        "                0,\n",
        "            )\n",
        "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
        "            output_embeddings.out_features = input_embeddings.num_embeddings\n",
        "\n",
        "    @classmethod\n",
        "    def _load_pretrained_model(\n",
        "        cls,\n",
        "        model: \"PreTrainedModel\",\n",
        "        state_dict: Optional[dict],\n",
        "        checkpoint_files: Optional[list[str]],\n",
        "        pretrained_model_name_or_path: Optional[str],\n",
        "        ignore_mismatched_sizes: bool = False,\n",
        "        sharded_metadata: Optional[dict] = None,\n",
        "        device_map: Optional[dict] = None,\n",
        "        disk_offload_folder: Optional[str] = None,\n",
        "        offload_state_dict: Optional[bool] = None,\n",
        "        dtype: Optional[torch.dtype] = None,\n",
        "        hf_quantizer: Optional[HfQuantizer] = None,\n",
        "        keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "        device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        weights_only: bool = True,\n",
        "    ):\n",
        "        is_quantized = hf_quantizer is not None\n",
        "        is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "            QuantizationMethod.HQQ,\n",
        "            QuantizationMethod.QUARK,\n",
        "        }\n",
        "        is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "            QuantizationMethod.HQQ,\n",
        "            QuantizationMethod.BITS_AND_BYTES,\n",
        "        }\n",
        "\n",
        "        if sharded_metadata is not None:\n",
        "            original_checkpoint_keys = sharded_metadata[\"all_checkpoint_keys\"]\n",
        "        elif state_dict is not None:\n",
        "            original_checkpoint_keys = list(state_dict.keys())\n",
        "        else:\n",
        "            original_checkpoint_keys = list(\n",
        "                load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
        "            )\n",
        "\n",
        "        prefix = model.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "        has_prefix_module = any(s.startswith(prefix) for s in original_checkpoint_keys) if len(prefix) > 0 else False\n",
        "        expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n",
        "        loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n",
        "        loading_base_model_from_task_state_dict = has_prefix_module and not expects_prefix_module\n",
        "\n",
        "        key_renaming_mapping = model._get_key_renaming_mapping(\n",
        "            original_checkpoint_keys,\n",
        "            key_mapping,\n",
        "            loading_base_model_from_task_state_dict,\n",
        "            loading_task_model_from_base_state_dict,\n",
        "        )\n",
        "        checkpoint_keys = list(key_renaming_mapping.values())\n",
        "\n",
        "        missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(\n",
        "            cls,\n",
        "            model,\n",
        "            original_checkpoint_keys,\n",
        "            checkpoint_keys,\n",
        "            loading_base_model_from_task_state_dict,\n",
        "            hf_quantizer,\n",
        "            device_map,\n",
        "        )\n",
        "        mismatched_keys, mismatched_shapes = _find_mismatched_keys(\n",
        "            model,\n",
        "            state_dict,\n",
        "            checkpoint_files,\n",
        "            ignore_mismatched_sizes,\n",
        "            key_renaming_mapping,\n",
        "            is_quantized,\n",
        "            weights_only,\n",
        "        )\n",
        "\n",
        "        key_renaming_mapping = {k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys}\n",
        "        checkpoint_keys = list(key_renaming_mapping.values())\n",
        "\n",
        "        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, unexpected_keys, dtype, hf_quantizer)\n",
        "\n",
        "        model._initialize_missing_keys(checkpoint_keys, ignore_mismatched_sizes, is_quantized)\n",
        "\n",
        "        if keep_in_fp32_regex is not None:\n",
        "            for name, param in model.named_parameters():\n",
        "                if keep_in_fp32_regex.search(name):\n",
        "                    param.data = param.data.to(torch.float32)\n",
        "\n",
        "        model_to_load = model\n",
        "        if loading_task_model_from_base_state_dict:\n",
        "            model_to_load = getattr(model, prefix)\n",
        "            key_renaming_mapping = {k: v[len(_prefix) :] for k, v in key_renaming_mapping.items()}\n",
        "            checkpoint_keys = list(key_renaming_mapping.values())\n",
        "            if device_map is not None:\n",
        "                device_map = {k[len(_prefix) :] if k.startswith(_prefix) else k: v for k, v in device_map.items()}\n",
        "            task_specific_expected_keys = [s for s in model.state_dict().keys() if not s.startswith(_prefix)]\n",
        "            base_model_expected_keys = list(model_to_load.state_dict().keys())\n",
        "        reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n",
        "\n",
        "        is_offloaded_safetensors = False\n",
        "        disk_offload_index = None\n",
        "        disk_only_shard_files = []\n",
        "        if device_map is not None and \"disk\" in device_map.values():\n",
        "            if offload_state_dict is None:\n",
        "                offload_state_dict = True\n",
        "            if disk_offload_folder is not None:\n",
        "                os.makedirs(disk_offload_folder, exist_ok=True)\n",
        "            is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n",
        "            if is_offloaded_safetensors:\n",
        "                param_device_map = expand_device_map(device_map, checkpoint_keys)\n",
        "                str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n",
        "                if sharded_metadata is None:\n",
        "                    weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n",
        "                else:\n",
        "                    folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n",
        "                    weight_map = {\n",
        "                        key_renaming_mapping[k]: v\n",
        "                        for k, v in sharded_metadata[\"weight_map\"].items()\n",
        "                        if k in key_renaming_mapping\n",
        "                    }\n",
        "                    weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n",
        "                    disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n",
        "                disk_offload_index = {\n",
        "                    name: {\n",
        "                        \"safetensors_file\": file,\n",
        "                        \"weight_name\": reverse_key_renaming_mapping[name],\n",
        "                        \"dtype\": str_dtype,\n",
        "                    }\n",
        "                    for name, file in weight_map.items()\n",
        "                    if param_device_map[name] == \"disk\"\n",
        "                }\n",
        "            else:\n",
        "                disk_offload_index = {}\n",
        "\n",
        "        cpu_offload_folder = None\n",
        "        cpu_offload_index = None\n",
        "        if offload_state_dict:\n",
        "            cpu_offload_folder = tempfile.mkdtemp()\n",
        "            cpu_offload_index = {}\n",
        "\n",
        "        elif state_dict is not None:\n",
        "            checkpoint_files = [\"\"]\n",
        "\n",
        "        expected_keys = list(model_to_load.state_dict().keys())\n",
        "        if hf_quantizer is not None:\n",
        "            expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n",
        "\n",
        "        if device_map is not None and not is_hqq_or_quark:\n",
        "            expanded_device_map = expand_device_map(device_map, expected_keys)\n",
        "            caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)\n",
        "\n",
        "        args_list = [\n",
        "            (\n",
        "                shard_file,\n",
        "                state_dict,\n",
        "                disk_only_shard_files,\n",
        "                is_hqq_or_bnb,\n",
        "                is_quantized,\n",
        "                device_map,\n",
        "                hf_quantizer,\n",
        "                key_renaming_mapping,\n",
        "                weights_only,\n",
        "                model_to_load,\n",
        "                expected_keys,\n",
        "                reverse_key_renaming_mapping,\n",
        "                disk_offload_folder,\n",
        "                disk_offload_index,\n",
        "                cpu_offload_folder,\n",
        "                cpu_offload_index,\n",
        "                is_offloaded_safetensors,\n",
        "                keep_in_fp32_regex,\n",
        "                unexpected_keys,\n",
        "                device_mesh,\n",
        "            )\n",
        "            for shard_file in checkpoint_files\n",
        "        ]\n",
        "\n",
        "        error_msgs = []\n",
        "\n",
        "        if (\n",
        "            os.environ.get(\"HF_ENABLE_PARALLEL_LOADING\", \"\").upper() in ENV_VARS_TRUE_VALUES\n",
        "            and not is_deepspeed_zero3_enabled()\n",
        "        ):\n",
        "            _error_msgs, disk_offload_index, cpu_offload_index = load_shard_files_with_threadpool(args_list)\n",
        "            error_msgs += _error_msgs\n",
        "        else:\n",
        "            for args in args_list:\n",
        "                _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n",
        "                error_msgs += _error_msgs\n",
        "\n",
        "        if disk_offload_index is not None and len(disk_offload_index) > 0:\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                prefix = cls.base_model_prefix\n",
        "                if not is_offloaded_safetensors:\n",
        "                    for weight_name in disk_offload_index:\n",
        "                        shutil.move(\n",
        "                            os.path.join(disk_offload_folder, f\"{weight_name}.dat\"),\n",
        "                            os.path.join(disk_offload_folder, f\"{prefix}.{weight_name}.dat\"),\n",
        "                        )\n",
        "                disk_offload_index = {f\"{prefix}.{key}\": value for key, value in disk_offload_index.items()}\n",
        "            if not is_offloaded_safetensors:\n",
        "                save_offload_index(disk_offload_index, disk_offload_folder)\n",
        "                disk_offload_index = None\n",
        "        if offload_state_dict:\n",
        "            load_offloaded_weights(model_to_load, cpu_offload_index, cpu_offload_folder)\n",
        "            shutil.rmtree(cpu_offload_folder)\n",
        "\n",
        "        if hf_quantizer is not None:\n",
        "            missing_keys = hf_quantizer.update_missing_keys_after_loading(model_to_load, missing_keys, prefix)\n",
        "\n",
        "        if device_mesh is not None:\n",
        "            tp_device = list(device_map.values())[0]\n",
        "            for buffer in model.buffers():\n",
        "                if buffer.device != tp_device:\n",
        "                    buffer.data = buffer.to(tp_device)\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                parameters_to_initialize = {\n",
        "                    name: param for name, param in model.named_parameters() if not name.startswith(prefix)\n",
        "                }\n",
        "                for name, param in parameters_to_initialize.items():\n",
        "                    if param.device.type == \"meta\":\n",
        "                        continue\n",
        "                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param, keep_in_fp32_regex)\n",
        "                    shard_and_distribute_module(\n",
        "                        model,\n",
        "                        param.to(tp_device),\n",
        "                        param,\n",
        "                        name,\n",
        "                        casting_dtype,\n",
        "                        to_contiguous,\n",
        "                        device_mesh.get_local_rank(),\n",
        "                        device_mesh,\n",
        "                    )\n",
        "\n",
        "        if len(error_msgs) > 0:\n",
        "            error_msg = \"\\n\\t\".join(error_msgs)\n",
        "            if \"size mismatch\" in error_msg:\n",
        "                error_msg += (\n",
        "                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n",
        "                )\n",
        "            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n",
        "        if len(unexpected_keys) > 0:\n",
        "            archs = [] if model.config.architectures is None else model.config.architectures\n",
        "        if len(mismatched_keys) > 0:\n",
        "            mismatched_warning = \"\\n\".join(\n",
        "                [\n",
        "                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n",
        "                    for key, (shape1, shape2) in zip(mismatched_keys, mismatched_shapes)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n",
        "\n",
        "    def _get_key_renaming_mapping(\n",
        "        self,\n",
        "        checkpoint_keys: list[str],\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        loading_base_model_from_task_state_dict: bool = False,\n",
        "        loading_task_model_from_base_state_dict: bool = False,\n",
        "    ):\n",
        "        prefix = self.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "\n",
        "        renamed_keys = {}\n",
        "        key_renaming_mapping = {}\n",
        "        for key in checkpoint_keys:\n",
        "            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n",
        "\n",
        "            if key_mapping is not None:\n",
        "                for pattern, replacement in key_mapping.items():\n",
        "                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n",
        "                    if n_replace > 0:\n",
        "                        has_changed = True\n",
        "                        break\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                new_key = \".\".join([prefix, new_key])\n",
        "            elif loading_base_model_from_task_state_dict:\n",
        "                if not new_key.startswith(_prefix):\n",
        "                    continue\n",
        "                new_key = new_key[len(_prefix) :]\n",
        "\n",
        "            key_renaming_mapping[key] = new_key\n",
        "\n",
        "            if has_changed:\n",
        "                if key.endswith(\"LayerNorm.gamma\"):\n",
        "                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n",
        "                elif key.endswith(\"LayerNorm.beta\"):\n",
        "                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n",
        "\n",
        "        if renamed_keys:\n",
        "            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n",
        "            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n",
        "            for old_key, new_key in renamed_keys.values():\n",
        "                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n",
        "            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n",
        "\n",
        "        return key_renaming_mapping\n",
        "\n",
        "    @staticmethod\n",
        "    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:\n",
        "        if key.endswith(\"LayerNorm.beta\"):\n",
        "            return key.replace(\"LayerNorm.beta\", \"LayerNorm.bias\"), True\n",
        "        if key.endswith(\"LayerNorm.gamma\"):\n",
        "            return key.replace(\"LayerNorm.gamma\", \"LayerNorm.weight\"), True\n",
        "\n",
        "        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n",
        "            if key.endswith(\"weight_g\"):\n",
        "                return key.replace(\"weight_g\", \"parametrizations.weight.original0\"), True\n",
        "            if key.endswith(\"weight_v\"):\n",
        "                return key.replace(\"weight_v\", \"parametrizations.weight.original1\"), True\n",
        "        else:\n",
        "            if key.endswith(\"parametrizations.weight.original0\"):\n",
        "                return key.replace(\"parametrizations.weight.original0\", \"weight_g\"), True\n",
        "            if key.endswith(\"parametrizations.weight.original1\"):\n",
        "                return key.replace(\"parametrizations.weight.original1\", \"weight_v\"), True\n",
        "\n",
        "        return key, False\n",
        "\n",
        "    def _move_missing_keys_from_meta_to_cpu(\n",
        "        self,\n",
        "        missing_keys: list[str],\n",
        "        unexpected_keys: list[str],\n",
        "        dtype: Optional[torch.dtype],\n",
        "        hf_quantizer: Optional[HfQuantizer],\n",
        "    ) -> \"PreTrainedModel\":\n",
        "        is_quantized = hf_quantizer is not None\n",
        "\n",
        "        if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n",
        "            for key, param in self.named_parameters():\n",
        "                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n",
        "                _load_parameter_into_model(self, key, value)\n",
        "            return\n",
        "\n",
        "        model_state_dict = self.state_dict()\n",
        "        for key in missing_keys:\n",
        "            param = model_state_dict[key]\n",
        "            if param.device == torch.device(\"meta\"):\n",
        "                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n",
        "                if (\n",
        "                    not is_quantized\n",
        "                    or (getattr(hf_quantizer, \"requires_parameters_quantization\", False))\n",
        "                    or not hf_quantizer.check_quantized_param(self, param_value=value, param_name=key, state_dict={})\n",
        "                ):\n",
        "                    _load_parameter_into_model(self, key, value)\n",
        "                else:\n",
        "                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\", model_state_dict, unexpected_keys)\n",
        "\n",
        "    def _initialize_missing_keys(\n",
        "        self,\n",
        "        loaded_keys: list[str],\n",
        "        ignore_mismatched_sizes: bool,\n",
        "        is_quantized: bool,\n",
        "    ) -> \"PreTrainedModel\":\n",
        "        if not ignore_mismatched_sizes:\n",
        "            not_initialized_submodules = set_initialized_submodules(self, loaded_keys)\n",
        "            if (\n",
        "                hasattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\")\n",
        "                and self.config.get_text_config(decoder=True).tie_word_embeddings\n",
        "            ):\n",
        "                output_embeddings = self.get_output_embeddings()\n",
        "                if output_embeddings is not None:\n",
        "                    if not hasattr(output_embeddings, \"bias\") or output_embeddings.bias is None:\n",
        "                        output_embeddings._is_hf_initialized = True\n",
        "        else:\n",
        "            not_initialized_submodules = dict(self.named_modules())\n",
        "        if is_deepspeed_zero3_enabled() and not is_quantized:\n",
        "            import deepspeed\n",
        "\n",
        "            not_initialized_parameters = list(\n",
        "                set(\n",
        "                    itertools.chain.from_iterable(\n",
        "                        submodule.parameters(recurse=False) for submodule in not_initialized_submodules.values()\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n",
        "                self.initialize_weights()\n",
        "        else:\n",
        "            self.initialize_weights()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def initialize_weights(self):\n",
        "        if not hasattr(torch.nn.Module, \"smart_apply\"):\n",
        "            def smart_apply(self, fn):\n",
        "                for module in self.children():\n",
        "                    if isinstance(module, PreTrainedModel):\n",
        "                        module.smart_apply(module._initialize_weights)\n",
        "                    else:\n",
        "                        module.smart_apply(fn)\n",
        "                fn(self)\n",
        "                return self\n",
        "\n",
        "            torch.nn.Module.smart_apply = smart_apply\n",
        "\n",
        "        self.smart_apply(self._initialize_weights)\n",
        "\n",
        "    def _initialize_weights(self, module):\n",
        "        if getattr(module, \"_is_hf_initialized\", False):\n",
        "            return\n",
        "        self._init_weights(module)\n",
        "        module._is_hf_initialized = True\n",
        "\n",
        "    def get_parameter_or_buffer(self, target: str):\n",
        "        return self.get_parameter(target)\n",
        "        return self.get_buffer(target)\n",
        "        module, param_name = get_module_from_name(self, target)\n",
        "        if (\n",
        "            param_name == \"_extra_state\"\n",
        "            and getattr(module.__class__, \"get_extra_state\", torch.nn.Module.get_extra_state)\n",
        "            is not torch.nn.Module.get_extra_state\n",
        "        ):\n",
        "            return module.get_extra_state()\n",
        "\n",
        "        raise AttributeError(f\"`{target}` is neither a parameter, buffer, nor extra state.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA_1qOCvkv_B"
      },
      "source": [
        "## 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgAPBwBd0dVx"
      },
      "outputs": [],
      "source": [
        "\n",
        "CONFIG_MAPPING_NAMES = OrderedDict[str, str]([])\n",
        "\n",
        "TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n",
        "\n",
        "_T = TypeVar(\"_T\")\n",
        "_LazyAutoMappingValue = tuple[Union[type[Any], None], Union[type[Any], None]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBE3rOCFvo_V"
      },
      "outputs": [],
      "source": [
        "\n",
        "_has_opentelemetry = True\n",
        "\n",
        "DEPRECATED_MODELS = [\n",
        "    \"bort\",\n",
        "    \"deta\",\n",
        "    \"efficientformer\",\n",
        "    \"ernie_m\",\n",
        "    \"gptsan_japanese\",\n",
        "    \"graphormer\",\n",
        "    \"jukebox\",\n",
        "    \"mctct\",\n",
        "    \"mega\",\n",
        "    \"mmbt\",\n",
        "    \"nat\",\n",
        "    \"nezha\",\n",
        "    \"open_llama\",\n",
        "    \"qdqbert\",\n",
        "    \"realm\",\n",
        "    \"retribert\",\n",
        "    \"speech_to_text_2\",\n",
        "    \"tapex\",\n",
        "    \"trajectory_transformer\",\n",
        "    \"transfo_xl\",\n",
        "    \"tvlt\",\n",
        "    \"van\",\n",
        "    \"vit_hybrid\",\n",
        "    \"xlm_prophetnet\",\n",
        "]\n",
        "\n",
        "SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict[str, str](\n",
        "    [\n",
        "        (\"openai-gpt\", \"openai\"),\n",
        "        (\"data2vec-audio\", \"data2vec\"),\n",
        "        (\"data2vec-text\", \"data2vec\"),\n",
        "        (\"data2vec-vision\", \"data2vec\"),\n",
        "        (\"donut-swin\", \"donut\"),\n",
        "        (\"kosmos-2\", \"kosmos2\"),\n",
        "        (\"maskformer-swin\", \"maskformer\"),\n",
        "        (\"xclip\", \"x_clip\"),\n",
        "        (\"clip_vision_model\", \"clip\"),\n",
        "        (\"qwen2_audio_encoder\", \"qwen2_audio\"),\n",
        "        (\"clip_text_model\", \"clip\"),\n",
        "        (\"aria_text\", \"aria\"),\n",
        "        (\"gemma3_text\", \"gemma3\"),\n",
        "        (\"gemma3n_audio\", \"gemma3n\"),\n",
        "        (\"gemma3n_text\", \"gemma3n\"),\n",
        "        (\"gemma3n_vision\", \"gemma3n\"),\n",
        "        (\"glm4v_text\", \"glm4v\"),\n",
        "        (\"idefics3_vision\", \"idefics3\"),\n",
        "        (\"siglip_vision_model\", \"siglip\"),\n",
        "        (\"aimv2_vision_model\", \"aimv2\"),\n",
        "        (\"smolvlm_vision\", \"smolvlm\"),\n",
        "        (\"chinese_clip_vision_model\", \"chinese_clip\"),\n",
        "        (\"rt_detr_resnet\", \"rt_detr\"),\n",
        "        (\"granitevision\", \"llava_next\"),\n",
        "        (\"internvl_vision\", \"internvl\"),\n",
        "        (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n",
        "        (\"qwen2_vl_text\", \"qwen2_vl\"),\n",
        "        (\"sam_vision_model\", \"sam\"),\n",
        "        (\"sam_hq_vision_model\", \"sam_hq\"),\n",
        "        (\"llama4_text\", \"llama4\"),\n",
        "        (\"blip_2_qformer\", \"blip_2\"),\n",
        "        (\"fastspeech2_conformer_with_hifigan\", \"fastspeech2_conformer\"),\n",
        "        (\"perception_encoder\", \"perception_lm\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ALL_CACHE_NAMES = [\n",
        "    \"past_key_values\",\n",
        "    \"cache_params\",\n",
        "    \"state\",\n",
        "    \"mems\",\n",
        "    \"past_buckets_states\",\n",
        "]\n",
        "\n",
        "METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n",
        "\n",
        "def model_type_to_module_name(key) -> str:\n",
        "    if key in SPECIAL_MODEL_TYPE_TO_MODULE_NAME:\n",
        "        key = SPECIAL_MODEL_TYPE_TO_MODULE_NAME[key]\n",
        "\n",
        "        if key in DEPRECATED_MODELS:\n",
        "            key = f\"deprecated.{key}\"\n",
        "        return key\n",
        "\n",
        "    key = key.replace(\"-\", \"_\")\n",
        "    if key in DEPRECATED_MODELS:\n",
        "        key = f\"deprecated.{key}\"\n",
        "\n",
        "    return key"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNNz/kGgIbn//EfiVke0CdR",
      "include_colab_link": true,
      "mount_file_id": "1jchku-EAuPvmq1MgTLSy37yI2Iwe2K9p",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
