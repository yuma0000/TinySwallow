{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuma0000/TinySwallow/blob/main/%E5%AE%8C%E5%85%A8%E7%89%88_tinyswallow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "BC6s4FOmcrgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 741,
      "metadata": {
        "id": "WnudQPPV43xC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import inspect\n",
        "import importlib.util\n",
        "import re\n",
        "import json\n",
        "import huggingface_hub\n",
        "import numpy as np\n",
        "import time\n",
        "import queue\n",
        "import threading\n",
        "import functools\n",
        "import packaging\n",
        "\n",
        "from torch import nn\n",
        "from typing import Any, Union, Callable, Tuple, Optional, TypeVar, ContextManager\n",
        "from packaging import version\n",
        "from dataclasses import dataclass, fields, field, is_dataclass\n",
        "from collections.abc import Iterable, Iterator, MutableMapping\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from functools import wraps, lru_cache\n",
        "from abc import abstractmethod\n",
        "import unicodedata\n",
        "import regex as re\n",
        "from urllib.parse import urlparse\n",
        "from contextlib import ExitStack\n",
        "\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def init_on_device(device: \"torch.device\", include_buffers: bool = False):\n",
        "    if include_buffers:\n",
        "        with device:\n",
        "            yield\n",
        "        return\n",
        "\n",
        "    old_register_parameter = nn.Module.register_parameter\n",
        "    if include_buffers:\n",
        "        old_register_buffer = nn.Module.register_buffer\n",
        "\n",
        "    def register_empty_parameter(module, name, param):\n",
        "        old_register_parameter(module, name, param)\n",
        "        if param is not None:\n",
        "            param_cls = type(module._parameters[name])\n",
        "            kwargs = module._parameters[name].__dict__\n",
        "            kwargs[\"requires_grad\"] = param.requires_grad\n",
        "            module._parameters[name] = param_cls(module._parameters[name].to(device), **kwargs)\n",
        "\n",
        "    def register_empty_buffer(module, name, buffer, persistent=True):\n",
        "        old_register_buffer(module, name, buffer, persistent=persistent)\n",
        "        if buffer is not None:\n",
        "            module._buffers[name] = module._buffers[name].to(device)\n",
        "\n",
        "    if include_buffers:\n",
        "        tensor_constructors_to_patch = {\n",
        "            torch_function_name: getattr(torch, torch_function_name)\n",
        "            for torch_function_name in [\"empty\", \"zeros\", \"ones\", \"full\"]\n",
        "        }\n",
        "    else:\n",
        "        tensor_constructors_to_patch = {}\n",
        "\n",
        "    def patch_tensor_constructor(fn):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            kwargs[\"device\"] = device\n",
        "            return fn(*args, **kwargs)\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    try:\n",
        "        nn.Module.register_parameter = register_empty_parameter\n",
        "        if include_buffers:\n",
        "            nn.Module.register_buffer = register_empty_buffer\n",
        "        for torch_function_name in tensor_constructors_to_patch.keys():\n",
        "            setattr(torch, torch_function_name, patch_tensor_constructor(getattr(torch, torch_function_name)))\n",
        "        yield\n",
        "    finally:\n",
        "        nn.Module.register_parameter = old_register_parameter\n",
        "        if include_buffers:\n",
        "            nn.Module.register_buffer = old_register_buffer\n",
        "        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items():\n",
        "            setattr(torch, torch_function_name, old_torch_function)\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def init_empty_weights(include_buffers: bool = False):\n",
        "    with init_on_device(torch.device(\"meta\"), include_buffers=include_buffers) as f:\n",
        "        yield f\n",
        "\n",
        "__version__ = \"4.54.0.dev0\"\n",
        "\n",
        "def is_deepspeed_zero3_enabled():\n",
        "    if _hf_deepspeed_config_weak_ref is not None and _hf_deepspeed_config_weak_ref() is not None:\n",
        "        return _hf_deepspeed_config_weak_ref().is_zero3()\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def is_fsdp_managed_module(module: nn.Module) -> bool:\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "\n",
        "    import torch\n",
        "\n",
        "    if not torch.distributed.is_available():\n",
        "        return False\n",
        "\n",
        "    import torch.distributed.fsdp\n",
        "\n",
        "    return isinstance(module, torch.distributed.fsdp.FullyShardedDataParallel) or getattr(\n",
        "        module, \"_is_fsdp_managed_module\", False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EosTokenCriteria():\n",
        "    def __init__(self, eos_token_id: Union[int, list[int], torch.Tensor]):\n",
        "        if not isinstance(eos_token_id, torch.Tensor):\n",
        "            if isinstance(eos_token_id, int):\n",
        "                eos_token_id = [eos_token_id]\n",
        "            eos_token_id = torch.tensor(eos_token_id)\n",
        "        self.eos_token_id = eos_token_id\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        self.eos_token_id = self.eos_token_id.to(input_ids.device)\n",
        "        is_done = isin_mps_friendly(input_ids[:, -1], self.eos_token_id)\n",
        "        return is_done\n",
        "\n",
        "class MaxLengthCriteria():\n",
        "    def __init__(self, max_length: int, max_position_embeddings: Optional[int] = None):\n",
        "        self.max_length = max_length\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        cur_len = input_ids.shape[1]\n",
        "        is_done = cur_len >= self.max_length\n",
        "        return torch.full((input_ids.shape[0],), is_done, device=input_ids.device, dtype=torch.bool)\n",
        "\n",
        "class StoppingCriteriaList(list):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
        "        is_done = torch.full((input_ids.shape[0],), False, device=input_ids.device, dtype=torch.bool)\n",
        "        for criteria in self:\n",
        "            is_done = is_done | criteria(input_ids, scores, **kwargs)\n",
        "        return is_done\n",
        "\n",
        "    @property\n",
        "    def max_length(self) -> Optional[int]:\n",
        "        for stopping_criterium in self:\n",
        "            if isinstance(stopping_criterium, MaxLengthCriteria):\n",
        "                return stopping_criterium.max_length\n",
        "        return None"
      ],
      "metadata": {
        "id": "agA2kVceADvd"
      },
      "execution_count": 742,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogitsProcessorList(list):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
        "        for processor in self:\n",
        "            function_args = inspect.signature(processor.__call__).parameters\n",
        "            if len(function_args) > 2:\n",
        "                scores = processor(input_ids, scores, **kwargs)\n",
        "            else:\n",
        "                scores = processor(input_ids, scores)\n",
        "\n",
        "        return scores\n",
        "\n",
        "class RepetitionPenaltyLogitsProcessor():\n",
        "    def __init__(self, penalty: float, prompt_ignore_length: Optional[int] = None):\n",
        "        self.penalty = penalty\n",
        "        self.prompt_ignore_length = prompt_ignore_length\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        if self.prompt_ignore_length:\n",
        "            input_ids = input_ids[:, self.prompt_ignore_length :]\n",
        "\n",
        "        score = torch.gather(scores, 1, input_ids)\n",
        "\n",
        "        score = torch.where(score < 0, score * self.penalty, score / self.penalty)\n",
        "\n",
        "        scores_processed = scores.scatter(1, input_ids, score)\n",
        "        return scores_processed\n",
        "\n",
        "class TopKLogitsWarper():\n",
        "    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
        "        self.top_k = max(top_k, min_tokens_to_keep)\n",
        "        self.filter_value = filter_value\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        top_k = min(self.top_k, scores.size(-1))\n",
        "        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n",
        "        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
        "        return scores_processed\n",
        "\n",
        "class TemperatureLogitsWarper():\n",
        "    def __init__(self, temperature: float):\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        scores_processed = scores / self.temperature\n",
        "        return scores_processed\n",
        "\n",
        "class TopPLogitsWarper():\n",
        "    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
        "        top_p = float(top_p)\n",
        "        self.top_p = top_p\n",
        "        self.filter_value = filter_value\n",
        "        self.min_tokens_to_keep = min_tokens_to_keep\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        sorted_logits, sorted_indices = torch.sort(scores, descending=False)\n",
        "        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)\n",
        "        sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
        "        return scores_processed"
      ],
      "metadata": {
        "id": "tKEATT6k7DnK"
      },
      "execution_count": 743,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_tied_parameters(model: \"nn.Module\", **kwargs):\n",
        "    all_named_parameters = dict(model.named_parameters(remove_duplicate=False))\n",
        "    no_duplicate_named_parameters = dict(model.named_parameters(remove_duplicate=True))\n",
        "    tied_param_names = set(all_named_parameters.keys()) - set(no_duplicate_named_parameters.keys())\n",
        "    tied_param_groups = {}\n",
        "    for tied_param_name in tied_param_names:\n",
        "        tied_param = all_named_parameters[tied_param_name]\n",
        "        for param_name, param in no_duplicate_named_parameters.items():\n",
        "            if param is tied_param:\n",
        "                if param_name not in tied_param_groups:\n",
        "                    tied_param_groups[param_name] = []\n",
        "                tied_param_groups[param_name].append(tied_param_name)\n",
        "\n",
        "    return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]"
      ],
      "metadata": {
        "id": "qG2B9hBw6dAQ"
      },
      "execution_count": 744,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "7YHXOB4yxt0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 745,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KAkcvf3o07Z",
        "outputId": "3a26752f-98dd-4898-fc27-6f909ce6cb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected tokenizers version: 0.21.2\n"
          ]
        }
      ],
      "source": [
        "def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[tuple[bool, str], bool]:\n",
        "    package_exists = importlib.util.find_spec(pkg_name) is not None\n",
        "    package_version = \"N/A\"\n",
        "    if package_exists:\n",
        "        package_version = importlib.metadata.version(pkg_name)\n",
        "        print(f\"Detected {pkg_name} version: {package_version}\")\n",
        "    if return_version:\n",
        "        return package_exists, package_version\n",
        "    else:\n",
        "        return package_exists\n",
        "\n",
        "_tokenizers_available = _is_package_available(\"tokenizers\")\n",
        "\n",
        "def is_tokenizers_available():\n",
        "    return _tokenizers_available\n",
        "\n",
        "def is_torch_fx_proxy(x):\n",
        "    return False\n",
        "\n",
        "def _is_torch(x):\n",
        "    import torch\n",
        "\n",
        "    return isinstance(x, torch.Tensor)\n",
        "\n",
        "def is_torch_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_tf_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_flax_available():\n",
        "    return False\n",
        "\n",
        "def is_jax_tensor(x):\n",
        "    return False\n",
        "\n",
        "def is_numpy_array(x):\n",
        "    return _is_numpy(x)\n",
        "\n",
        "def is_mlx_array(x):\n",
        "    return False\n",
        "\n",
        "def _is_numpy(x):\n",
        "    return isinstance(x, np.ndarray)\n",
        "\n",
        "def infer_framework_from_repr(x):\n",
        "    representation = str(type(x))\n",
        "    if representation.startswith(\"<class 'torch.\"):\n",
        "        return \"pt\"\n",
        "    elif representation.startswith(\"<class 'tensorflow.\"):\n",
        "        return \"tf\"\n",
        "    elif representation.startswith(\"<class 'jax\"):\n",
        "        return \"jax\"\n",
        "    elif representation.startswith(\"<class 'numpy.\"):\n",
        "        return \"np\"\n",
        "    elif representation.startswith(\"<class 'mlx.\"):\n",
        "        return \"mlx\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n",
        "    if not _is_package_available(\"torch\"):\n",
        "        return False\n",
        "\n",
        "    if accept_dev:\n",
        "        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n",
        "            library_version\n",
        "        )\n",
        "    else:\n",
        "        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n",
        "\n",
        "_torch_distributed_available = torch.distributed.is_available()\n",
        "\n",
        "if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n",
        "    from torch.distributed.tensor import DTensor, Placement, Replicate, Shard"
      ],
      "metadata": {
        "id": "4s3djMBmyr8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e007974-bc5f-4b94-e47f-5638d4ee050d"
      },
      "execution_count": 746,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected torch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TensorParallelLayer:\n",
        "    use_dtensor = True\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh): ...\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh): ...\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        if self.use_dtensor:\n",
        "            distribute_module(\n",
        "                module,\n",
        "                device_mesh,\n",
        "                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n",
        "                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n",
        "            )\n",
        "\n",
        "class ColwiseParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "        use_dtensor=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Replicate(),)\n",
        "        self.output_layouts = (output_layouts or Shard(-1),)\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=False)\n",
        "        return input_tensor\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        if param_type == \"bias\":\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n",
        "            shard = [Shard(-1)]\n",
        "        else:\n",
        "            shard = [Shard(-2)]\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2)\n",
        "\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(\n",
        "                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n",
        "            )\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        if outputs.placements != output_layouts:\n",
        "            outputs = outputs.redistribute(placements=output_layouts, async_op=False)\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "class RowwiseParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "        use_dtensor=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Shard(-1),)\n",
        "        self.output_layouts = (output_layouts or Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        if param_type != \"bias\":\n",
        "            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n",
        "            shard = [Shard(-1)]\n",
        "        else:\n",
        "            shard = [Replicate()]\n",
        "            parameter = param[:]\n",
        "\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(\n",
        "                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n",
        "            )\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        if hasattr(mod, \"bias\") and mod.bias is not None:\n",
        "            mod._bias = mod.bias\n",
        "            mod.bias = None\n",
        "\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        if outputs.placements != output_layouts:\n",
        "            outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n",
        "        if hasattr(mod, \"_bias\"):\n",
        "            outputs += mod._bias\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        module._distribute_module_applied = True\n",
        "        if self.use_dtensor:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                self.desired_input_layouts: tuple[Placement, ...] = (Shard(-1),)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                self.desired_input_layouts = (Replicate(),)\n",
        "            elif isinstance(module, nn.Parameter):\n",
        "                self.desired_input_layouts = (Shard(-1),)\n",
        "            else:\n",
        "                raise NotImplementedError(\"RowwiseParallel currently only support nn.Linear and nn.Embedding!\")\n",
        "\n",
        "            distribute_module(\n",
        "                module,\n",
        "                device_mesh,\n",
        "                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n",
        "                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n",
        "            )\n",
        "\n",
        "class IsolatedParallel(TensorParallelLayer):\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh=None):\n",
        "        input_tensor = inputs[0]\n",
        "        if isinstance(input_tensor, DTensor):\n",
        "            input_tensor = input_tensor.to_local()\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh=None):\n",
        "        return outputs\n",
        "\n",
        "    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n",
        "        distribute_module(\n",
        "            module,\n",
        "            device_mesh,\n",
        "            partial(self._prepare_input_fn, None, None),\n",
        "            partial(self._prepare_output_fn, None, None),\n",
        "        )\n",
        "\n",
        "class GatherParallel(TensorParallelLayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        input_layouts: Placement | None = None,\n",
        "        output_layouts: Placement | None = None,\n",
        "        use_local_output: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (input_layouts or Replicate(),)\n",
        "        self.output_layouts = output_layouts\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        if inputs and isinstance(inputs[0], DTensor):\n",
        "            inputs = inputs[0].to_local()\n",
        "        return inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        torch.distributed.all_reduce(outputs[0], op=torch.distributed.ReduceOp.SUM, async_op=False)\n",
        "        return outputs\n",
        "\n",
        "class PackedRowwiseParallel(RowwiseParallel):\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -1)\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-1)], run_check=False)\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "class SequenceParallel(TensorParallelLayer):\n",
        "    def __init__(self, *, sequence_dim: int = 1, use_local_output: bool = False, use_dtensor=False):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (Replicate(),)\n",
        "        self.desired_input_layouts = (Shard(1),)\n",
        "        self.output_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = True\n",
        "        self.sequence_sharding = (Shard(sequence_dim),)\n",
        "        self.use_local_output = use_local_output\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "        if input_layouts != desired_input_layouts:\n",
        "            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        outputs = outputs.redistribute(\n",
        "            placements=(Replicate(),), async_op=True\n",
        "        )\n",
        "        return outputs.to_local()\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        parameter = param[...]\n",
        "        parameter = parameter.to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            parameter = parameter.contiguous()\n",
        "        if self.use_dtensor:\n",
        "            parameter = DTensor.from_local(parameter, device_mesh, [Replicate()], run_check=False)\n",
        "        return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n",
        "\n",
        "class ReplicateParallel(TensorParallelLayer):\n",
        "    def __init__(self, *, use_dtensor=True, use_local_output=True):\n",
        "        super().__init__()\n",
        "        self.input_layouts = (Replicate(),)\n",
        "        self.output_layouts = (Replicate(),)\n",
        "        self.desired_input_layouts = (Replicate(),)\n",
        "        self.use_local_output = use_local_output\n",
        "        self.use_dtensor = use_dtensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n",
        "        input_tensor = inputs[0]\n",
        "        if not isinstance(input_tensor, DTensor):\n",
        "            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n",
        "\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n",
        "        return outputs.to_local() if use_local_output else outputs\n",
        "\n",
        "    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n",
        "        param = param[...].to(param_casting_dtype)\n",
        "        if to_contiguous:\n",
        "            param = param.contiguous()\n",
        "        param = DTensor.from_local(param, device_mesh, [Replicate()], run_check=False)\n",
        "        return param"
      ],
      "metadata": {
        "id": "G-3ixmlD9DDz"
      },
      "execution_count": 747,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cache:\n",
        "    is_compileable = False\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        key_states: torch.Tensor,\n",
        "        value_states: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "        cache_kwargs: Optional[dict[str, Any]] = None,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        raise NotImplementedError(\"Make sure to implement `update` in a subclass.\")\n",
        "\n",
        "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
        "        raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n",
        "\n",
        "    def get_max_cache_shape(self) -> Optional[int]:\n",
        "        raise NotImplementedError(\"Make sure to implement `get_max_cache_shape` in a subclass.\")\n",
        "\n",
        "    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n",
        "        max_length = self.get_max_cache_shape()\n",
        "        previous_seq_length = self.get_seq_length(layer_idx)\n",
        "        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n",
        "            return max_length - new_seq_length\n",
        "        return previous_seq_length\n",
        "\n",
        "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
        "        for layer_idx in range(len(self.key_cache)):\n",
        "            if self.key_cache[layer_idx].numel():\n",
        "                device = self.key_cache[layer_idx].device\n",
        "                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
        "            if self.value_cache[layer_idx].numel():\n",
        "                device = self.value_cache[layer_idx].device\n",
        "                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n",
        "\n",
        "    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n",
        "        query_length = cache_position.shape[0]\n",
        "        past_seen_tokens = self.get_seq_length()\n",
        "        kv_length = query_length + past_seen_tokens\n",
        "        return kv_length, 0\n",
        "\n",
        "class DynamicCache(Cache):\n",
        "    def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n",
        "        super().__init__()\n",
        "        self.key_cache: list[torch.Tensor] = []\n",
        "        self.value_cache: list[torch.Tensor] = []\n",
        "\n",
        "        if _distributed_cache_data is not None:\n",
        "            for key_states, value_states in _distributed_cache_data:\n",
        "                self.key_cache.append(key_states)\n",
        "                self.value_cache.append(value_states)\n",
        "\n",
        "    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        if layer_idx < len(self):\n",
        "            return (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
        "        else:\n",
        "            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        for layer_idx in range(len(self)):\n",
        "            yield (self.key_cache[layer_idx], self.value_cache[layer_idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.key_cache)\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        key_states: torch.Tensor,\n",
        "        value_states: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "        cache_kwargs: Optional[dict[str, Any]] = None,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        if key_states is not None:\n",
        "            if len(self.key_cache) <= layer_idx:\n",
        "                for _ in range(len(self.key_cache), layer_idx):\n",
        "                    self.key_cache.append(torch.tensor([]))\n",
        "                    self.value_cache.append(torch.tensor([]))\n",
        "                self.key_cache.append(key_states)\n",
        "                self.value_cache.append(value_states)\n",
        "            elif (\n",
        "                not self.key_cache[layer_idx].numel()\n",
        "            ):\n",
        "                self.key_cache[layer_idx] = key_states\n",
        "                self.value_cache[layer_idx] = value_states\n",
        "            else:\n",
        "                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
        "                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
        "\n",
        "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
        "\n",
        "    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n",
        "        is_empty_layer = (\n",
        "            len(self.key_cache) == 0\n",
        "            or len(self.key_cache) <= layer_idx\n",
        "            or not self.key_cache[layer_idx].numel()\n",
        "        )\n",
        "        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n",
        "        return layer_seq_length\n",
        "\n",
        "    def get_max_cache_shape(self) -> Optional[int]:\n",
        "        return None\n",
        "\n",
        "    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n",
        "        legacy_cache = ()\n",
        "        for layer_idx in range(len(self)):\n",
        "            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx]),)\n",
        "        return legacy_cache\n",
        "\n",
        "    @classmethod\n",
        "    def from_legacy_cache(\n",
        "        cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor, torch.FloatTensor]]] = None\n",
        "    ) -> \"DynamicCache\":\n",
        "        cache = cls()\n",
        "        if past_key_values is not None:\n",
        "            for layer_idx in range(len(past_key_values)):\n",
        "                key_states, value_states = past_key_values[layer_idx]\n",
        "                cache.update(key_states, value_states, layer_idx)\n",
        "        return cache"
      ],
      "metadata": {
        "id": "_iD0At3RRAJ_"
      },
      "execution_count": 748,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_NAME = \"config.json\"\n",
        "SAFE_WEIGHTS_NAME = \"model.safetensors\"\n",
        "GENERATION_CONFIG_NAME = \"generation_config.json\"\n",
        "\n",
        "ENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\n",
        "_safetensors_available = _is_package_available(\"safetensors\")\n",
        "_peft_available = _is_package_available(\"peft\")\n",
        "_torch_xla_available = False\n",
        "\n",
        "SpecificPretrainedConfigType = TypeVar(\"SpecificPretrainedConfigType\", bound=\"PretrainedConfig\")\n",
        "\n",
        "def is_safetensors_available():\n",
        "    return _safetensors_available\n",
        "\n",
        "def is_peft_available():\n",
        "    return _peft_available\n",
        "\n",
        "def is_remote_url(url_or_filename):\n",
        "    parsed = urlparse(url_or_filename)\n",
        "    return parsed.scheme in (\"http\", \"https\")\n",
        "\n",
        "def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False):\n",
        "    assert not (check_is_tpu and check_is_gpu), \"The check_is_tpu and check_is_gpu cannot both be true.\"\n",
        "\n",
        "    if not _torch_xla_available:\n",
        "        return False\n",
        "\n",
        "    import torch_xla\n",
        "\n",
        "    if check_is_gpu:\n",
        "        return torch_xla.runtime.device_type() in [\"GPU\", \"CUDA\"]\n",
        "    elif check_is_tpu:\n",
        "        return torch_xla.runtime.device_type() == \"TPU\"\n",
        "\n",
        "    return True\n",
        "\n",
        "def is_torch_sdpa_available():\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "    elif _torch_version == \"N/A\":\n",
        "        return False\n",
        "\n",
        "    if is_torch_mlu_available():\n",
        "        return True\n",
        "    if is_torch_npu_available():\n",
        "        return True\n",
        "    return version.parse(_torch_version) >= version.parse(\"2.1.1\")\n",
        "\n",
        "def deprecate_kwarg(\n",
        "    old_name: str,\n",
        "    version: str,\n",
        "    new_name: Optional[str] = None,\n",
        "    warn_if_greater_or_equal_version: bool = False,\n",
        "    raise_if_greater_or_equal_version: bool = False,\n",
        "    raise_if_both_names: bool = False,\n",
        "    additional_message: Optional[str] = None,\n",
        "):\n",
        "    deprecated_version = packaging.version.parse(version)\n",
        "    current_version = packaging.version.parse(__version__)\n",
        "    is_greater_or_equal_version = current_version >= deprecated_version\n",
        "\n",
        "    if is_greater_or_equal_version:\n",
        "        version_message = f\"and removed starting from version {version}\"\n",
        "    else:\n",
        "        version_message = f\"and will be removed in version {version}\"\n",
        "\n",
        "    def wrapper(func):\n",
        "        sig = inspect.signature(func)\n",
        "        function_named_args = set(sig.parameters.keys())\n",
        "        is_instance_method = \"self\" in function_named_args\n",
        "        is_class_method = \"cls\" in function_named_args\n",
        "\n",
        "        @wraps(func)\n",
        "        def wrapped_func(*args, **kwargs):\n",
        "            func_name = func.__name__\n",
        "            if is_instance_method:\n",
        "                func_name = f\"{args[0].__class__.__name__}.{func_name}\"\n",
        "            elif is_class_method:\n",
        "                func_name = f\"{args[0].__name__}.{func_name}\"\n",
        "\n",
        "            minimum_action = Action.NONE\n",
        "            message = None\n",
        "\n",
        "            if old_name in kwargs and new_name in kwargs:\n",
        "                minimum_action = Action.RAISE if raise_if_both_names else Action.NOTIFY_ALWAYS\n",
        "                message = f\"Both `{old_name}` and `{new_name}` are set for `{func_name}`. Using `{new_name}={kwargs[new_name]}` and ignoring deprecated `{old_name}={kwargs[old_name]}`.\"\n",
        "                kwargs.pop(old_name)\n",
        "\n",
        "            elif old_name in kwargs and new_name is not None and new_name not in kwargs:\n",
        "                minimum_action = Action.NOTIFY\n",
        "                message = f\"`{old_name}` is deprecated {version_message} for `{func_name}`. Use `{new_name}` instead.\"\n",
        "                kwargs[new_name] = kwargs.pop(old_name)\n",
        "\n",
        "            elif old_name in kwargs:\n",
        "                minimum_action = Action.NOTIFY\n",
        "                message = f\"`{old_name}` is deprecated {version_message} for `{func_name}`.\"\n",
        "\n",
        "            if message is not None and additional_message is not None:\n",
        "                message = f\"{message} {additional_message}\"\n",
        "\n",
        "            if is_greater_or_equal_version:\n",
        "                if raise_if_greater_or_equal_version and minimum_action != Action.NONE:\n",
        "                    minimum_action = Action.RAISE\n",
        "\n",
        "                elif not warn_if_greater_or_equal_version and minimum_action == Action.NOTIFY:\n",
        "                    minimum_action = Action.NONE\n",
        "\n",
        "            return func(*args, **kwargs)\n",
        "\n",
        "        return wrapped_func\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "class ContextManagers:\n",
        "    def __init__(self, context_managers: list[ContextManager]):\n",
        "        self.context_managers = context_managers\n",
        "        self.stack = ExitStack()\n",
        "\n",
        "    def __enter__(self):\n",
        "        for context_manager in self.context_managers:\n",
        "            self.stack.enter_context(context_manager)\n",
        "\n",
        "    def __exit__(self, *args, **kwargs):\n",
        "        self.stack.__exit__(*args, **kwargs)"
      ],
      "metadata": {
        "id": "Ij91P1vQZfu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92775e32-b277-40cd-ff7b-2ab3249423b7"
      },
      "execution_count": 749,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected safetensors version: 0.5.3\n",
            "Detected peft version: 0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PretrainedConfig():\n",
        "    model_type: str = \"\"\n",
        "    base_config_key: str = \"\"\n",
        "    sub_configs: dict[str, type[\"PretrainedConfig\"]] = {}\n",
        "    has_no_defaults_at_init: bool = False\n",
        "    attribute_map: dict[str, str] = {}\n",
        "    base_model_tp_plan: Optional[dict[str, Any]] = None\n",
        "    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n",
        "    _auto_class: Optional[str] = None\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        output_hidden_states: bool = False,\n",
        "        output_attentions: bool = False,\n",
        "        return_dict: bool = True,\n",
        "        torchscript: bool = False,\n",
        "        torch_dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n",
        "        pruned_heads: Optional[dict[int, list[int]]] = None,\n",
        "        tie_word_embeddings: bool = True,\n",
        "        chunk_size_feed_forward: int = 0,\n",
        "        is_encoder_decoder: bool = False,\n",
        "        is_decoder: bool = False,\n",
        "        cross_attention_hidden_size: Optional[int] = None,\n",
        "        add_cross_attention: bool = False,\n",
        "        tie_encoder_decoder: bool = False,\n",
        "        architectures: Optional[list[str]] = None,\n",
        "        finetuning_task: Optional[str] = None,\n",
        "        id2label: Optional[dict[int, str]] = None,\n",
        "        label2id: Optional[dict[str, int]] = None,\n",
        "        num_labels: Optional[int] = None,\n",
        "        task_specific_params: Optional[dict[str, Any]] = None,\n",
        "        problem_type: Optional[str] = None,\n",
        "        tokenizer_class: Optional[str] = None,\n",
        "        prefix: Optional[str] = None,\n",
        "        bos_token_id: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        sep_token_id: Optional[int] = None,\n",
        "        decoder_start_token_id: Optional[int] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        if torch_dtype is not None and isinstance(torch_dtype, str) and is_torch_available():\n",
        "            import torch\n",
        "\n",
        "            torch_dtype = getattr(torch, torch_dtype)\n",
        "\n",
        "        self.return_dict = return_dict\n",
        "        self.output_hidden_states = output_hidden_states\n",
        "        self.torchscript = torchscript\n",
        "        self.torch_dtype = torch_dtype\n",
        "        self._output_attentions = output_attentions\n",
        "\n",
        "        self.pruned_heads = pruned_heads if pruned_heads is not None else {}\n",
        "        self.tie_word_embeddings = tie_word_embeddings\n",
        "        self.chunk_size_feed_forward = chunk_size_feed_forward\n",
        "\n",
        "        self.is_encoder_decoder = is_encoder_decoder\n",
        "        self.is_decoder = is_decoder\n",
        "        self.cross_attention_hidden_size = cross_attention_hidden_size\n",
        "        self.add_cross_attention = add_cross_attention\n",
        "        self.tie_encoder_decoder = tie_encoder_decoder\n",
        "\n",
        "        self.architectures = architectures\n",
        "        self.finetuning_task = finetuning_task\n",
        "        self.id2label = id2label\n",
        "        self.label2id = label2id\n",
        "        self.task_specific_params = task_specific_params\n",
        "        self.problem_type = problem_type\n",
        "\n",
        "        if self.id2label is None:\n",
        "            self._create_id_label_maps(num_labels if num_labels is not None else 2)\n",
        "        else:\n",
        "            self.id2label = {int(key): value for key, value in self.id2label.items()}\n",
        "\n",
        "        self.tokenizer_class = tokenizer_class\n",
        "        self.prefix = prefix\n",
        "        self.bos_token_id = bos_token_id\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.sep_token_id = sep_token_id\n",
        "        self.decoder_start_token_id = decoder_start_token_id\n",
        "\n",
        "        for parameter_name, default_value in self._get_global_generation_defaults().items():\n",
        "            setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n",
        "\n",
        "        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n",
        "        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        self._attn_implementation_internal = kwargs.pop(\"attn_implementation\", None)\n",
        "        self._attn_implementation_autoset = False\n",
        "\n",
        "        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "        self.tf_legacy_loss = kwargs.pop(\"tf_legacy_loss\", False)\n",
        "        self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls: type[SpecificPretrainedConfigType],\n",
        "        pretrained_model_name_or_path: Union[str, os.PathLike],\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        **kwargs,\n",
        "    ) -> SpecificPretrainedConfigType:\n",
        "        kwargs[\"cache_dir\"] = cache_dir\n",
        "        kwargs[\"force_download\"] = force_download\n",
        "        kwargs[\"local_files_only\"] = local_files_only\n",
        "        kwargs[\"revision\"] = revision\n",
        "\n",
        "        cls._set_token_in_kwargs(kwargs, token)\n",
        "\n",
        "        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "        if cls.base_config_key and cls.base_config_key in config_dict:\n",
        "            config_dict = config_dict[cls.base_config_key]\n",
        "\n",
        "        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n",
        "            for v in config_dict.values():\n",
        "                if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n",
        "                    config_dict = v\n",
        "\n",
        "        return cls.from_dict(config_dict, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_token_in_kwargs(kwargs, token=None):\n",
        "        if token is None:\n",
        "            token = kwargs.pop(\"token\", None)\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "\n",
        "        if token is not None:\n",
        "            kwargs[\"token\"] = token\n",
        "\n",
        "    @classmethod\n",
        "    def _get_config_dict(\n",
        "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
        "    ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
        "        force_download = kwargs.pop(\"force_download\", False)\n",
        "        resume_download = kwargs.pop(\"resume_download\", None)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        token = kwargs.pop(\"token\", None)\n",
        "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
        "        revision = kwargs.pop(\"revision\", None)\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        gguf_file = kwargs.get(\"gguf_file\", None)\n",
        "\n",
        "        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n",
        "        if from_pipeline is not None:\n",
        "            user_agent[\"using_pipeline\"] = from_pipeline\n",
        "\n",
        "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
        "\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "        if os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n",
        "            resolved_config_file = pretrained_model_name_or_path\n",
        "            is_local = True\n",
        "        elif is_remote_url(pretrained_model_name_or_path):\n",
        "            configuration_file = pretrained_model_name_or_path if gguf_file is None else gguf_file\n",
        "            resolved_config_file = download_url(pretrained_model_name_or_path)\n",
        "        else:\n",
        "            configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME) if gguf_file is None else gguf_file\n",
        "\n",
        "            resolved_config_file = cached_file(\n",
        "                pretrained_model_name_or_path,\n",
        "                configuration_file,\n",
        "                cache_dir=cache_dir,\n",
        "                force_download=force_download,\n",
        "                proxies=proxies,\n",
        "                resume_download=resume_download,\n",
        "                local_files_only=local_files_only,\n",
        "                token=token,\n",
        "                user_agent=user_agent,\n",
        "                revision=revision,\n",
        "                subfolder=subfolder,\n",
        "                _commit_hash=commit_hash,\n",
        "            )\n",
        "            if resolved_config_file is None:\n",
        "                return None, kwargs\n",
        "            commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "\n",
        "        if gguf_file:\n",
        "            config_dict = load_gguf_checkpoint(resolved_config_file, return_tensors=False)[\"config\"]\n",
        "        else:\n",
        "            config_dict = cls._dict_from_json_file(resolved_config_file)\n",
        "\n",
        "        config_dict[\"_commit_hash\"] = commit_hash\n",
        "        if \"model_type\" not in config_dict and is_timm_config_dict(config_dict):\n",
        "            config_dict[\"model_type\"] = \"timm_wrapper\"\n",
        "\n",
        "        return config_dict, kwargs\n",
        "\n",
        "    @classmethod\n",
        "    def get_config_dict(\n",
        "        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n",
        "    ) -> tuple[dict[str, Any], dict[str, Any]]:\n",
        "        cls._set_token_in_kwargs(kwargs)\n",
        "\n",
        "        original_kwargs = copy.deepcopy(kwargs)\n",
        "        config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "        if config_dict is None:\n",
        "            return {}, kwargs\n",
        "        if \"_commit_hash\" in config_dict:\n",
        "            original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        if \"configuration_files\" in config_dict:\n",
        "            configuration_file = get_configuration_file(config_dict[\"configuration_files\"])\n",
        "            config_dict, kwargs = cls._get_config_dict(\n",
        "                pretrained_model_name_or_path, _configuration_file=configuration_file, **original_kwargs\n",
        "            )\n",
        "\n",
        "        return config_dict, kwargs\n",
        "\n",
        "    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n",
        "        decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n",
        "        encoder_possible_text_config_names = (\"text_encoder\",)\n",
        "        if decoder:\n",
        "            possible_text_config_names = decoder_possible_text_config_names\n",
        "        else:\n",
        "            possible_text_config_names = encoder_possible_text_config_names + decoder_possible_text_config_names\n",
        "\n",
        "        valid_text_config_names = []\n",
        "        for text_config_name in possible_text_config_names:\n",
        "            if hasattr(self, text_config_name):\n",
        "                text_config = getattr(self, text_config_name, None)\n",
        "                if text_config is not None:\n",
        "                    valid_text_config_names += [text_config_name]\n",
        "\n",
        "        if len(valid_text_config_names) == 1:\n",
        "            config_to_return = getattr(self, valid_text_config_names[0])\n",
        "        else:\n",
        "            config_to_return = self\n",
        "        return config_to_return\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_global_generation_defaults() -> dict[str, Any]:\n",
        "        return {\n",
        "            \"max_length\": 20,\n",
        "            \"min_length\": 0,\n",
        "            \"do_sample\": False,\n",
        "            \"early_stopping\": False,\n",
        "            \"num_beams\": 1,\n",
        "            \"num_beam_groups\": 1,\n",
        "            \"diversity_penalty\": 0.0,\n",
        "            \"temperature\": 1.0,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 1.0,\n",
        "            \"typical_p\": 1.0,\n",
        "            \"repetition_penalty\": 1.0,\n",
        "            \"length_penalty\": 1.0,\n",
        "            \"no_repeat_ngram_size\": 0,\n",
        "            \"encoder_no_repeat_ngram_size\": 0,\n",
        "            \"bad_words_ids\": None,\n",
        "            \"num_return_sequences\": 1,\n",
        "            \"output_scores\": False,\n",
        "            \"return_dict_in_generate\": False,\n",
        "            \"forced_bos_token_id\": None,\n",
        "            \"forced_eos_token_id\": None,\n",
        "            \"remove_invalid_values\": False,\n",
        "            \"exponential_decay_length_penalty\": None,\n",
        "            \"suppress_tokens\": None,\n",
        "            \"begin_suppress_tokens\": None,\n",
        "            }\n",
        "\n",
        "    def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n",
        "        if hasattr(self, \"quantization_config\"):\n",
        "            _ = d.pop(\"_pre_quantization_dtype\", None)\n",
        "\n",
        "        if \"_auto_class\" in d:\n",
        "            del d[\"_auto_class\"]\n",
        "        if \"_output_attentions\" in d:\n",
        "            d[\"output_attentions\"] = d.pop(\"_output_attentions\")\n",
        "        if \"_commit_hash\" in d:\n",
        "            del d[\"_commit_hash\"]\n",
        "        if \"_attn_implementation_internal\" in d:\n",
        "            del d[\"_attn_implementation_internal\"]\n",
        "        if \"_attn_implementation_autoset\" in d:\n",
        "            del d[\"_attn_implementation_autoset\"]\n",
        "        if \"base_model_tp_plan\" in d:\n",
        "            del d[\"base_model_tp_plan\"]\n",
        "        if \"base_model_pp_plan\" in d:\n",
        "            del d[\"base_model_pp_plan\"]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self._remove_keys_not_serialized(value)\n",
        "\n",
        "\n",
        "    def _create_id_label_maps(self, num_labels: int):\n",
        "        self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n",
        "        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n",
        "\n",
        "    @classmethod\n",
        "    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
        "        with open(json_file, encoding=\"utf-8\") as reader:\n",
        "            text = reader.read()\n",
        "        return json.loads(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(\n",
        "        cls: type[SpecificPretrainedConfigType], config_dict: dict[str, Any], **kwargs\n",
        "    ) -> SpecificPretrainedConfigType:\n",
        "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
        "        kwargs.pop(\"_from_auto\", None)\n",
        "        kwargs.pop(\"_from_pipeline\", None)\n",
        "        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n",
        "            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n",
        "\n",
        "        config = cls(**config_dict)\n",
        "\n",
        "        if hasattr(config, \"pruned_heads\"):\n",
        "            config.pruned_heads = {int(key): value for key, value in config.pruned_heads.items()}\n",
        "\n",
        "        if \"num_labels\" in kwargs and \"id2label\" in kwargs:\n",
        "            num_labels = kwargs[\"num_labels\"]\n",
        "            id2label = kwargs[\"id2label\"] if kwargs[\"id2label\"] is not None else []\n",
        "        to_remove = []\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(config, key):\n",
        "                current_attr = getattr(config, key)\n",
        "                if isinstance(current_attr, PretrainedConfig) and isinstance(value, dict):\n",
        "                    value = current_attr.__class__(**value)\n",
        "                setattr(config, key, value)\n",
        "                if key != \"torch_dtype\":\n",
        "                    to_remove.append(key)\n",
        "        for key in to_remove:\n",
        "            kwargs.pop(key, None)\n",
        "\n",
        "        if return_unused_kwargs:\n",
        "            return config, kwargs\n",
        "        else:\n",
        "            return config\n",
        "\n",
        "    def to_dict(self) -> dict[str, Any]:\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        if hasattr(self.__class__, \"model_type\"):\n",
        "            output[\"model_type\"] = self.__class__.model_type\n",
        "\n",
        "        output[\"transformers_version\"] = __version__\n",
        "\n",
        "        for key, value in output.items():\n",
        "            if isinstance(value, PretrainedConfig):\n",
        "                value = value.to_dict()\n",
        "                del value[\"transformers_version\"]\n",
        "\n",
        "            output[key] = value\n",
        "\n",
        "        self._remove_keys_not_serialized(output)\n",
        "\n",
        "        if hasattr(self, \"quantization_config\"):\n",
        "            output[\"quantization_config\"] = (\n",
        "                self.quantization_config.to_dict()\n",
        "                if not isinstance(self.quantization_config, dict)\n",
        "                else self.quantization_config\n",
        "            )\n",
        "        self.dict_torch_dtype_to_str(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n",
        "        if d.get(\"torch_dtype\", None) is not None:\n",
        "            if isinstance(d[\"torch_dtype\"], dict):\n",
        "                d[\"torch_dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"torch_dtype\"].items()}\n",
        "            elif not isinstance(d[\"torch_dtype\"], str):\n",
        "                d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self.dict_torch_dtype_to_str(value)"
      ],
      "metadata": {
        "id": "Mj3ZBcGc1DoS"
      },
      "execution_count": 750,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreTrainedTokenizer():\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        self.tokens_trie = Trie()\n",
        "\n",
        "        if not hasattr(self, \"_added_tokens_decoder\"):\n",
        "            self._added_tokens_decoder = {}\n",
        "\n",
        "        self._added_tokens_decoder.update(kwargs.pop(\"added_tokens_decoder\", {}))\n",
        "        self._added_tokens_encoder: dict[str, int] = {k.content: v for v, k in self._added_tokens_decoder.items()}\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self._add_tokens(\n",
        "            [token for token in self.all_special_tokens_extended if token not in self._added_tokens_encoder],\n",
        "            special_tokens=True,\n",
        "        )\n",
        "\n",
        "        self._decode_use_source_tokenizer = False"
      ],
      "metadata": {
        "id": "W5AqaGacSvRU"
      },
      "execution_count": 751,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelOutput(OrderedDict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        is_modeloutput_subclass = self.__class__ != ModelOutput\n",
        "\n",
        "    def __getitem__(self, k):\n",
        "        if isinstance(k, str):\n",
        "            inner_dict = dict(self.items())\n",
        "            return inner_dict[k]\n",
        "        else:\n",
        "            return self.to_tuple()[k]\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if name in self.keys() and value is not None:\n",
        "            super().__setitem__(name, value)\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        super().__setitem__(key, value)\n",
        "        super().__setattr__(key, value)\n",
        "\n",
        "    def __reduce__(self):\n",
        "        if not is_dataclass(self):\n",
        "            return super().__reduce__()\n",
        "        callable, _args, *remaining = super().__reduce__()\n",
        "        args = tuple(getattr(self, field.name) for field in fields(self))\n",
        "        return callable, args, *remaining\n",
        "\n",
        "    def to_tuple(self) -> tuple[Any]:\n",
        "        return tuple(self[k] for k in self.keys())"
      ],
      "metadata": {
        "id": "iTKZ8pDnQQkL"
      },
      "execution_count": 752,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 753,
      "metadata": {
        "id": "JyA21IWgn2gJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd010ed9-0a35-4193-91d4-06e5db4eda91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected torch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n",
        "    if not _is_package_available(\"torch\"):\n",
        "        return False\n",
        "\n",
        "    if accept_dev:\n",
        "        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n",
        "            library_version\n",
        "        )\n",
        "    else:\n",
        "        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n",
        "\n",
        "_is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)\n",
        "\n",
        "def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n",
        "    return kv_idx <= q_idx\n",
        "\n",
        "def prepare_padding_mask(\n",
        "    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int, _slice: bool = True\n",
        ") -> Optional[torch.Tensor]:\n",
        "    local_padding_mask = attention_mask\n",
        "    if attention_mask is not None:\n",
        "        if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
        "            local_padding_mask = torch.nn.functional.pad(attention_mask, (0, padding_length))\n",
        "        if _slice:\n",
        "            mask_indices = torch.arange(kv_length, device=local_padding_mask.device)\n",
        "            mask_indices += kv_offset\n",
        "            local_padding_mask = local_padding_mask[:, mask_indices]\n",
        "    return local_padding_mask\n",
        "\n",
        "_torch_available = False\n",
        "\n",
        "def is_torch_available():\n",
        "    return _torch_available\n",
        "\n",
        "def is_torchdynamo_compiling():\n",
        "    if not is_torch_available():\n",
        "        return False\n",
        "\n",
        "    import torch\n",
        "\n",
        "    return torch.compiler.is_compiling()\n",
        "\n",
        "def _ignore_causal_mask_sdpa(\n",
        "    padding_mask: Optional[torch.Tensor],\n",
        "    query_length: int,\n",
        "    kv_length: int,\n",
        "    kv_offset: int,\n",
        "    local_attention_size: Optional[int] = None,\n",
        ") -> bool:\n",
        "    is_tracing = torch.jit.is_tracing() or isinstance(padding_mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n",
        "    if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n",
        "        mask_indices = torch.arange(kv_length, device=padding_mask.device)\n",
        "        mask_indices += kv_offset\n",
        "        padding_mask = padding_mask[:, mask_indices]\n",
        "\n",
        "    if (\n",
        "        not is_tracing\n",
        "        and (query_length == 1 or kv_length == query_length)\n",
        "        and (local_attention_size is None or kv_length < local_attention_size)\n",
        "        and (padding_mask is None or padding_mask.all())\n",
        "    ):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def sdpa_mask_recent_torch(\n",
        "    batch_size: int,\n",
        "    cache_position: torch.Tensor,\n",
        "    kv_length: int,\n",
        "    kv_offset: int = 0,\n",
        "    mask_function: Callable = causal_mask_function,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    local_size: Optional[int] = None,\n",
        "    allow_is_causal_skip: bool = True,\n",
        "    **kwargs,\n",
        ") -> Optional[torch.Tensor]:\n",
        "    q_length = cache_position.shape[0]\n",
        "    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n",
        "\n",
        "    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n",
        "        return None\n",
        "\n",
        "    kv_arange = torch.arange(kv_length, device=cache_position.device)\n",
        "    kv_arange += kv_offset\n",
        "\n",
        "    if padding_mask is not None:\n",
        "        mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n",
        "\n",
        "    batch_arange = torch.arange(batch_size, device=cache_position.device)\n",
        "    head_arange = torch.arange(1, device=cache_position.device)\n",
        "    with TransformGetItemToIndex():\n",
        "        causal_mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n",
        "\n",
        "    return causal_mask\n",
        "\n",
        "sdpa_mask = sdpa_mask_recent_torch if _is_torch_greater_or_equal_than_2_6 else sdpa_mask_older_torch\n",
        "\n",
        "def _preprocess_mask_arguments(\n",
        "    config: PretrainedConfig,\n",
        "    input_embeds: torch.Tensor,\n",
        "    attention_mask: Optional[Union[torch.Tensor, torch.Tensor]],\n",
        "    cache_position: torch.Tensor,\n",
        "    past_key_values: Optional[Cache],\n",
        "    position_ids: Optional[torch.Tensor],\n",
        "    layer_idx: Optional[int],\n",
        ") -> tuple[bool, Optional[Union[torch.Tensor, torch.Tensor]], int, int]:\n",
        "\n",
        "    if isinstance(attention_mask, (torch.Tensor, torch.Tensor)) and len(attention_mask.shape) == 4:\n",
        "        return True, attention_mask, None, None, None\n",
        "\n",
        "    if config._attn_implementation not in AttentionMaskInterface._global_mapping:\n",
        "        return True, None, None, None, None\n",
        "\n",
        "    if attention_mask is not None and attention_mask.ndim == 2:\n",
        "        attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n",
        "\n",
        "    if past_key_values is not None:\n",
        "        kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n",
        "    else:\n",
        "        kv_length, kv_offset = input_embeds.shape[1], 0\n",
        "\n",
        "    packed_sequence_mask = None\n",
        "    if position_ids is not None and attention_mask is None and past_key_values is None:\n",
        "        batch_size = input_embeds.shape[0]\n",
        "        if batch_size != position_ids.shape[0]:\n",
        "            position_ids = position_ids.expand(batch_size, -1)\n",
        "        packed_sequence_mask = find_packed_sequence_indices(position_ids)\n",
        "\n",
        "    return False, attention_mask, packed_sequence_mask, kv_length, kv_offset\n",
        "\n",
        "def create_causal_mask(\n",
        "    config: PretrainedConfig,\n",
        "    input_embeds: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    cache_position: torch.Tensor,\n",
        "    past_key_values: Optional[Cache],\n",
        "    position_ids: Optional[torch.Tensor] = None,\n",
        "    or_mask_function: Optional[Callable] = None,\n",
        "    and_mask_function: Optional[Callable] = None,\n",
        ") -> Optional[Union[torch.Tensor, torch.Tensor]]:\n",
        "    if hasattr(past_key_values, \"is_sliding\") and False in past_key_values.is_sliding:\n",
        "        layer_idx = past_key_values.is_sliding.index(False)\n",
        "    else:\n",
        "        layer_idx = 0\n",
        "\n",
        "    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n",
        "        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n",
        "    )\n",
        "    if early_exit:\n",
        "        return attention_mask\n",
        "\n",
        "    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n",
        "    mask_factory_function = causal_mask_function\n",
        "    mask_interface = sdpa_mask\n",
        "\n",
        "    allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n",
        "\n",
        "    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n",
        "        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n",
        "        allow_is_causal_skip = False\n",
        "\n",
        "    if or_mask_function is not None:\n",
        "        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n",
        "        allow_is_causal_skip = False\n",
        "    if and_mask_function is not None:\n",
        "        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n",
        "        allow_is_causal_skip = False\n",
        "\n",
        "    causal_mask = mask_interface(\n",
        "        batch_size=batch_size,\n",
        "        cache_position=cache_position,\n",
        "        kv_length=kv_length,\n",
        "        kv_offset=kv_offset,\n",
        "        mask_function=mask_factory_function,\n",
        "        attention_mask=attention_mask,\n",
        "        allow_is_causal_skip=allow_is_causal_skip,\n",
        "        dtype=dtype,\n",
        "        config=config,\n",
        "    )\n",
        "    return causal_mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneralInterface(MutableMapping):\n",
        "    _global_mapping = {}\n",
        "    def __init__(self):\n",
        "        self._local_mapping = {}\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if key in self._local_mapping:\n",
        "            return self._local_mapping[key]\n",
        "        return self._global_mapping[key]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self._local_mapping.update({key: value})\n",
        "\n",
        "    def __delitem__(self, key):\n",
        "        del self._local_mapping[key]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter({**self._global_mapping, **self._local_mapping})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._global_mapping.keys() | self._local_mapping.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def register(cls, key: str, value: Callable):\n",
        "        cls._global_mapping.update({key: value})\n",
        "\n",
        "    @classmethod\n",
        "    def valid_keys(cls) -> list[str]:\n",
        "        return list(cls._global_mapping.keys())\n",
        "\n",
        "@dataclass\n",
        "class CausalLMOutputWithPast(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: Optional[torch.FloatTensor] = None\n",
        "    past_key_values: Optional[Cache] = None\n",
        "    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "    attentions: Optional[tuple[torch.FloatTensor, ...]] = None"
      ],
      "metadata": {
        "id": "mdlQFOkoQ0El"
      },
      "execution_count": 754,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 755,
      "metadata": {
        "id": "18p38Q9meo6q"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class AttentionMaskInterface(GeneralInterface):\n",
        "    _global_mapping = {\n",
        "        \"sdpa\": sdpa_mask,\n",
        "    }\n",
        "\"\"\"\n",
        "def _get_frameworks_and_test_func(x):\n",
        "    framework_to_test = {\n",
        "        \"pt\": is_torch_tensor,\n",
        "        \"tf\": is_tf_tensor,\n",
        "        \"jax\": is_jax_tensor,\n",
        "        \"np\": is_numpy_array,\n",
        "        \"mlx\": is_mlx_array,\n",
        "    }\n",
        "    preferred_framework = infer_framework_from_repr(x)\n",
        "    frameworks = [] if preferred_framework is None else [preferred_framework]\n",
        "    if preferred_framework != \"np\":\n",
        "        frameworks.append(\"np\")\n",
        "    frameworks.extend([f for f in framework_to_test if f not in [preferred_framework, \"np\"]])\n",
        "    return {f: framework_to_test[f] for f in frameworks}\n",
        "\n",
        "\n",
        "\n",
        "def is_tensor(x):\n",
        "    framework_to_test_func = _get_frameworks_and_test_func(x)\n",
        "    for test_func in framework_to_test_func.values():\n",
        "        if test_func(x):\n",
        "            return True\n",
        "\n",
        "    if is_torch_fx_proxy(x):\n",
        "        return True\n",
        "\n",
        "    if is_flax_available():\n",
        "        from jax.core import Tracer\n",
        "\n",
        "        if isinstance(x, Tracer):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\"\"\"\n",
        "@dataclass\n",
        "class BaseModelOutputWithPast(ModelOutput):\n",
        "    last_hidden_state: Optional[torch.FloatTensor] = None\n",
        "    past_key_values: Optional[Cache] = None\n",
        "    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n",
        "    attentions: Optional[tuple[torch.FloatTensor, ...]] = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_hf_deepspeed_config_weak_ref = None\n",
        "\n",
        "def get_module_from_name(module, tensor_name: str) -> tuple[Any, str]:\n",
        "    if \".\" in tensor_name:\n",
        "        module_name, tensor_name = tensor_name.rsplit(\".\", 1)\n",
        "        module = module.get_submodule(module_name)\n",
        "    return module, tensor_name\n",
        "\n",
        "class AutoHfQuantizer:\n",
        "    @classmethod\n",
        "    def from_config(cls, quantization_config: Union[dict], **kwargs):\n",
        "        if isinstance(quantization_config, dict):\n",
        "            quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n",
        "\n",
        "        quant_method = quantization_config.quant_method\n",
        "\n",
        "        if quant_method == QuantizationMethod.BITS_AND_BYTES:\n",
        "            if quantization_config.load_in_8bit:\n",
        "                quant_method += \"_8bit\"\n",
        "            else:\n",
        "                quant_method += \"_4bit\"\n",
        "\n",
        "        target_cls = AUTO_QUANTIZER_MAPPING[quant_method]\n",
        "        return target_cls(quantization_config, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n",
        "        quantization_config = AutoQuantizationConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
        "        return cls.from_config(quantization_config)\n",
        "\n",
        "    @classmethod\n",
        "    def merge_quantization_configs(\n",
        "        cls,\n",
        "        quantization_config: Union[dict],\n",
        "        quantization_config_from_args,\n",
        "    ):\n",
        "        if quantization_config_from_args is not None:\n",
        "            warning_msg = (\n",
        "                \"You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading\"\n",
        "                \" already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\"\n",
        "            )\n",
        "        else:\n",
        "            warning_msg = \"\"\n",
        "\n",
        "        if isinstance(quantization_config, dict):\n",
        "            if isinstance(quantization_config_from_args, AutoRoundConfig):\n",
        "                quantization_config = AutoRoundConfig.from_dict(quantization_config)\n",
        "            else:\n",
        "                quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n",
        "\n",
        "        if (\n",
        "            isinstance(\n",
        "                quantization_config, (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig)\n",
        "            )\n",
        "            and quantization_config_from_args is not None\n",
        "        ):\n",
        "            loading_attr_dict = quantization_config_from_args.get_loading_attributes()\n",
        "            for attr, val in loading_attr_dict.items():\n",
        "                setattr(quantization_config, attr, val)\n",
        "\n",
        "            warning_msg += f\"However, loading attributes (e.g. {list(loading_attr_dict.keys())}) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\"\n",
        "\n",
        "        if warning_msg != \"\":\n",
        "            warnings.warn(warning_msg)\n",
        "\n",
        "        return quantization_config\n",
        "\n",
        "    @staticmethod\n",
        "    def supports_quant_method(quantization_config_dict):\n",
        "        quant_method = quantization_config_dict.get(\"quant_method\", None)\n",
        "        if quantization_config_dict.get(\"load_in_8bit\", False) or quantization_config_dict.get(\"load_in_4bit\", False):\n",
        "            suffix = \"_4bit\" if quantization_config_dict.get(\"load_in_4bit\", False) else \"_8bit\"\n",
        "            quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n",
        "\n",
        "        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING.keys():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "class HfQuantizer():\n",
        "    requires_calibration = False\n",
        "    required_packages = None\n",
        "    requires_parameters_quantization = False\n",
        "\n",
        "    def __init__(self, quantization_config, **kwargs):\n",
        "        self.quantization_config = quantization_config\n",
        "\n",
        "        self.modules_to_not_convert = kwargs.pop(\"modules_to_not_convert\", [])\n",
        "        self.pre_quantized = kwargs.pop(\"pre_quantized\", True)\n",
        "\n",
        "    def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n",
        "        return torch_dtype\n",
        "\n",
        "    def update_device_map(self, device_map: Optional[dict[str, Any]]) -> Optional[dict[str, Any]]:\n",
        "        return device_map\n",
        "\n",
        "    def adjust_target_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n",
        "        return torch_dtype\n",
        "\n",
        "    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n",
        "        return missing_keys\n",
        "\n",
        "    def update_unexpected_keys(self, model, unexpected_keys: list[str], prefix: str) -> list[str]:\n",
        "        return unexpected_keys\n",
        "\n",
        "    def update_missing_keys_after_loading(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n",
        "        return missing_keys\n",
        "\n",
        "    def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: list[str]) -> list[str]:\n",
        "        return expected_keys\n",
        "\n",
        "    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n",
        "        return max_memory\n",
        "\n",
        "    def check_quantized_param(\n",
        "        self,\n",
        "        model: \"PreTrainedModel\",\n",
        "        param_value: \"torch.Tensor\",\n",
        "        param_name: str,\n",
        "        state_dict: dict[str, Any],\n",
        "        **kwargs,\n",
        "    ) -> bool:\n",
        "        return False\n",
        "\n",
        "    def create_quantized_param(self, *args, **kwargs) -> \"torch.nn.Parameter\":\n",
        "        if not self.requires_parameters_quantization:\n",
        "            raise AttributeError(\n",
        "                f\"`.create_quantized_param()` method is not supported by quantizer class {self.__class__.__name__}.\"\n",
        "            )\n",
        "\n",
        "    def validate_environment(self, *args, **kwargs):\n",
        "        return\n",
        "\n",
        "    def update_tp_plan(self, config):\n",
        "        \"updates the tp plan for the scales\"\n",
        "        return config\n",
        "\n",
        "    def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n",
        "        model.is_quantized = True\n",
        "        model.quantization_method = self.quantization_config.quant_method\n",
        "        if self.pre_quantized:\n",
        "            self._convert_model_for_quantization(model)\n",
        "        return self._process_model_before_weight_loading(model, **kwargs)\n",
        "\n",
        "    def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n",
        "        return self._process_model_after_weight_loading(model, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_modules_to_not_convert(\n",
        "        model: \"PreTrainedModel\",\n",
        "        skip_modules: Optional[list[str]] = None,\n",
        "        keep_in_fp32_modules: Optional[list[str]] = None,\n",
        "        add_default_skips: bool = False,\n",
        "    ):\n",
        "        from ..integrations import get_keys_to_not_convert\n",
        "\n",
        "        if skip_modules is None or add_default_skips:\n",
        "            modules_to_not_convert = get_keys_to_not_convert(model)\n",
        "        else:\n",
        "            modules_to_not_convert = []\n",
        "\n",
        "        if skip_modules is not None:\n",
        "            modules_to_not_convert.extend(skip_modules)\n",
        "\n",
        "        if keep_in_fp32_modules is not None:\n",
        "            modules_to_not_convert.extend(keep_in_fp32_modules)\n",
        "\n",
        "        return modules_to_not_convert\n",
        "\n",
        "    @property\n",
        "    def is_qat_trainable(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def is_compileable(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @abstractmethod\n",
        "    def _process_model_before_weight_loading(self, model, **kwargs): ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def _process_model_after_weight_loading(self, model, **kwargs): ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_serializable(self, safe_serialization=None): ...\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def is_trainable(self): ...\n",
        "\n",
        "    def _convert_model_for_quantization(self, model):\n",
        "        from accelerate import init_empty_weights\n",
        "        \"\"\"\n",
        "        for name, module in model.named_modules():\n",
        "            module_class_name = module.__class__.__name__\n",
        "            if module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION.keys() and (\n",
        "                self.quantization_config.quant_method\n",
        "                in MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"quantization_methods\"]\n",
        "            ):\n",
        "                with init_empty_weights():\n",
        "                    parent_module, name = get_module_from_name(model, name)\n",
        "                    parent_module._modules[name] = MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"module_name\"](\n",
        "                        model.config.get_text_config()\n",
        "                    )\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "HDy8-19HyQz6"
      },
      "execution_count": 756,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int) -> torch.Tensor:\n",
        "    if elements.device.type == \"mps\" and not is_torch_greater_or_equal_than_2_4:\n",
        "        test_elements = torch.tensor(test_elements)\n",
        "        if test_elements.ndim == 0:\n",
        "            test_elements = test_elements.unsqueeze(0)\n",
        "        return elements.tile(test_elements.shape[0], 1).eq(test_elements.unsqueeze(1)).sum(dim=0).bool().squeeze()\n",
        "    else:\n",
        "        return torch.isin(elements, test_elements)"
      ],
      "metadata": {
        "id": "yQSiZa52xyNo"
      },
      "execution_count": 757,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 758,
      "metadata": {
        "id": "vZM7SGAS7fHB"
      },
      "outputs": [],
      "source": [
        "def _vec_softmax(input: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None):\n",
        "\n",
        "    dim = dim if dim >= 0 else input.dim() + dim\n",
        "\n",
        "    shape = input.shape\n",
        "    assert 0 <= dim < input.dim()\n",
        "\n",
        "    outer_size = 1\n",
        "    for i in range(dim):\n",
        "        outer_size *= shape[i]\n",
        "\n",
        "    inner_size = 1\n",
        "    for i in range(dim + 1, input.dim()):\n",
        "        inner_size *= shape[i]\n",
        "\n",
        "    dim_size = shape[dim]\n",
        "\n",
        "    input_transposed = input.permute(\n",
        "        *[i for i in range(input.dim()) if i != dim], dim\n",
        "    ).contiguous()\n",
        "    reshaped = input_transposed.view(outer_size, dim_size, inner_size)\n",
        "\n",
        "    max_vals, _ = reshaped.max(dim=1, keepdim=True)\n",
        "\n",
        "    exps = (reshaped - max_vals).exp()\n",
        "    sum_exps = exps.sum(dim=1, keepdim=True)\n",
        "\n",
        "    softmaxed = exps / sum_exps\n",
        "\n",
        "    output = softmaxed.view(input_transposed.shape).permute(\n",
        "        *[i for i in range(input.dim()) if i != dim], input.dim() - 1\n",
        "    ).contiguous()\n",
        "\n",
        "    return output\n",
        "\n",
        "def softmax(input: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n",
        "\n",
        "    orig_dtype = input.dtype\n",
        "    if dtype is not None:\n",
        "        input = input.to(dtype)\n",
        "    elif input.dtype in (torch.float16, torch.bfloat16):\n",
        "        input = input.to(torch.float32)\n",
        "\n",
        "    max_val, _ = input.max(dim=dim, keepdim=True)\n",
        "    input = input - max_val\n",
        "\n",
        "    exp_input = input.exp()\n",
        "    sum_exp = exp_input.sum(dim=dim, keepdim=True)\n",
        "    softmax_output = exp_input / sum_exp\n",
        "\n",
        "    return softmax_output.to(orig_dtype)\n",
        "\n",
        "def safe_softmax(input: torch.Tensor, dim: int, dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n",
        "    dim = dim if dim >= 0 else input.dim() + dim\n",
        "\n",
        "    out = softmax(input, dim=dim, dtype=dtype)\n",
        "\n",
        "    masked = input == float(\"-inf\")\n",
        "\n",
        "    masked_rows = masked.all(dim=dim, keepdim=True)\n",
        "\n",
        "    out = torch.where(masked_rows, torch.zeros_like(out), out)\n",
        "    return out\n",
        "\n",
        "def dropout_cpu(input: torch.Tensor, p: float, train: Optional[bool] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    if input.numel() == 0:\n",
        "        return input, torch.empty_like(input)\n",
        "\n",
        "    if train is None:\n",
        "        train = True\n",
        "\n",
        "    if train:\n",
        "        p1m = 1.0 - p\n",
        "        scale = 0.0 if p1m == 0 else 1.0 / p1m\n",
        "        mask = torch.empty_like(input, dtype=torch.bool).bernoulli_(p1m)\n",
        "        output = input * mask.to(input.dtype) * scale\n",
        "    else:\n",
        "        mask = torch.ones_like(input, dtype=torch.bool)\n",
        "        output = input.clone()\n",
        "\n",
        "    return output, mask\n",
        "\n",
        "def dropout(input: torch.Tensor, p: float = 0.5, train: bool = True) -> torch.Tensor:\n",
        "    if input.is_nested():\n",
        "        raise NotImplementedError(\"nested tensor の dropout は未対応です\")\n",
        "\n",
        "    return dropout_cpu(input, p=p, train=train)\n",
        "\n",
        "def scaled_dot_product_attention(\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    attn_mask: Optional[torch.Tensor] = None,\n",
        "    dropout_p: float = 0.0,\n",
        "    is_causal: bool = False,\n",
        "    scale: Optional[float] = None,\n",
        ") -> Tuple[torch.Tensor]:\n",
        "\n",
        "    origin_dtype = query.dtype\n",
        "    def maybe_upcast(t):\n",
        "        return t.float() if t.dtype in (torch.float16, torch.bfloat16) else t\n",
        "\n",
        "    query = maybe_upcast(query)\n",
        "    key = maybe_upcast(key)\n",
        "    value = maybe_upcast(value)\n",
        "\n",
        "    if query.size(1) != key.size(1):\n",
        "        qh, kh = query.size(1), key.size(1)\n",
        "        assert qh % kh == 0, \"GQA: query_heads must be divisible by key_heads\"\n",
        "        repeat_factor = qh // kh\n",
        "        key = key.repeat_interleave(repeat_factor, dim=1)\n",
        "        value = value.repeat_interleave(repeat_factor, dim=1)\n",
        "\n",
        "    if scale is None:\n",
        "        scale = 1.0 / (key.size(-1) ** 0.5)\n",
        "\n",
        "    attn_weights = torch.matmul(query, key.transpose(-2, -1)) * scale\n",
        "\n",
        "    if is_causal:\n",
        "        q_len, k_len = attn_weights.size(-2), attn_weights.size(-1)\n",
        "        causal_mask = torch.tril(torch.ones(q_len, k_len, device=attn_weights.device, dtype=torch.bool))\n",
        "        attn_weights = attn_weights.masked_fill(~causal_mask.view(1, 1, q_len, k_len), float('-inf'))\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        if attn_mask.dtype == torch.bool:\n",
        "            attn_weights = attn_weights.masked_fill(~attn_mask, float('-inf'))\n",
        "        else:\n",
        "            attn_weights = attn_weights + attn_mask\n",
        "\n",
        "    attn_probs = safe_softmax(attn_weights, dim=-1)\n",
        "\n",
        "    if dropout_p > 0.0 and torch.is_grad_enabled():\n",
        "        attn_probs = dropout(attn_probs, p=dropout_p, training=True)\n",
        "\n",
        "    out = torch.matmul(attn_probs, value)\n",
        "\n",
        "    return out.to(origin_dtype)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n",
        "    if variant is not None:\n",
        "        path, name = weights_name.rsplit(\".\", 1)\n",
        "        weights_name = f\"{path}.{variant}.{name}\"\n",
        "    return weights_name\n",
        "\n",
        "if is_safetensors_available():\n",
        "    from safetensors import safe_open\n",
        "    from safetensors.torch import load_file as safe_load_file\n",
        "    from safetensors.torch import save_file as safe_save_file\n",
        "\n",
        "if is_peft_available():\n",
        "    from transformers.utils import find_adapter_config_file\n",
        "\n",
        "_init_weights = True\n",
        "_is_ds_init_called = False\n",
        "_torch_distributed_available = torch.distributed.is_available()\n",
        "SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n",
        "\n",
        "_CAN_RECORD_REGISTRY = {}\n",
        "\n",
        "TORCH_INIT_FUNCTIONS = {\n",
        "    \"uniform_\": nn.init.uniform_,\n",
        "    \"normal_\": nn.init.normal_,\n",
        "    \"trunc_normal_\": nn.init.trunc_normal_,\n",
        "    \"constant_\": nn.init.constant_,\n",
        "    \"xavier_uniform_\": nn.init.xavier_uniform_,\n",
        "    \"xavier_normal_\": nn.init.xavier_normal_,\n",
        "    \"kaiming_uniform_\": nn.init.kaiming_uniform_,\n",
        "    \"kaiming_normal_\": nn.init.kaiming_normal_,\n",
        "    \"uniform\": nn.init.uniform,\n",
        "    \"normal\": nn.init.normal,\n",
        "    \"xavier_uniform\": nn.init.xavier_uniform,\n",
        "    \"xavier_normal\": nn.init.xavier_normal,\n",
        "    \"kaiming_uniform\": nn.init.kaiming_uniform,\n",
        "    \"kaiming_normal\": nn.init.kaiming_normal,\n",
        "}\n",
        "\n",
        "VLMS = [\n",
        "    \"aria\",\n",
        "    \"ayavision\",\n",
        "    \"colpali\",\n",
        "    \"emu3\",\n",
        "    \"fuyu\",\n",
        "    \"gotocr2\",\n",
        "    \"gemma3\",\n",
        "    \"internvl\",\n",
        "    \"llava\",\n",
        "    \"mistral3\",\n",
        "    \"mllama\",\n",
        "    \"paligemma\",\n",
        "    \"shieldgemma2\",\n",
        "    \"qwen2vl\",\n",
        "    \"qwen2_5_vl\",\n",
        "    \"videollava\",\n",
        "    \"vipllava\",\n",
        "]\n",
        "\n",
        "str_to_torch_dtype = {\n",
        "    \"BOOL\": torch.bool,\n",
        "    \"U8\": torch.uint8,\n",
        "    \"I8\": torch.int8,\n",
        "    \"I16\": torch.int16,\n",
        "    \"F16\": torch.float16,\n",
        "    \"BF16\": torch.bfloat16,\n",
        "    \"I32\": torch.int32,\n",
        "    \"F32\": torch.float32,\n",
        "    \"F64\": torch.float64,\n",
        "    \"I64\": torch.int64,\n",
        "    \"F8_E4M3\": torch.float8_e4m3fn,\n",
        "    \"F8_E5M2\": torch.float8_e5m2,\n",
        "}\n",
        "\n",
        "def restore_default_torch_dtype(func):\n",
        "    @wraps(func)\n",
        "    def _wrapper(*args, **kwargs):\n",
        "        old_dtype = torch.get_default_dtype()\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        finally:\n",
        "            torch.set_default_dtype(old_dtype)\n",
        "\n",
        "    return _wrapper\n",
        "\n",
        "def get_torch_context_manager_or_global_device():\n",
        "    device_in_context = torch.tensor([]).device\n",
        "    default_device = torch.get_default_device() if is_torch_greater_or_equal(\"2.3\") else torch.device(\"cpu\")\n",
        "    if device_in_context == default_device:\n",
        "        if default_device != torch.device(\"cpu\"):\n",
        "            return default_device\n",
        "        return None\n",
        "    return device_in_context\n",
        "\n",
        "def load_state_dict(\n",
        "    checkpoint_file: Union[str, os.PathLike],\n",
        "    is_quantized: bool = False,\n",
        "    map_location: Optional[Union[str, torch.device]] = \"cpu\",\n",
        "    weights_only: bool = True,\n",
        "):\n",
        "    if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n",
        "        with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
        "            metadata = f.metadata()\n",
        "\n",
        "            if metadata is not None and metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\", \"mlx\"]:\n",
        "                raise OSError(\n",
        "                    f\"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure \"\n",
        "                    \"you save your model with the `save_pretrained` method.\"\n",
        "                )\n",
        "            state_dict = {}\n",
        "            for k in f.keys():\n",
        "                if map_location == \"meta\":\n",
        "                    _slice = f.get_slice(k)\n",
        "                    k_dtype = _slice.get_dtype()\n",
        "                    if k_dtype in str_to_torch_dtype:\n",
        "                        dtype = str_to_torch_dtype[k_dtype]\n",
        "                    state_dict[k] = torch.empty(size=_slice.get_shape(), dtype=dtype, device=\"meta\")\n",
        "                else:\n",
        "                    state_dict[k] = f.get_tensor(k)\n",
        "            return state_dict\n",
        "\n",
        "    if weights_only:\n",
        "        check_torch_load_is_safe()\n",
        "    if map_location is None:\n",
        "        if (\n",
        "            (\n",
        "                is_deepspeed_zero3_enabled()\n",
        "                and torch.distributed.is_initialized()\n",
        "                and torch.distributed.get_rank() > 0\n",
        "            )\n",
        "            or (is_fsdp_enabled() and not is_local_dist_rank_0())\n",
        "        ) and not is_quantized:\n",
        "            map_location = \"meta\"\n",
        "        else:\n",
        "            map_location = \"cpu\"\n",
        "    extra_args = {}\n",
        "    if isinstance(checkpoint_file, str) and map_location != \"meta\" and is_zipfile(checkpoint_file):\n",
        "        extra_args = {\"mmap\": True}\n",
        "    return torch.load(\n",
        "        checkpoint_file,\n",
        "        map_location=map_location,\n",
        "        weights_only=weights_only,\n",
        "        **extra_args,\n",
        "    )\n",
        "\n",
        "    def _get_key_renaming_mapping(\n",
        "        self,\n",
        "        checkpoint_keys: list[str],\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        loading_base_model_from_task_state_dict: bool = False,\n",
        "        loading_task_model_from_base_state_dict: bool = False,\n",
        "    ):\n",
        "        prefix = self.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "\n",
        "        renamed_keys = {}\n",
        "        key_renaming_mapping = {}\n",
        "        for key in checkpoint_keys:\n",
        "            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n",
        "\n",
        "            if key_mapping is not None:\n",
        "                for pattern, replacement in key_mapping.items():\n",
        "                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n",
        "                    if n_replace > 0:\n",
        "                        has_changed = True\n",
        "                        break\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                new_key = \".\".join([prefix, new_key])\n",
        "            elif loading_base_model_from_task_state_dict:\n",
        "                if not new_key.startswith(_prefix):\n",
        "                    continue\n",
        "                new_key = new_key[len(_prefix) :]\n",
        "\n",
        "            key_renaming_mapping[key] = new_key\n",
        "\n",
        "            if has_changed:\n",
        "                if key.endswith(\"LayerNorm.gamma\"):\n",
        "                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n",
        "                elif key.endswith(\"LayerNorm.beta\"):\n",
        "                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n",
        "\n",
        "        if renamed_keys:\n",
        "            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n",
        "            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n",
        "            for old_key, new_key in renamed_keys.values():\n",
        "                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n",
        "            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n",
        "        return key_renaming_mapping\n",
        "\n",
        "def _find_missing_and_unexpected_keys(\n",
        "    cls,\n",
        "    model: \"PreTrainedModel\",\n",
        "    original_checkpoint_keys: list[str],\n",
        "    checkpoint_keys: list[str],\n",
        "    loading_base_model_from_task_state_dict: bool,\n",
        "    hf_quantizer: Optional[HfQuantizer],\n",
        "    device_map: dict,\n",
        ") -> tuple[list[str], list[str]]:\n",
        "    prefix = model.base_model_prefix\n",
        "\n",
        "    expected_keys = list(model.state_dict().keys())\n",
        "    if hf_quantizer is not None:\n",
        "        expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n",
        "\n",
        "    missing_keys = sorted(set(expected_keys) - set(checkpoint_keys))\n",
        "    unexpected_keys = set(checkpoint_keys) - set(expected_keys)\n",
        "\n",
        "    if loading_base_model_from_task_state_dict:\n",
        "        task_specific_keys = [k for k in original_checkpoint_keys if not k.startswith(f\"{prefix}.\")]\n",
        "        unexpected_keys.update(task_specific_keys)\n",
        "\n",
        "    model_buffers = {n for n, _ in model.named_buffers()}\n",
        "    unexpected_keys = sorted(unexpected_keys - model_buffers)\n",
        "    has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer in model_buffers)\n",
        "    if has_inv_freq_buffers:\n",
        "        unexpected_keys = [k for k in unexpected_keys if \"rotary_emb.inv_freq\" not in k]\n",
        "\n",
        "    tied_params = find_tied_parameters(model)\n",
        "    for group in tied_params:\n",
        "        missing_in_group = [k for k in missing_keys if k in group]\n",
        "        if len(missing_in_group) > 0 and len(missing_in_group) < len(group):\n",
        "            missing_keys = [k for k in missing_keys if k not in missing_in_group]\n",
        "\n",
        "    if hf_quantizer is not None:\n",
        "        missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n",
        "        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n",
        "    if cls._keys_to_ignore_on_load_missing is not None:\n",
        "        for pattern in cls._keys_to_ignore_on_load_missing:\n",
        "            missing_keys = [k for k in missing_keys if re.search(pattern, k) is None]\n",
        "\n",
        "    if cls._keys_to_ignore_on_load_unexpected is not None:\n",
        "        for pattern in cls._keys_to_ignore_on_load_unexpected:\n",
        "            unexpected_keys = [k for k in unexpected_keys if re.search(pattern, k) is None]\n",
        "\n",
        "    return missing_keys, unexpected_keys\n",
        "\n",
        "def _find_mismatched_keys(\n",
        "    model: \"PreTrainedModel\",\n",
        "    state_dict: Optional[dict],\n",
        "    checkpoint_files: Optional[list[str]],\n",
        "    ignore_mismatched_sizes: bool,\n",
        "    keys_to_rename_mapping: dict[str, str],\n",
        "    is_quantized: bool,\n",
        "    weights_only: bool,\n",
        ") -> tuple[list[str], list[tuple[int, int]]]:\n",
        "    if not ignore_mismatched_sizes:\n",
        "        return [], []\n",
        "\n",
        "    if state_dict is not None:\n",
        "        checkpoint_files = [\"\"]\n",
        "\n",
        "    model_state_dict = model.state_dict()\n",
        "    mismatched_keys = []\n",
        "    mismatched_shapes = []\n",
        "    for shard_file in checkpoint_files:\n",
        "        if shard_file != \"\":\n",
        "            state_dict = load_state_dict(\n",
        "                shard_file, is_quantized=is_quantized, map_location=\"meta\", weights_only=weights_only\n",
        "            )\n",
        "\n",
        "        new_state_dict = {keys_to_rename_mapping[k]: v for k, v in state_dict.items() if k in keys_to_rename_mapping}\n",
        "\n",
        "        for key in new_state_dict.keys():\n",
        "            if key in model_state_dict and new_state_dict[key].shape != model_state_dict[key].shape:\n",
        "                if not (\n",
        "                    is_quantized\n",
        "                    and new_state_dict[key].shape[-1] == 1\n",
        "                    and new_state_dict[key].numel() * 2 == model_state_dict[key].numel()\n",
        "                ):\n",
        "                    mismatched_keys.append(key)\n",
        "                    mismatched_shapes.append((new_state_dict[key].shape, model_state_dict[key].shape))\n",
        "\n",
        "    return mismatched_keys, mismatched_shapes\n",
        "\n",
        "def is_fsdp_enabled():\n",
        "    return (\n",
        "        torch.distributed.is_available()\n",
        "        and torch.distributed.is_initialized()\n",
        "        and strtobool(os.environ.get(\"ACCELERATE_USE_FSDP\", \"False\")) == 1\n",
        "        and strtobool(os.environ.get(\"FSDP_CPU_RAM_EFFICIENT_LOADING\", \"False\")) == 1\n",
        "    )\n",
        "\n",
        "def set_initialized_submodules(model, state_dict_keys):\n",
        "    state_dict_keys = set(state_dict_keys)\n",
        "    not_initialized_submodules = {}\n",
        "    for module_name, module in model.named_modules():\n",
        "        if module_name == \"\":\n",
        "            module_keys = set(module.state_dict())\n",
        "        else:\n",
        "            module_keys = {f\"{module_name}.{k}\" for k in module.state_dict()}\n",
        "        if module_keys.issubset(state_dict_keys):\n",
        "            module._is_hf_initialized = True\n",
        "        else:\n",
        "            not_initialized_submodules[module_name] = module\n",
        "    return not_initialized_submodules\n",
        "\n",
        "def load_shard_file(args):\n",
        "    (\n",
        "        shard_file,\n",
        "        state_dict,\n",
        "        disk_only_shard_files,\n",
        "        is_hqq_or_bnb,\n",
        "        is_quantized,\n",
        "        device_map,\n",
        "        hf_quantizer,\n",
        "        key_renaming_mapping,\n",
        "        weights_only,\n",
        "        model_to_load,\n",
        "        expected_keys,\n",
        "        reverse_key_renaming_mapping,\n",
        "        disk_offload_folder,\n",
        "        disk_offload_index,\n",
        "        cpu_offload_folder,\n",
        "        cpu_offload_index,\n",
        "        is_offloaded_safetensors,\n",
        "        keep_in_fp32_regex,\n",
        "        unexpected_keys,\n",
        "        device_mesh,\n",
        "    ) = args\n",
        "\n",
        "    if shard_file in disk_only_shard_files:\n",
        "        return [], disk_offload_index, cpu_offload_index\n",
        "\n",
        "    map_location = \"cpu\"\n",
        "    if (\n",
        "        shard_file.endswith(\".safetensors\")\n",
        "        and not is_hqq_or_bnb\n",
        "        and not (is_deepspeed_zero3_enabled() and not is_quantized)\n",
        "    ):\n",
        "        map_location = \"meta\"\n",
        "    elif (\n",
        "        device_map is not None\n",
        "        and hf_quantizer is not None\n",
        "        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n",
        "        and (\n",
        "            hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n",
        "            or isinstance(hf_quantizer.quantization_config.quant_type, Int4WeightOnlyConfig)\n",
        "        )\n",
        "    ):\n",
        "        map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n",
        "\n",
        "    if shard_file != \"\":\n",
        "        state_dict = load_state_dict(\n",
        "            shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only\n",
        "        )\n",
        "\n",
        "    state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n",
        "\n",
        "    error_msgs = []\n",
        "\n",
        "    if is_deepspeed_zero3_enabled() and not is_quantized:\n",
        "        error_msgs += _load_state_dict_into_zero3_model(model_to_load, state_dict)\n",
        "    elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n",
        "        disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n",
        "            model_to_load,\n",
        "            state_dict,\n",
        "            shard_file,\n",
        "            expected_keys,\n",
        "            reverse_key_renaming_mapping,\n",
        "            device_map=device_map,\n",
        "            disk_offload_folder=disk_offload_folder,\n",
        "            disk_offload_index=disk_offload_index,\n",
        "            cpu_offload_folder=cpu_offload_folder,\n",
        "            cpu_offload_index=cpu_offload_index,\n",
        "            hf_quantizer=hf_quantizer,\n",
        "            is_safetensors=is_offloaded_safetensors,\n",
        "            keep_in_fp32_regex=keep_in_fp32_regex,\n",
        "            unexpected_keys=unexpected_keys,\n",
        "            device_mesh=device_mesh,\n",
        "        )\n",
        "\n",
        "    return error_msgs, disk_offload_index, cpu_offload_index\n",
        "\n",
        "@torch.no_grad()\n",
        "def _load_state_dict_into_meta_model(\n",
        "    model: \"PreTrainedModel\",\n",
        "    state_dict: dict,\n",
        "    shard_file: str,\n",
        "    expected_keys: list[str],\n",
        "    reverse_renaming_mapping: dict[str, str],\n",
        "    device_map: Optional[dict] = None,\n",
        "    disk_offload_folder: Optional[str] = None,\n",
        "    disk_offload_index: Optional[dict] = None,\n",
        "    cpu_offload_folder: Optional[str] = None,\n",
        "    cpu_offload_index: Optional[dict] = None,\n",
        "    hf_quantizer: Optional[HfQuantizer] = None,\n",
        "    is_safetensors: bool = False,\n",
        "    keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "    unexpected_keys: Optional[list[str]] = None,\n",
        "    device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n",
        ") -> tuple[Optional[dict], Optional[dict]]:\n",
        "    tensor_device = \"cpu\"\n",
        "    if device_map is not None and device_map.get(\"\", None) is not None:\n",
        "        if device_map[\"\"] not in (\"cpu\", torch.device(\"cpu\")):\n",
        "            tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n",
        "    if device_map is not None:\n",
        "        device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n",
        "\n",
        "    is_quantized = hf_quantizer is not None\n",
        "    is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "        QuantizationMethod.HQQ,\n",
        "        QuantizationMethod.BITS_AND_BYTES,\n",
        "    }\n",
        "    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb\n",
        "    file_pointer = None\n",
        "    if is_meta_state_dict:\n",
        "        file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n",
        "\n",
        "    for param_name, empty_param in state_dict.items():\n",
        "        if param_name not in expected_keys:\n",
        "            continue\n",
        "\n",
        "        if is_meta_state_dict:\n",
        "            serialized_param_name = reverse_renaming_mapping[param_name]\n",
        "            param = file_pointer.get_slice(serialized_param_name)\n",
        "        else:\n",
        "            param = empty_param.to(tensor_device)\n",
        "\n",
        "        to_contiguous, casting_dtype = _infer_parameter_dtype(\n",
        "            model,\n",
        "            param_name,\n",
        "            empty_param,\n",
        "            keep_in_fp32_regex,\n",
        "            hf_quantizer,\n",
        "        )\n",
        "\n",
        "        if device_mesh is not None:\n",
        "            shard_and_distribute_module(\n",
        "                model,\n",
        "                param,\n",
        "                empty_param,\n",
        "                param_name,\n",
        "                casting_dtype,\n",
        "                to_contiguous,\n",
        "                device_mesh.get_local_rank(),\n",
        "                device_mesh,\n",
        "            )\n",
        "        else:\n",
        "            param = param[...]\n",
        "            if casting_dtype is not None:\n",
        "                param = param.to(casting_dtype)\n",
        "            if to_contiguous:\n",
        "                param = param.contiguous()\n",
        "\n",
        "            if device_map is None:\n",
        "                param_device = \"cpu\"\n",
        "            else:\n",
        "                module_layer = re.search(device_map_regex, param_name)\n",
        "                param_device = device_map[module_layer.group()]\n",
        "\n",
        "            if param_device == \"disk\":\n",
        "                if not is_safetensors:\n",
        "                    disk_offload_index = offload_weight(param, param_name, disk_offload_folder, disk_offload_index)\n",
        "            elif param_device == \"cpu\" and cpu_offload_index is not None:\n",
        "                cpu_offload_index = offload_weight(param, param_name, cpu_offload_folder, cpu_offload_index)\n",
        "            elif (\n",
        "                not is_quantized\n",
        "                or (not hf_quantizer.requires_parameters_quantization)\n",
        "                or (\n",
        "                    not hf_quantizer.check_quantized_param(\n",
        "                        model,\n",
        "                        param,\n",
        "                        param_name,\n",
        "                        state_dict,\n",
        "                        param_device=param_device,\n",
        "                        device_map=device_map,\n",
        "                    )\n",
        "                )\n",
        "            ):\n",
        "                if is_fsdp_enabled():\n",
        "                    param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n",
        "\n",
        "                _load_parameter_into_model(model, param_name, param.to(param_device))\n",
        "\n",
        "            else:\n",
        "                hf_quantizer.create_quantized_param(\n",
        "                    model, param, param_name, param_device, state_dict, unexpected_keys\n",
        "                )\n",
        "                if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n",
        "                    module, param_type = get_module_from_name(model, param_name)\n",
        "                    value = getattr(module, param_type)\n",
        "                    param_to = \"cpu\"\n",
        "                    if is_fsdp_enabled() and not is_local_dist_rank_0():\n",
        "                        param_to = \"meta\"\n",
        "                    val_kwargs = {}\n",
        "                    if hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\":\n",
        "                        val_kwargs[\"requires_grad\"] = False\n",
        "                    value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n",
        "                    setattr(module, param_type, value)\n",
        "\n",
        "    if file_pointer is not None:\n",
        "        file_pointer.__exit__(None, None, None)\n",
        "\n",
        "    return disk_offload_index, cpu_offload_index\n",
        "\n",
        "def _infer_parameter_dtype(\n",
        "    model: \"PreTrainedModel\",\n",
        "    param_name: str,\n",
        "    empty_param: torch.Tensor,\n",
        "    keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "    hf_quantizer: Optional[HfQuantizer] = None,\n",
        ") -> Union[bool, Optional[torch.dtype]]:\n",
        "    old_param = model.get_parameter_or_buffer(param_name)\n",
        "    is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n",
        "    casting_dtype = None\n",
        "    is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n",
        "    if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n",
        "        if keep_in_fp32_regex is not None and keep_in_fp32_regex.search(param_name):\n",
        "            casting_dtype = torch.float32\n",
        "        elif hf_quantizer is not None:\n",
        "            casting_dtype = model.config._pre_quantization_dtype\n",
        "        else:\n",
        "            casting_dtype = old_param.dtype\n",
        "    return old_param is not None and old_param.is_contiguous(), casting_dtype\n",
        "\n",
        "def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor: torch.Tensor):\n",
        "    module, param_type = get_module_from_name(model, param_name)\n",
        "    module.load_state_dict({param_type: tensor}, strict=False, assign=True)\n",
        "\n",
        "def get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n",
        "    return next(parameter.parameters()).device"
      ],
      "metadata": {
        "id": "JmOIHtMGVdJX"
      },
      "execution_count": 759,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ForCausalLMLoss(\n",
        "    logits,\n",
        "    labels,\n",
        "    vocab_size: int,\n",
        "    num_items_in_batch: Optional[torch.Tensor] = None,\n",
        "    ignore_index: int = -100,\n",
        "    shift_labels: Optional[torch.Tensor] = None,\n",
        "    **kwargs,\n",
        ") -> torch.Tensor:\n",
        "    logits = logits.float()\n",
        "\n",
        "    if shift_labels is None:\n",
        "        labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    shift_labels = shift_labels.view(-1)\n",
        "    shift_labels = shift_labels.to(logits.device)\n",
        "    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n",
        "    return loss\n",
        "\n",
        "def ForMaskedLMLoss(\n",
        "    logits: torch.Tensor,\n",
        "    labels: torch.Tensor,\n",
        "    vocab_size: int,\n",
        "    num_items_in_batch: Optional[torch.Tensor] = None,\n",
        "    ignore_index: int = -100,\n",
        "    **kwargs,\n",
        "):\n",
        "    logits = logits.float()\n",
        "\n",
        "    logits = logits.view(-1, vocab_size)\n",
        "    labels = labels.view(-1)\n",
        "\n",
        "    labels = labels.to(logits.device)\n",
        "    loss = fixed_cross_entropy(logits, labels, num_items_in_batch, ignore_index, **kwargs)\n",
        "    return loss\n",
        "\n",
        "LOSS_MAPPING = {\n",
        "    \"ForCausalLM\": ForCausalLMLoss,\n",
        "    \"ForMaskedLM\": ForMaskedLMLoss,\n",
        "}"
      ],
      "metadata": {
        "id": "QvGuCtAt20f4"
      },
      "execution_count": 760,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreTrainedModel(nn.Module):\n",
        "    _auto_class = None\n",
        "    _keep_in_fp32_modules = None\n",
        "    _keep_in_fp32_modules_strict = None\n",
        "    _keys_to_ignore_on_load_missing = None\n",
        "    _keys_to_ignore_on_load_unexpected = None\n",
        "    main_input_name = \"input_ids\"\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return get_parameter_device(self)\n",
        "\n",
        "    def _check_attn_implementation(cls, attn_implementation: Union[dict, str]) -> Union[dict, str]:\n",
        "        if isinstance(attn_implementation, str) and re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", attn_implementation):\n",
        "            repo_id, kernel_name = attn_implementation.split(\":\")\n",
        "            kernel_name = kernel_name.strip()\n",
        "            repo_id = repo_id.strip()\n",
        "\n",
        "            kernel = get_kernel(repo_id)\n",
        "            AttentionInterface.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n",
        "            attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n",
        "        return attn_implementation\n",
        "\n",
        "    def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n",
        "        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n",
        "\n",
        "        for key in self.config.sub_configs.keys():\n",
        "            sub_config = getattr(self.config, key)\n",
        "            curr_attn_implementation = (\n",
        "                requested_attn_implementation\n",
        "                if not isinstance(requested_attn_implementation, dict)\n",
        "                else requested_attn_implementation.get(key, None)\n",
        "            )\n",
        "            if (\n",
        "                sub_config is not None\n",
        "                and sub_config._attn_implementation_internal is None\n",
        "                and curr_attn_implementation is not None\n",
        "            ):\n",
        "                sub_config._attn_implementation_internal = curr_attn_implementation\n",
        "\n",
        "        if requested_attn_implementation == \"flash_attention_3\" and self._flash_attn_3_can_dispatch():\n",
        "            self.config._attn_implementation = \"flash_attention_3\"\n",
        "        if requested_attn_implementation == \"flash_attention_2\" and self._flash_attn_2_can_dispatch():\n",
        "            self.config._attn_implementation = \"flash_attention_2\"\n",
        "        elif requested_attn_implementation == \"flex_attention\" and self._flex_attn_can_dispatch():\n",
        "            self.config._attn_implementation = \"flex_attention\"\n",
        "        elif (\n",
        "            requested_attn_implementation in [None, \"sdpa\"]\n",
        "            and not is_torch_xla_available()\n",
        "            and self._sdpa_can_dispatch(hard_check_only=requested_attn_implementation is not None)\n",
        "        ):\n",
        "            self.config._attn_implementation = \"sdpa\"\n",
        "        elif requested_attn_implementation in AttentionInterface.valid_keys():\n",
        "            self.config._attn_implementation = requested_attn_implementation\n",
        "        elif isinstance(requested_attn_implementation, dict):\n",
        "            self.config._attn_implementation = requested_attn_implementation.get(\"\", None)\n",
        "        else:\n",
        "            self.config._attn_implementation = \"eager\"\n",
        "\n",
        "        self.config._attn_implementation_autoset = True\n",
        "\n",
        "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        if hasattr(config, \"_attn_implementation_internal\") and not getattr(\n",
        "            config, \"_attn_implementation_autoset\", False\n",
        "        ):\n",
        "            self.set_attention_implementation(self.config._attn_implementation_internal)\n",
        "\n",
        "        loss_type = self.__class__.__name__\n",
        "        if loss_type not in LOSS_MAPPING:\n",
        "            loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n",
        "            loss_type = re.findall(loss_groups, self.__class__.__name__)\n",
        "            if len(loss_type) > 0:\n",
        "                loss_type = loss_type[0]\n",
        "            else:\n",
        "                loss_type = None\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        self.name_or_path = config.name_or_path\n",
        "        self.warnings_issued = {}\n",
        "        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n",
        "        self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n",
        "        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n",
        "\n",
        "        self._no_split_modules = self._no_split_modules or []\n",
        "        _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs\n",
        "\n",
        "    @classmethod\n",
        "    @restore_default_torch_dtype\n",
        "    def from_pretrained(\n",
        "        cls: type[SpecificPreTrainedModelType],\n",
        "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
        "        *model_args,\n",
        "        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        ignore_mismatched_sizes: bool = False,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        use_safetensors: Optional[bool] = None,\n",
        "        weights_only: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> SpecificPreTrainedModelType:\n",
        "        state_dict = kwargs.pop(\"state_dict\", None)\n",
        "        from_tf = kwargs.pop(\"from_tf\", False)\n",
        "        from_flax = kwargs.pop(\"from_flax\", False)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
        "        device_map = kwargs.pop(\"device_map\", None)\n",
        "        max_memory = kwargs.pop(\"max_memory\", None)\n",
        "        offload_folder = kwargs.pop(\"offload_folder\", None)\n",
        "        offload_state_dict = kwargs.pop(\"offload_state_dict\", False)\n",
        "        offload_buffers = kwargs.pop(\"offload_buffers\", False)\n",
        "        load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n",
        "        load_in_4bit = kwargs.pop(\"load_in_4bit\", False)\n",
        "        quantization_config = kwargs.pop(\"quantization_config\", None)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "        variant = kwargs.pop(\"variant\", None)\n",
        "        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n",
        "        adapter_name = kwargs.pop(\"adapter_name\", \"default\")\n",
        "        generation_config = kwargs.pop(\"generation_config\", None)\n",
        "        gguf_file = kwargs.pop(\"gguf_file\", None)\n",
        "        tp_plan = kwargs.pop(\"tp_plan\", None)\n",
        "        tp_size = kwargs.pop(\"tp_size\", None)\n",
        "        device_mesh = kwargs.pop(\"device_mesh\", None)\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        use_kernels = kwargs.pop(\"use_kernels\", False)\n",
        "\n",
        "        key_mapping = kwargs.pop(\"key_mapping\", None)\n",
        "        if key_mapping is None and any(\n",
        "            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n",
        "        ):\n",
        "            key_mapping = cls._checkpoint_conversion_mapping\n",
        "\n",
        "        _ = kwargs.pop(\"resume_download\", None)\n",
        "        _ = kwargs.pop(\"mirror\", None)\n",
        "        _ = kwargs.pop(\"_fast_init\", True)\n",
        "        _ = kwargs.pop(\"low_cpu_mem_usage\", None)\n",
        "\n",
        "        if commit_hash is None:\n",
        "            commit_hash = getattr(config, \"_commit_hash\", None)\n",
        "\n",
        "        if is_peft_available():\n",
        "            _adapter_model_path = adapter_kwargs.pop(\"_adapter_model_path\", None)\n",
        "\n",
        "            if _adapter_model_path is None:\n",
        "                _adapter_model_path = find_adapter_config_file(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    cache_dir=cache_dir,\n",
        "                    force_download=force_download,\n",
        "                    proxies=proxies,\n",
        "                    local_files_only=local_files_only,\n",
        "                    _commit_hash=commit_hash,\n",
        "                    **adapter_kwargs,\n",
        "                )\n",
        "            if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):\n",
        "                with open(_adapter_model_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    _adapter_model_path = pretrained_model_name_or_path\n",
        "                    pretrained_model_name_or_path = json.load(f)[\"base_model_name_or_path\"]\n",
        "\n",
        "        if device_map is None and not is_deepspeed_zero3_enabled():\n",
        "            device_in_context = get_torch_context_manager_or_global_device()\n",
        "\n",
        "            device_map = device_in_context\n",
        "\n",
        "        from_pt = not (from_tf | from_flax)\n",
        "\n",
        "        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n",
        "\n",
        "        config = copy.deepcopy(config)\n",
        "\n",
        "        kwarg_attn_imp = kwargs.pop(\"attn_implementation\", None)\n",
        "        if kwarg_attn_imp is not None:\n",
        "            config._attn_implementation = kwarg_attn_imp\n",
        "\n",
        "        model_kwargs = kwargs\n",
        "\n",
        "\n",
        "        transformers_explicit_filename = getattr(config, \"transformers_weights\", None)\n",
        "\n",
        "        pre_quantized = hasattr(config, \"quantization_config\")\n",
        "\n",
        "        hf_quantizer = None\n",
        "\n",
        "        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n",
        "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "            subfolder=subfolder,\n",
        "            variant=variant,\n",
        "            gguf_file=gguf_file,\n",
        "            from_tf=from_tf,\n",
        "            from_flax=from_flax,\n",
        "            use_safetensors=use_safetensors,\n",
        "            cache_dir=cache_dir,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            local_files_only=local_files_only,\n",
        "            token=token,\n",
        "            user_agent=user_agent,\n",
        "            revision=revision,\n",
        "            commit_hash=commit_hash,\n",
        "            is_remote_code=cls._auto_class is not None,\n",
        "            transformers_explicit_filename=transformers_explicit_filename,\n",
        "        )\n",
        "\n",
        "        is_sharded = sharded_metadata is not None\n",
        "        is_quantized = hf_quantizer is not None\n",
        "        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None\n",
        "\n",
        "        if (\n",
        "            is_safetensors_available()\n",
        "            and is_from_file\n",
        "            and not is_sharded\n",
        "            and checkpoint_files[0].endswith(\".safetensors\")\n",
        "        ):\n",
        "            with safe_open(checkpoint_files[0], framework=\"pt\") as f:\n",
        "                metadata = f.metadata()\n",
        "\n",
        "            if metadata is None:\n",
        "                pass\n",
        "            elif metadata.get(\"format\") == \"pt\":\n",
        "                pass\n",
        "            elif metadata.get(\"format\") == \"tf\":\n",
        "                from_tf = True\n",
        "            elif metadata.get(\"format\") == \"flax\":\n",
        "                from_flax = True\n",
        "            elif metadata.get(\"format\") == \"mlx\":\n",
        "                 pass\n",
        "\n",
        "        from_pt = not (from_tf | from_flax)\n",
        "        if from_pt:\n",
        "            config, torch_dtype, dtype_orig = _get_torch_dtype(\n",
        "                cls, torch_dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n",
        "        )\n",
        "\n",
        "        config.name_or_path = pretrained_model_name_or_path\n",
        "\n",
        "        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n",
        "\n",
        "        config = copy.deepcopy(config)\n",
        "        with ContextManagers(model_init_context):\n",
        "            model = cls(config, *model_args, **model_kwargs)\n",
        "\n",
        "        model.tie_weights()\n",
        "\n",
        "        config = model.config\n",
        "\n",
        "        keep_in_fp32_modules = []\n",
        "\n",
        "        keep_in_fp32_regex = None\n",
        "\n",
        "        if from_pt:\n",
        "            if dtype_orig is not None:\n",
        "                torch.set_default_dtype(dtype_orig)\n",
        "\n",
        "            (\n",
        "                model,\n",
        "                missing_keys,\n",
        "                unexpected_keys,\n",
        "                mismatched_keys,\n",
        "                offload_index,\n",
        "                error_msgs,\n",
        "            ) = cls._load_pretrained_model(\n",
        "                model,\n",
        "                state_dict,\n",
        "                checkpoint_files,\n",
        "                pretrained_model_name_or_path,\n",
        "                ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
        "                sharded_metadata=sharded_metadata,\n",
        "                device_map=device_map,\n",
        "                disk_offload_folder=offload_folder,\n",
        "                offload_state_dict=offload_state_dict,\n",
        "                dtype=torch_dtype,\n",
        "                hf_quantizer=hf_quantizer,\n",
        "                keep_in_fp32_regex=keep_in_fp32_regex,\n",
        "                device_mesh=device_mesh,\n",
        "                key_mapping=key_mapping,\n",
        "                weights_only=weights_only,\n",
        "            )\n",
        "\n",
        "        model._tp_size = tp_size\n",
        "        model._device_mesh = device_mesh\n",
        "\n",
        "        model.tie_weights()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        if model.can_generate() and pretrained_model_name_or_path is not None:\n",
        "            repo_loading_kwargs = {\n",
        "                \"cache_dir\": cache_dir,\n",
        "                \"force_download\": force_download,\n",
        "                \"proxies\": proxies,\n",
        "                \"local_files_only\": local_files_only,\n",
        "                \"token\": token,\n",
        "                \"revision\": revision,\n",
        "                \"subfolder\": subfolder,\n",
        "                **kwargs,\n",
        "            }\n",
        "\n",
        "            model.generation_config = GenerationConfig.from_pretrained(\n",
        "                pretrained_model_name_or_path,\n",
        "                    _from_auto=from_auto_class,\n",
        "                    _from_pipeline=from_pipeline,\n",
        "                    **repo_loading_kwargs,\n",
        "                )\n",
        "\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n",
        "\n",
        "        dtype_orig = torch.get_default_dtype()\n",
        "        torch.set_default_dtype(dtype)\n",
        "        return dtype_orig\n",
        "\n",
        "    @classmethod\n",
        "    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n",
        "        if is_deepspeed_zero3_enabled():\n",
        "            import deepspeed\n",
        "\n",
        "            init_contexts = [no_init_weights()]\n",
        "            if not is_quantized and not _is_ds_init_called:\n",
        "                init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n",
        "            elif is_quantized:\n",
        "                init_contexts.extend([init_empty_weights(), set_quantized_state()])\n",
        "        else:\n",
        "            init_contexts = [no_init_weights(), init_empty_weights()]\n",
        "\n",
        "        return init_contexts\n",
        "\n",
        "    def _sdpa_can_dispatch(self, hard_check_only: bool = False) -> bool:\n",
        "        if (\n",
        "            torch.version.hip is not None\n",
        "            and torch.cuda.device_count() > 1\n",
        "            and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n",
        "        ):\n",
        "            torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n",
        "        if not is_torch_sdpa_available() or not self._supports_sdpa or _is_bettertransformer:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    @classmethod\n",
        "    def can_generate(cls) -> bool:\n",
        "        if \"GenerationMixin\" in str(cls.__bases__):\n",
        "            return True\n",
        "        for base in cls.__bases__:\n",
        "            if not hasattr(base, \"can_generate\"):\n",
        "                continue\n",
        "            if \"PreTrainedModel\" not in str(base) and base.can_generate():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def post_init(self):\n",
        "        self.init_weights()\n",
        "        self._backward_compatibility_gradient_checkpointing()\n",
        "\n",
        "        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n",
        "            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n",
        "            unique_module_names = set()\n",
        "            for param in all_parameters:\n",
        "                unique_module_names.update(\n",
        "                    [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n",
        "                )\n",
        "\n",
        "        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n",
        "        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n",
        "        for name, module in self.named_children():\n",
        "            if plan := getattr(module, \"_tp_plan\", None):\n",
        "                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        pass\n",
        "\n",
        "    def init_weights(self):\n",
        "        if self.config.pruned_heads:\n",
        "            self.prune_heads(self.config.pruned_heads)\n",
        "\n",
        "        if _init_weights:\n",
        "            self.initialize_weights()\n",
        "\n",
        "            self.tie_weights()\n",
        "\n",
        "    def _backward_compatibility_gradient_checkpointing(self):\n",
        "        if self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\n",
        "            self.gradient_checkpointing_enable()\n",
        "            delattr(self.config, \"gradient_checkpointing\")\n",
        "\n",
        "    def tie_weights(self):\n",
        "        if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n",
        "            output_embeddings = self.get_output_embeddings()\n",
        "            if output_embeddings is not None:\n",
        "                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
        "\n",
        "        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n",
        "            if hasattr(self, self.base_model_prefix):\n",
        "                self = getattr(self, self.base_model_prefix)\n",
        "            tied_weights = self._tie_encoder_decoder_weights(\n",
        "                self.encoder, self.decoder, self.base_model_prefix, \"encoder\"\n",
        "            )\n",
        "            self._dynamic_tied_weights_keys = tied_weights\n",
        "\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, \"_tie_weights\"):\n",
        "                module._tie_weights()\n",
        "\n",
        "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
        "        if self.config.torchscript:\n",
        "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
        "        else:\n",
        "            output_embeddings.weight = input_embeddings.weight\n",
        "\n",
        "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
        "            output_embeddings.bias.data = nn.functional.pad(\n",
        "                output_embeddings.bias.data,\n",
        "                (\n",
        "                    0,\n",
        "                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
        "                ),\n",
        "                \"constant\",\n",
        "                0,\n",
        "            )\n",
        "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
        "            output_embeddings.out_features = input_embeddings.num_embeddings\n",
        "\n",
        "    @classmethod\n",
        "    def _load_pretrained_model(\n",
        "        cls,\n",
        "        model: \"PreTrainedModel\",\n",
        "        state_dict: Optional[dict],\n",
        "        checkpoint_files: Optional[list[str]],\n",
        "        pretrained_model_name_or_path: Optional[str],\n",
        "        ignore_mismatched_sizes: bool = False,\n",
        "        sharded_metadata: Optional[dict] = None,\n",
        "        device_map: Optional[dict] = None,\n",
        "        disk_offload_folder: Optional[str] = None,\n",
        "        offload_state_dict: Optional[bool] = None,\n",
        "        dtype: Optional[torch.dtype] = None,\n",
        "        hf_quantizer: Optional[HfQuantizer] = None,\n",
        "        keep_in_fp32_regex: Optional[re.Pattern] = None,\n",
        "        device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        weights_only: bool = True,\n",
        "    ):\n",
        "        is_quantized = hf_quantizer is not None\n",
        "        is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "            QuantizationMethod.HQQ,\n",
        "            QuantizationMethod.QUARK,\n",
        "        }\n",
        "        is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n",
        "            QuantizationMethod.HQQ,\n",
        "            QuantizationMethod.BITS_AND_BYTES,\n",
        "        }\n",
        "\n",
        "        if sharded_metadata is not None:\n",
        "            original_checkpoint_keys = sharded_metadata[\"all_checkpoint_keys\"]\n",
        "        elif state_dict is not None:\n",
        "            original_checkpoint_keys = list(state_dict.keys())\n",
        "        else:\n",
        "            original_checkpoint_keys = list(\n",
        "                load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
        "            )\n",
        "\n",
        "        prefix = model.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "        has_prefix_module = any(s.startswith(prefix) for s in original_checkpoint_keys) if len(prefix) > 0 else False\n",
        "        expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n",
        "        loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n",
        "        loading_base_model_from_task_state_dict = has_prefix_module and not expects_prefix_module\n",
        "\n",
        "        key_renaming_mapping = model._get_key_renaming_mapping(\n",
        "            original_checkpoint_keys,\n",
        "            key_mapping,\n",
        "            loading_base_model_from_task_state_dict,\n",
        "            loading_task_model_from_base_state_dict,\n",
        "        )\n",
        "        checkpoint_keys = list(key_renaming_mapping.values())\n",
        "\n",
        "        missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(\n",
        "            cls,\n",
        "            model,\n",
        "            original_checkpoint_keys,\n",
        "            checkpoint_keys,\n",
        "            loading_base_model_from_task_state_dict,\n",
        "            hf_quantizer,\n",
        "            device_map,\n",
        "        )\n",
        "        mismatched_keys, mismatched_shapes = _find_mismatched_keys(\n",
        "            model,\n",
        "            state_dict,\n",
        "            checkpoint_files,\n",
        "            ignore_mismatched_sizes,\n",
        "            key_renaming_mapping,\n",
        "            is_quantized,\n",
        "            weights_only,\n",
        "        )\n",
        "\n",
        "        key_renaming_mapping = {k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys}\n",
        "        checkpoint_keys = list(key_renaming_mapping.values())\n",
        "\n",
        "        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, unexpected_keys, dtype, hf_quantizer)\n",
        "\n",
        "        model._initialize_missing_keys(checkpoint_keys, ignore_mismatched_sizes, is_quantized)\n",
        "\n",
        "        if keep_in_fp32_regex is not None:\n",
        "            for name, param in model.named_parameters():\n",
        "                if keep_in_fp32_regex.search(name):\n",
        "                    param.data = param.data.to(torch.float32)\n",
        "\n",
        "        model_to_load = model\n",
        "        if loading_task_model_from_base_state_dict:\n",
        "            model_to_load = getattr(model, prefix)\n",
        "            key_renaming_mapping = {k: v[len(_prefix) :] for k, v in key_renaming_mapping.items()}\n",
        "            checkpoint_keys = list(key_renaming_mapping.values())\n",
        "            if device_map is not None:\n",
        "                device_map = {k[len(_prefix) :] if k.startswith(_prefix) else k: v for k, v in device_map.items()}\n",
        "            task_specific_expected_keys = [s for s in model.state_dict().keys() if not s.startswith(_prefix)]\n",
        "            base_model_expected_keys = list(model_to_load.state_dict().keys())\n",
        "        reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n",
        "\n",
        "        is_offloaded_safetensors = False\n",
        "        disk_offload_index = None\n",
        "        disk_only_shard_files = []\n",
        "        if device_map is not None and \"disk\" in device_map.values():\n",
        "            if offload_state_dict is None:\n",
        "                offload_state_dict = True\n",
        "            if disk_offload_folder is not None:\n",
        "                os.makedirs(disk_offload_folder, exist_ok=True)\n",
        "            is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n",
        "            if is_offloaded_safetensors:\n",
        "                param_device_map = expand_device_map(device_map, checkpoint_keys)\n",
        "                str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n",
        "                if sharded_metadata is None:\n",
        "                    weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n",
        "                else:\n",
        "                    folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n",
        "                    weight_map = {\n",
        "                        key_renaming_mapping[k]: v\n",
        "                        for k, v in sharded_metadata[\"weight_map\"].items()\n",
        "                        if k in key_renaming_mapping\n",
        "                    }\n",
        "                    weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n",
        "                    disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n",
        "                disk_offload_index = {\n",
        "                    name: {\n",
        "                        \"safetensors_file\": file,\n",
        "                        \"weight_name\": reverse_key_renaming_mapping[name],\n",
        "                        \"dtype\": str_dtype,\n",
        "                    }\n",
        "                    for name, file in weight_map.items()\n",
        "                    if param_device_map[name] == \"disk\"\n",
        "                }\n",
        "            else:\n",
        "                disk_offload_index = {}\n",
        "\n",
        "        cpu_offload_folder = None\n",
        "        cpu_offload_index = None\n",
        "        if offload_state_dict:\n",
        "            cpu_offload_folder = tempfile.mkdtemp()\n",
        "            cpu_offload_index = {}\n",
        "\n",
        "        elif state_dict is not None:\n",
        "            checkpoint_files = [\"\"]\n",
        "\n",
        "        expected_keys = list(model_to_load.state_dict().keys())\n",
        "        if hf_quantizer is not None:\n",
        "            expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n",
        "\n",
        "        if device_map is not None and not is_hqq_or_quark:\n",
        "            expanded_device_map = expand_device_map(device_map, expected_keys)\n",
        "            caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)\n",
        "\n",
        "        args_list = [\n",
        "            (\n",
        "                shard_file,\n",
        "                state_dict,\n",
        "                disk_only_shard_files,\n",
        "                is_hqq_or_bnb,\n",
        "                is_quantized,\n",
        "                device_map,\n",
        "                hf_quantizer,\n",
        "                key_renaming_mapping,\n",
        "                weights_only,\n",
        "                model_to_load,\n",
        "                expected_keys,\n",
        "                reverse_key_renaming_mapping,\n",
        "                disk_offload_folder,\n",
        "                disk_offload_index,\n",
        "                cpu_offload_folder,\n",
        "                cpu_offload_index,\n",
        "                is_offloaded_safetensors,\n",
        "                keep_in_fp32_regex,\n",
        "                unexpected_keys,\n",
        "                device_mesh,\n",
        "            )\n",
        "            for shard_file in checkpoint_files\n",
        "        ]\n",
        "\n",
        "        error_msgs = []\n",
        "\n",
        "        if (\n",
        "            os.environ.get(\"HF_ENABLE_PARALLEL_LOADING\", \"\").upper() in ENV_VARS_TRUE_VALUES\n",
        "            and not is_deepspeed_zero3_enabled()\n",
        "        ):\n",
        "            _error_msgs, disk_offload_index, cpu_offload_index = load_shard_files_with_threadpool(args_list)\n",
        "            error_msgs += _error_msgs\n",
        "        else:\n",
        "            for args in args_list:\n",
        "                _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n",
        "                error_msgs += _error_msgs\n",
        "\n",
        "        if disk_offload_index is not None and len(disk_offload_index) > 0:\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                prefix = cls.base_model_prefix\n",
        "                if not is_offloaded_safetensors:\n",
        "                    for weight_name in disk_offload_index:\n",
        "                        shutil.move(\n",
        "                            os.path.join(disk_offload_folder, f\"{weight_name}.dat\"),\n",
        "                            os.path.join(disk_offload_folder, f\"{prefix}.{weight_name}.dat\"),\n",
        "                        )\n",
        "                disk_offload_index = {f\"{prefix}.{key}\": value for key, value in disk_offload_index.items()}\n",
        "            if not is_offloaded_safetensors:\n",
        "                save_offload_index(disk_offload_index, disk_offload_folder)\n",
        "                disk_offload_index = None\n",
        "        if offload_state_dict:\n",
        "            load_offloaded_weights(model_to_load, cpu_offload_index, cpu_offload_folder)\n",
        "            shutil.rmtree(cpu_offload_folder)\n",
        "\n",
        "        if hf_quantizer is not None:\n",
        "            missing_keys = hf_quantizer.update_missing_keys_after_loading(model_to_load, missing_keys, prefix)\n",
        "\n",
        "        if device_mesh is not None:\n",
        "            tp_device = list(device_map.values())[0]\n",
        "            for buffer in model.buffers():\n",
        "                if buffer.device != tp_device:\n",
        "                    buffer.data = buffer.to(tp_device)\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                parameters_to_initialize = {\n",
        "                    name: param for name, param in model.named_parameters() if not name.startswith(prefix)\n",
        "                }\n",
        "                for name, param in parameters_to_initialize.items():\n",
        "                    if param.device.type == \"meta\":\n",
        "                        continue\n",
        "                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param, keep_in_fp32_regex)\n",
        "                    shard_and_distribute_module(\n",
        "                        model,\n",
        "                        param.to(tp_device),\n",
        "                        param,\n",
        "                        name,\n",
        "                        casting_dtype,\n",
        "                        to_contiguous,\n",
        "                        device_mesh.get_local_rank(),\n",
        "                        device_mesh,\n",
        "                    )\n",
        "\n",
        "        if len(error_msgs) > 0:\n",
        "            error_msg = \"\\n\\t\".join(error_msgs)\n",
        "            if \"size mismatch\" in error_msg:\n",
        "                error_msg += (\n",
        "                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n",
        "                )\n",
        "            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n",
        "        if len(unexpected_keys) > 0:\n",
        "            archs = [] if model.config.architectures is None else model.config.architectures\n",
        "        if len(mismatched_keys) > 0:\n",
        "            mismatched_warning = \"\\n\".join(\n",
        "                [\n",
        "                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n",
        "                    for key, (shape1, shape2) in zip(mismatched_keys, mismatched_shapes)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n",
        "\n",
        "    def _get_key_renaming_mapping(\n",
        "        self,\n",
        "        checkpoint_keys: list[str],\n",
        "        key_mapping: Optional[dict[str, str]] = None,\n",
        "        loading_base_model_from_task_state_dict: bool = False,\n",
        "        loading_task_model_from_base_state_dict: bool = False,\n",
        "    ):\n",
        "        prefix = self.base_model_prefix\n",
        "        _prefix = f\"{prefix}.\"\n",
        "\n",
        "        renamed_keys = {}\n",
        "        key_renaming_mapping = {}\n",
        "        for key in checkpoint_keys:\n",
        "            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n",
        "\n",
        "            if key_mapping is not None:\n",
        "                for pattern, replacement in key_mapping.items():\n",
        "                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n",
        "                    if n_replace > 0:\n",
        "                        has_changed = True\n",
        "                        break\n",
        "\n",
        "            if loading_task_model_from_base_state_dict:\n",
        "                new_key = \".\".join([prefix, new_key])\n",
        "            elif loading_base_model_from_task_state_dict:\n",
        "                if not new_key.startswith(_prefix):\n",
        "                    continue\n",
        "                new_key = new_key[len(_prefix) :]\n",
        "\n",
        "            key_renaming_mapping[key] = new_key\n",
        "\n",
        "            if has_changed:\n",
        "                if key.endswith(\"LayerNorm.gamma\"):\n",
        "                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n",
        "                elif key.endswith(\"LayerNorm.beta\"):\n",
        "                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n",
        "\n",
        "        if renamed_keys:\n",
        "            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n",
        "            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n",
        "            for old_key, new_key in renamed_keys.values():\n",
        "                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n",
        "            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n",
        "\n",
        "        return key_renaming_mapping\n",
        "\n",
        "    @staticmethod\n",
        "    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:\n",
        "        if key.endswith(\"LayerNorm.beta\"):\n",
        "            return key.replace(\"LayerNorm.beta\", \"LayerNorm.bias\"), True\n",
        "        if key.endswith(\"LayerNorm.gamma\"):\n",
        "            return key.replace(\"LayerNorm.gamma\", \"LayerNorm.weight\"), True\n",
        "\n",
        "        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n",
        "            if key.endswith(\"weight_g\"):\n",
        "                return key.replace(\"weight_g\", \"parametrizations.weight.original0\"), True\n",
        "            if key.endswith(\"weight_v\"):\n",
        "                return key.replace(\"weight_v\", \"parametrizations.weight.original1\"), True\n",
        "        else:\n",
        "            if key.endswith(\"parametrizations.weight.original0\"):\n",
        "                return key.replace(\"parametrizations.weight.original0\", \"weight_g\"), True\n",
        "            if key.endswith(\"parametrizations.weight.original1\"):\n",
        "                return key.replace(\"parametrizations.weight.original1\", \"weight_v\"), True\n",
        "\n",
        "        return key, False\n",
        "\n",
        "    def _move_missing_keys_from_meta_to_cpu(\n",
        "        self,\n",
        "        missing_keys: list[str],\n",
        "        unexpected_keys: list[str],\n",
        "        dtype: Optional[torch.dtype],\n",
        "        hf_quantizer: Optional[HfQuantizer],\n",
        "    ) -> \"PreTrainedModel\":\n",
        "        is_quantized = hf_quantizer is not None\n",
        "\n",
        "        if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n",
        "            for key, param in self.named_parameters():\n",
        "                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n",
        "                _load_parameter_into_model(self, key, value)\n",
        "            return\n",
        "\n",
        "        model_state_dict = self.state_dict()\n",
        "        for key in missing_keys:\n",
        "            param = model_state_dict[key]\n",
        "            if param.device == torch.device(\"meta\"):\n",
        "                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n",
        "                if (\n",
        "                    not is_quantized\n",
        "                    or (getattr(hf_quantizer, \"requires_parameters_quantization\", False))\n",
        "                    or not hf_quantizer.check_quantized_param(self, param_value=value, param_name=key, state_dict={})\n",
        "                ):\n",
        "                    _load_parameter_into_model(self, key, value)\n",
        "                else:\n",
        "                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\", model_state_dict, unexpected_keys)\n",
        "\n",
        "    def _initialize_missing_keys(\n",
        "        self,\n",
        "        loaded_keys: list[str],\n",
        "        ignore_mismatched_sizes: bool,\n",
        "        is_quantized: bool,\n",
        "    ) -> \"PreTrainedModel\":\n",
        "        if not ignore_mismatched_sizes:\n",
        "            not_initialized_submodules = set_initialized_submodules(self, loaded_keys)\n",
        "            if (\n",
        "                hasattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\")\n",
        "                and self.config.get_text_config(decoder=True).tie_word_embeddings\n",
        "            ):\n",
        "                output_embeddings = self.get_output_embeddings()\n",
        "                if output_embeddings is not None:\n",
        "                    if not hasattr(output_embeddings, \"bias\") or output_embeddings.bias is None:\n",
        "                        output_embeddings._is_hf_initialized = True\n",
        "        else:\n",
        "            not_initialized_submodules = dict(self.named_modules())\n",
        "        if is_deepspeed_zero3_enabled() and not is_quantized:\n",
        "            import deepspeed\n",
        "\n",
        "            not_initialized_parameters = list(\n",
        "                set(\n",
        "                    itertools.chain.from_iterable(\n",
        "                        submodule.parameters(recurse=False) for submodule in not_initialized_submodules.values()\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n",
        "                self.initialize_weights()\n",
        "        else:\n",
        "            self.initialize_weights()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def initialize_weights(self):\n",
        "        if not hasattr(torch.nn.Module, \"smart_apply\"):\n",
        "            def smart_apply(self, fn):\n",
        "                for module in self.children():\n",
        "                    if isinstance(module, PreTrainedModel):\n",
        "                        module.smart_apply(module._initialize_weights)\n",
        "                    else:\n",
        "                        module.smart_apply(fn)\n",
        "                fn(self)\n",
        "                return self\n",
        "\n",
        "            torch.nn.Module.smart_apply = smart_apply\n",
        "\n",
        "        self.smart_apply(self._initialize_weights)\n",
        "\n",
        "    def _initialize_weights(self, module):\n",
        "        if getattr(module, \"_is_hf_initialized\", False):\n",
        "            return\n",
        "        self._init_weights(module)\n",
        "        module._is_hf_initialized = True\n",
        "\n",
        "    def get_parameter_or_buffer(self, target: str):\n",
        "        return self.get_parameter(target)\n",
        "        return self.get_buffer(target)\n",
        "        module, param_name = get_module_from_name(self, target)\n",
        "        if (\n",
        "            param_name == \"_extra_state\"\n",
        "            and getattr(module.__class__, \"get_extra_state\", torch.nn.Module.get_extra_state)\n",
        "            is not torch.nn.Module.get_extra_state\n",
        "        ):\n",
        "            return module.get_extra_state()\n",
        "\n",
        "        raise AttributeError(f\"`{target}` is neither a parameter, buffer, nor extra state.\")"
      ],
      "metadata": {
        "id": "C8J6-Q65yDkx"
      },
      "execution_count": 761,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "NA_1qOCvkv_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TOKENIZER_MAPPING_NAMES = OrderedDict[str, tuple[Optional[str], Optional[str]]](\n",
        "[\n",
        "    (\"colqwen2\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "    (\"internvl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "    (\n",
        "        \"qwen2\",\n",
        "        (\n",
        "            \"Qwen2Tokenizer\",\n",
        "            \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "        ),\n",
        "    ),\n",
        "          (\"qwen2_5_omni\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\"qwen2_5_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\"qwen2_audio\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\n",
        "            \"qwen2_moe\",\n",
        "            (\n",
        "                \"Qwen2Tokenizer\",\n",
        "                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "            ),\n",
        "        ),\n",
        "        (\"qwen2_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n",
        "        (\n",
        "            \"qwen3\",\n",
        "            (\n",
        "                \"Qwen2Tokenizer\",\n",
        "                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"qwen3_moe\",\n",
        "            (\n",
        "                \"Qwen2Tokenizer\",\n",
        "                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n",
        "            ),\n",
        "        ),\n",
        "])\n",
        "\n",
        "_CallableT = TypeVar(\"_CallableT\", bound=Callable[..., Any])\n",
        "\n",
        "def _compute_linear_scaling_rope_parameters(\n",
        "    config: Optional[PretrainedConfig] = None,\n",
        "    device: Optional[\"torch.device\"] = None,\n",
        "    seq_len: Optional[int] = None,\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    factor = config.rope_scaling[\"factor\"]\n",
        "\n",
        "    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n",
        "\n",
        "    inv_freq /= factor\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "\n",
        "def _compute_dynamic_ntk_parameters(\n",
        "    config: Optional[PretrainedConfig] = None,\n",
        "    device: Optional[\"torch.device\"] = None,\n",
        "    seq_len: Optional[int] = None,\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "    max_position_embeddings = config.max_position_embeddings\n",
        "    factor = config.rope_scaling[\"factor\"]\n",
        "\n",
        "    attention_factor = 1.0\n",
        "\n",
        "    if seq_len is None:\n",
        "        seq_len = max_position_embeddings\n",
        "    elif isinstance(seq_len, torch.Tensor):\n",
        "        seq_len = torch.maximum(\n",
        "            seq_len,\n",
        "            torch.tensor(max_position_embeddings, dtype=seq_len.dtype, device=seq_len.device),\n",
        "        )\n",
        "    else:\n",
        "        seq_len = max(seq_len, max_position_embeddings)\n",
        "\n",
        "    base = base * ((factor * seq_len / max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))\n",
        "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "\n",
        "def _compute_yarn_parameters(\n",
        "    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "    factor = config.rope_scaling[\"factor\"]\n",
        "    attention_factor = config.rope_scaling.get(\"attention_factor\")\n",
        "    mscale = config.rope_scaling.get(\"mscale\")\n",
        "    mscale_all_dim = config.rope_scaling.get(\"mscale_all_dim\")\n",
        "\n",
        "    if \"original_max_position_embeddings\" in config.rope_scaling:\n",
        "        original_max_position_embeddings = config.rope_scaling[\"original_max_position_embeddings\"]\n",
        "        factor = config.max_position_embeddings / original_max_position_embeddings\n",
        "    else:\n",
        "        original_max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "    def get_mscale(scale, mscale=1):\n",
        "        if scale <= 1:\n",
        "            return 1.0\n",
        "        return 0.1 * mscale * math.log(scale) + 1.0\n",
        "\n",
        "    if attention_factor is None:\n",
        "        if mscale and mscale_all_dim:\n",
        "            attention_factor = float(get_mscale(factor, mscale) / get_mscale(factor, mscale_all_dim))\n",
        "        else:\n",
        "            attention_factor = get_mscale(factor)\n",
        "\n",
        "    beta_fast = config.rope_scaling.get(\"beta_fast\") or 32\n",
        "    beta_slow = config.rope_scaling.get(\"beta_slow\") or 1\n",
        "\n",
        "    def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n",
        "        return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (2 * math.log(base))\n",
        "\n",
        "    def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings):\n",
        "        low = math.floor(find_correction_dim(low_rot, dim, base, max_position_embeddings))\n",
        "        high = math.ceil(find_correction_dim(high_rot, dim, base, max_position_embeddings))\n",
        "        return max(low, 0), min(high, dim - 1)\n",
        "\n",
        "    def linear_ramp_factor(min, max, dim):\n",
        "        if min == max:\n",
        "            max += 0.001\n",
        "\n",
        "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
        "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
        "        return ramp_func\n",
        "\n",
        "    pos_freqs = base ** (torch.arange(0, dim, 2).to(device=device, dtype=torch.float) / dim)\n",
        "    inv_freq_extrapolation = 1.0 / pos_freqs\n",
        "    inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n",
        "\n",
        "    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings)\n",
        "\n",
        "    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(device=device, dtype=torch.float)\n",
        "    inv_freq = (\n",
        "        inv_freq_interpolation * (1 - inv_freq_extrapolation_factor)\n",
        "        + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
        "    )\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "def _compute_longrope_parameters(\n",
        "    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "    long_factor = config.rope_scaling[\"long_factor\"]\n",
        "    short_factor = config.rope_scaling[\"short_factor\"]\n",
        "    factor = config.rope_scaling.get(\"factor\")\n",
        "    attention_factor = config.rope_scaling.get(\"attention_factor\")\n",
        "\n",
        "    if hasattr(config, \"original_max_position_embeddings\"):\n",
        "        original_max_position_embeddings = config.original_max_position_embeddings\n",
        "        factor = config.max_position_embeddings / config.original_max_position_embeddings\n",
        "    else:\n",
        "        original_max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "    if attention_factor is None:\n",
        "        if factor <= 1.0:\n",
        "            attention_factor = 1.0\n",
        "        else:\n",
        "            attention_factor = math.sqrt(1 + math.log(factor) / math.log(original_max_position_embeddings))\n",
        "\n",
        "    if seq_len and seq_len > original_max_position_embeddings:\n",
        "        ext_factors = torch.tensor(long_factor, dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        ext_factors = torch.tensor(short_factor, dtype=torch.float32, device=device)\n",
        "    inv_freq_shape = torch.arange(0, dim, 2, dtype=torch.int64, device=device).float() / dim\n",
        "    inv_freq = 1.0 / (ext_factors * base**inv_freq_shape)\n",
        "\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "def _compute_llama3_parameters(\n",
        "    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n",
        "\n",
        "    factor = config.rope_scaling[\"factor\"]\n",
        "    low_freq_factor = config.rope_scaling[\"low_freq_factor\"]\n",
        "    high_freq_factor = config.rope_scaling[\"high_freq_factor\"]\n",
        "    old_context_len = config.rope_scaling[\"original_max_position_embeddings\"]\n",
        "\n",
        "    low_freq_wavelen = old_context_len / low_freq_factor\n",
        "    high_freq_wavelen = old_context_len / high_freq_factor\n",
        "\n",
        "    wavelen = 2 * math.pi / inv_freq\n",
        "\n",
        "    inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)\n",
        "\n",
        "    smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n",
        "    smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama\n",
        "    is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)\n",
        "    inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
        "\n",
        "    return inv_freq_llama, attention_factor\n",
        "\n",
        "\n",
        "def _compute_default_rope_parameters(\n",
        "    config: Optional[PretrainedConfig] = None,\n",
        "    device: Optional[\"torch.device\"] = None,\n",
        "    seq_len: Optional[int] = None,\n",
        ") -> tuple[\"torch.Tensor\", float]:\n",
        "    base = config.rope_theta\n",
        "    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
        "    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n",
        "    dim = int(head_dim * partial_rotary_factor)\n",
        "\n",
        "    attention_factor = 1.0\n",
        "\n",
        "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n",
        "    return inv_freq, attention_factor\n",
        "\n",
        "    rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", \"default\"))\n",
        "    validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n",
        "    if validation_fn is not None:\n",
        "        validation_fn(config, ignore_keys=ignore_keys)\n",
        "\n",
        "ROPE_INIT_FUNCTIONS = {\n",
        "    \"default\": _compute_default_rope_parameters,\n",
        "    \"linear\": _compute_linear_scaling_rope_parameters,\n",
        "    \"dynamic\": _compute_dynamic_ntk_parameters,\n",
        "    \"yarn\": _compute_yarn_parameters,\n",
        "    \"longrope\": _compute_longrope_parameters,\n",
        "    \"llama3\": _compute_llama3_parameters,\n",
        "}"
      ],
      "metadata": {
        "id": "dZvtP16KEzSb"
      },
      "execution_count": 762,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CONFIG_MAPPING_NAMES = OrderedDict[str, str]([])\n",
        "\n",
        "TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n",
        "\n",
        "_T = TypeVar(\"_T\")\n",
        "_LazyAutoMappingValue = tuple[Union[type[Any], None], Union[type[Any], None]]"
      ],
      "metadata": {
        "id": "sgAPBwBd0dVx"
      },
      "execution_count": 763,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 764,
      "metadata": {
        "id": "ImrU5Z6qpOJy"
      },
      "outputs": [],
      "source": [
        "class _LazyAutoMapping(OrderedDict[type[PretrainedConfig], _LazyAutoMappingValue]):\n",
        "    def __init__(self, config_mapping, model_mapping) -> None:\n",
        "        self._config_mapping = config_mapping\n",
        "        self._reverse_config_mapping = {v: k for k, v in config_mapping.items()}\n",
        "        self._model_mapping = model_mapping\n",
        "        self._model_mapping._model_mapping = self\n",
        "        self._extra_content = {}\n",
        "        self._modules = {}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        common_keys = set(self._config_mapping.keys()).intersection(self._model_mapping.keys())\n",
        "        return len(common_keys) + len(self._extra_content)\n",
        "\n",
        "    def __getitem__(self, key: type[PretrainedConfig]) -> _LazyAutoMappingValue:\n",
        "        if key in self._extra_content:\n",
        "            return self._extra_content[key]\n",
        "        model_type = self._reverse_config_mapping[key.__name__]\n",
        "        if model_type in self._model_mapping:\n",
        "            model_name = self._model_mapping[model_type]\n",
        "            return self._load_attr_from_module(model_type, model_name)\n",
        "\n",
        "        model_types = [k for k, v in self._config_mapping.items() if v == key.__name__]\n",
        "        for mtype in model_types:\n",
        "            if mtype in self._model_mapping:\n",
        "                model_name = self._model_mapping[mtype]\n",
        "                return self._load_attr_from_module(mtype, model_name)\n",
        "        raise KeyError(key)\n",
        "\n",
        "    def _load_attr_from_module(self, model_type, attr):\n",
        "        module_name = model_type_to_module_name(model_type)\n",
        "        if module_name not in self._modules:\n",
        "            self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n",
        "        return getattribute_from_module(self._modules[module_name], attr)\n",
        "\n",
        "    def keys(self) -> list[type[PretrainedConfig]]:\n",
        "        mapping_keys = [\n",
        "            self._load_attr_from_module(key, name)\n",
        "            for key, name in self._config_mapping.items()\n",
        "            if key in self._model_mapping.keys()\n",
        "        ]\n",
        "        return mapping_keys + list(self._extra_content.keys())\n",
        "\n",
        "    def get(self, key: type[PretrainedConfig], default: _T) -> Union[_LazyAutoMappingValue, _T]:\n",
        "        return self.__getitem__(key)\n",
        "\n",
        "    def values(self) -> list[_LazyAutoMappingValue]:\n",
        "        mapping_values = [\n",
        "            self._load_attr_from_module(key, name)\n",
        "            for key, name in self._model_mapping.items()\n",
        "            if key in self._config_mapping.keys()\n",
        "        ]\n",
        "        return mapping_values + list(self._extra_content.values())\n",
        "\n",
        "    def items(self) -> list[tuple[type[PretrainedConfig], _LazyAutoMappingValue]]:\n",
        "        mapping_items = [\n",
        "            (\n",
        "                self._load_attr_from_module(key, self._config_mapping[key]),\n",
        "                self._load_attr_from_module(key, self._model_mapping[key]),\n",
        "            )\n",
        "            for key in self._model_mapping.keys()\n",
        "            if key in self._config_mapping.keys()\n",
        "        ]\n",
        "        return mapping_items + list(self._extra_content.items())\n",
        "\n",
        "    def __iter__(self) -> Iterator[type[PretrainedConfig]]:\n",
        "        return iter(self.keys())\n",
        "\n",
        "    def register(self, key: type[PretrainedConfig], value: _LazyAutoMappingValue, exist_ok=False) -> None:\n",
        "        if hasattr(key, \"__name__\") and key.__name__ in self._reverse_config_mapping:\n",
        "            model_type = self._reverse_config_mapping[key.__name__]\n",
        "\n",
        "        self._extra_content[key] = value\n",
        "\n",
        "TOKENIZER_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TOKENIZER_MAPPING_NAMES)\n",
        "\n",
        "class GELUActivation(nn.Module):\n",
        "    def __init__(self, use_gelu_python: bool = False):\n",
        "        super().__init__()\n",
        "        if use_gelu_python:\n",
        "            self.act = self._gelu_python\n",
        "        else:\n",
        "            self.act = nn.functional.gelu\n",
        "\n",
        "    def _gelu_python(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input * 0.5 * (1.0 + torch.erf(input / math.sqrt(2.0)))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(input)\n",
        "\n",
        "class ClassInstantier(OrderedDict):\n",
        "    def __getitem__(self, key):\n",
        "        content = super().__getitem__(key)\n",
        "        cls, kwargs = content if isinstance(content, tuple) else (content, {})\n",
        "        return cls(**kwargs)\n",
        "\n",
        "class ClippedGELUActivation(nn.Module):\n",
        "    def __init__(self, min: float, max: float):\n",
        "\n",
        "        super().__init__()\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.clip(gelu(x), self.min, self.max)\n",
        "\n",
        "class FastGELUActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * input * (1.0 + torch.tanh(input * 0.7978845608 * (1.0 + 0.044715 * input * input)))\n",
        "\n",
        "class NewGELUActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
        "\n",
        "class PytorchGELUTanh(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return nn.functional.gelu(input, approximate=\"tanh\")\n",
        "\n",
        "class AccurateGELUActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.precomputed_constant = math.sqrt(2 / math.pi)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * input * (1 + torch.tanh(self.precomputed_constant * (input + 0.044715 * torch.pow(input, 3))))\n",
        "\n",
        "class LaplaceActivation(nn.Module):\n",
        "    def forward(self, input, mu=0.707107, sigma=0.282095):\n",
        "        input = (input - mu).div(sigma * math.sqrt(2.0))\n",
        "        return 0.5 * (1.0 + torch.erf(input))\n",
        "\n",
        "class LinearActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input\n",
        "\n",
        "class MishActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.act = nn.functional.mish\n",
        "\n",
        "    def _mish_python(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input * torch.tanh(nn.functional.softplus(input))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(input)\n",
        "\n",
        "class QuickGELUActivation(nn.Module):\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return input * torch.sigmoid(1.702 * input)\n",
        "\n",
        "class ReLUSquaredActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        relu_applied = nn.functional.relu(input)\n",
        "        squared = torch.square(relu_applied)\n",
        "        return squared\n",
        "\n",
        "class AutoTokenizer:\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "        if use_auth_token is not None:\n",
        "            kwargs[\"token\"] = use_auth_token\n",
        "\n",
        "        config = kwargs.pop(\"config\", None)\n",
        "        kwargs[\"_from_auto\"] = True\n",
        "\n",
        "        use_fast = kwargs.pop(\"use_fast\", True)\n",
        "        tokenizer_type = kwargs.pop(\"tokenizer_type\", None)\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        gguf_file = kwargs.get(\"gguf_file\", None)\n",
        "\n",
        "        if tokenizer_type is not None:\n",
        "            tokenizer_class = None\n",
        "            tokenizer_class_tuple = TOKENIZER_MAPPING_NAMES.get(tokenizer_type, None)\n",
        "\n",
        "            tokenizer_class_name, tokenizer_fast_class_name = tokenizer_class_tuple\n",
        "\n",
        "            if use_fast:\n",
        "                if tokenizer_fast_class_name is not None:\n",
        "                    tokenizer_class = tokenizer_class_from_name(tokenizer_fast_class_name)\n",
        "            if tokenizer_class is None:\n",
        "                tokenizer_class = tokenizer_class_from_name(tokenizer_class_name)\n",
        "\n",
        "            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "\n",
        "        tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
        "        if \"_commit_hash\" in tokenizer_config:\n",
        "            kwargs[\"_commit_hash\"] = tokenizer_config[\"_commit_hash\"]\n",
        "        config_tokenizer_class = tokenizer_config.get(\"tokenizer_class\")\n",
        "        tokenizer_auto_map = None\n",
        "        if \"auto_map\" in tokenizer_config:\n",
        "            if isinstance(tokenizer_config[\"auto_map\"], (tuple, list)):\n",
        "                tokenizer_auto_map = tokenizer_config[\"auto_map\"]\n",
        "            else:\n",
        "                tokenizer_auto_map = tokenizer_config[\"auto_map\"].get(\"AutoTokenizer\", None)\n",
        "\n",
        "        if config_tokenizer_class is None:\n",
        "            if not isinstance(config, PretrainedConfig):\n",
        "                if gguf_file:\n",
        "                    gguf_path = cached_file(pretrained_model_name_or_path, gguf_file, **kwargs)\n",
        "                    config_dict = load_gguf_checkpoint(gguf_path, return_tensors=False)[\"config\"]\n",
        "                    config = AutoConfig.for_model(**config_dict)\n",
        "                else:\n",
        "                    config = AutoConfig.from_pretrained(\n",
        "                        pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n",
        "                    )\n",
        "            config_tokenizer_class = config.tokenizer_class\n",
        "            if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\n",
        "                tokenizer_auto_map = config.auto_map[\"AutoTokenizer\"]\n",
        "\n",
        "        has_remote_code = tokenizer_auto_map is not None\n",
        "        has_local_code = type(config) in TOKENIZER_MAPPING or (\n",
        "            config_tokenizer_class is not None\n",
        "            and (\n",
        "                tokenizer_class_from_name(config_tokenizer_class) is not None\n",
        "                or tokenizer_class_from_name(config_tokenizer_class + \"Fast\") is not None\n",
        "            )\n",
        "        )\n",
        "        if has_remote_code:\n",
        "            if use_fast and tokenizer_auto_map[1] is not None:\n",
        "                class_ref = tokenizer_auto_map[1]\n",
        "            else:\n",
        "                class_ref = tokenizer_auto_map[0]\n",
        "            if \"--\" in class_ref:\n",
        "                upstream_repo = class_ref.split(\"--\")[0]\n",
        "            else:\n",
        "                upstream_repo = None\n",
        "            trust_remote_code = resolve_trust_remote_code(\n",
        "                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n",
        "            )\n",
        "\n",
        "        if has_remote_code and trust_remote_code:\n",
        "            tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\n",
        "            _ = kwargs.pop(\"code_revision\", None)\n",
        "            tokenizer_class.register_for_auto_class()\n",
        "            return tokenizer_class.from_pretrained(\n",
        "                pretrained_model_name_or_path, *inputs, trust_remote_code=trust_remote_code, **kwargs\n",
        "            )\n",
        "        elif config_tokenizer_class is not None:\n",
        "            tokenizer_class = None\n",
        "            if use_fast and not config_tokenizer_class.endswith(\"Fast\"):\n",
        "                tokenizer_class_candidate = f\"{config_tokenizer_class}Fast\"\n",
        "                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n",
        "            if tokenizer_class is None:\n",
        "                tokenizer_class_candidate = config_tokenizer_class\n",
        "                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n",
        "            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "\n",
        "        if isinstance(config, EncoderDecoderConfig):\n",
        "            config = config.encoder\n",
        "\n",
        "        model_type = config_class_to_model_type(type(config).__name__)\n",
        "        if model_type is not None:\n",
        "            tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\n",
        "\n",
        "            if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):\n",
        "                return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "            else:\n",
        "                if tokenizer_class_py is not None:\n",
        "                    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False):\n",
        "        if config_class in TOKENIZER_MAPPING._extra_content:\n",
        "            existing_slow, existing_fast = TOKENIZER_MAPPING[config_class]\n",
        "            if slow_tokenizer_class is None:\n",
        "                slow_tokenizer_class = existing_slow\n",
        "            if fast_tokenizer_class is None:\n",
        "                fast_tokenizer_class = existing_fast\n",
        "\n",
        "        TOKENIZER_MAPPING.register(config_class, (slow_tokenizer_class, fast_tokenizer_class), exist_ok=exist_ok)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACT2CLS = {\n",
        "    \"gelu\": GELUActivation,\n",
        "    \"gelu_10\": (ClippedGELUActivation, {\"min\": -10, \"max\": 10}),\n",
        "    \"gelu_fast\": FastGELUActivation,\n",
        "    \"gelu_new\": NewGELUActivation,\n",
        "    \"gelu_python\": (GELUActivation, {\"use_gelu_python\": True}),\n",
        "    \"gelu_pytorch_tanh\": PytorchGELUTanh,\n",
        "    \"gelu_accurate\": AccurateGELUActivation,\n",
        "    \"laplace\": LaplaceActivation,\n",
        "    \"leaky_relu\": nn.LeakyReLU,\n",
        "    \"linear\": LinearActivation,\n",
        "    \"mish\": MishActivation,\n",
        "    \"quick_gelu\": QuickGELUActivation,\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"relu2\": ReLUSquaredActivation,\n",
        "    \"relu6\": nn.ReLU6,\n",
        "    \"sigmoid\": nn.Sigmoid,\n",
        "    \"silu\": nn.SiLU,\n",
        "    \"swish\": nn.SiLU,\n",
        "    \"tanh\": nn.Tanh,\n",
        "    \"prelu\": nn.PReLU,\n",
        "}\n",
        "ACT2FN = ClassInstantier(ACT2CLS)\n",
        "\n",
        "def get_tokenizer_config(\n",
        "    pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n",
        "    cache_dir: Optional[Union[str, os.PathLike[str]]] = None,\n",
        "    force_download: bool = False,\n",
        "    resume_download: Optional[bool] = None,\n",
        "    proxies: Optional[dict[str, str]] = None,\n",
        "    token: Optional[Union[bool, str]] = None,\n",
        "    revision: Optional[str] = None,\n",
        "    local_files_only: bool = False,\n",
        "    subfolder: str = \"\",\n",
        "    **kwargs,\n",
        ") -> dict[str, Any]:\n",
        "    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "    if use_auth_token is not None:\n",
        "        token = use_auth_token\n",
        "\n",
        "    commit_hash = kwargs.get(\"_commit_hash\", None)\n",
        "    resolved_config_file = cached_file(\n",
        "        pretrained_model_name_or_path,\n",
        "        TOKENIZER_CONFIG_FILE,\n",
        "        cache_dir=cache_dir,\n",
        "        force_download=force_download,\n",
        "        resume_download=resume_download,\n",
        "        proxies=proxies,\n",
        "        token=token,\n",
        "        revision=revision,\n",
        "        local_files_only=local_files_only,\n",
        "        subfolder=subfolder,\n",
        "        _raise_exceptions_for_gated_repo=False,\n",
        "        _raise_exceptions_for_missing_entries=False,\n",
        "        _raise_exceptions_for_connection_errors=False,\n",
        "        _commit_hash=commit_hash,\n",
        "    )\n",
        "    if resolved_config_file is None:\n",
        "        return {}\n",
        "    commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "\n",
        "    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n",
        "        result = json.load(reader)\n",
        "    result[\"_commit_hash\"] = commit_hash\n",
        "    return result\n",
        "\n",
        "def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n",
        "    if class_name == \"PreTrainedTokenizerFast\":\n",
        "        return PreTrainedTokenizerFast\n",
        "\n",
        "    for module_name, tokenizers in TOKENIZER_MAPPING_NAMES.items():\n",
        "        if class_name in tokenizers:\n",
        "            module_name = model_type_to_module_name(module_name)\n",
        "            if module_name in [\"mistral\", \"mixtral\"] and class_name == \"MistralCommonTokenizer\":\n",
        "                module = importlib.import_module(\".tokenization_mistral_common\", \"transformers\")\n",
        "            else:\n",
        "                module = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n",
        "            try:\n",
        "                return getattr(module, class_name)\n",
        "            except AttributeError:\n",
        "                continue\n",
        "\n",
        "    for config, tokenizers in TOKENIZER_MAPPING._extra_content.items():\n",
        "        for tokenizer in tokenizers:\n",
        "            if getattr(tokenizer, \"__name__\", None) == class_name:\n",
        "                return tokenizer\n",
        "\n",
        "    main_module = importlib.import_module(\"transformers\")\n",
        "    if hasattr(main_module, class_name):\n",
        "        return getattr(main_module, class_name)\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "loMtw72qzzAn"
      },
      "execution_count": 765,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_commit_hash(resolved_file: Optional[str], commit_hash: Optional[str]) -> Optional[str]:\n",
        "    if resolved_file is None or commit_hash is not None:\n",
        "        return commit_hash\n",
        "    resolved_file = str(Path(resolved_file).as_posix())\n",
        "    search = re.search(r\"snapshots/([^/]+)/\", resolved_file)\n",
        "    if search is None:\n",
        "        return None\n",
        "    commit_hash = search.groups()[0]\n",
        "    return commit_hash if REGEX_COMMIT_HASH.match(commit_hash) else None\n",
        "\n",
        "_is_offline_mode = huggingface_hub.constants.HF_HUB_OFFLINE\n",
        "\n",
        "def is_offline_mode():\n",
        "    return _is_offline_mode\n",
        "\n",
        "def cached_file(\n",
        "    path_or_repo_id: Union[str, os.PathLike],\n",
        "    filename: str,\n",
        "    **kwargs,\n",
        ") -> Optional[str]:\n",
        "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
        "    file = file[0] if file is not None else file\n",
        "    return file\n",
        "\n",
        "def cached_files(\n",
        "    path_or_repo_id: Union[str, os.PathLike],\n",
        "    filenames: list[str],\n",
        "    cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "    force_download: bool = False,\n",
        "    resume_download: Optional[bool] = None,\n",
        "    proxies: Optional[dict[str, str]] = None,\n",
        "    token: Optional[Union[bool, str]] = None,\n",
        "    revision: Optional[str] = None,\n",
        "    local_files_only: bool = False,\n",
        "    subfolder: str = \"\",\n",
        "    repo_type: Optional[str] = None,\n",
        "    user_agent: Optional[Union[str, dict[str, str]]] = None,\n",
        "    _raise_exceptions_for_gated_repo: bool = True,\n",
        "    _raise_exceptions_for_missing_entries: bool = True,\n",
        "    _raise_exceptions_for_connection_errors: bool = True,\n",
        "    _commit_hash: Optional[str] = None,\n",
        "    **deprecated_kwargs,\n",
        ") -> Optional[str]:\n",
        "    use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n",
        "    if use_auth_token is not None:\n",
        "        token = use_auth_token\n",
        "\n",
        "    if is_offline_mode() and not local_files_only:\n",
        "        local_files_only = True\n",
        "    if subfolder is None:\n",
        "        subfolder = \"\"\n",
        "\n",
        "    full_filenames = [os.path.join(subfolder, file) for file in filenames]\n",
        "\n",
        "    path_or_repo_id = str(path_or_repo_id)\n",
        "    existing_files = []\n",
        "    for filename in full_filenames:\n",
        "        if os.path.isdir(path_or_repo_id):\n",
        "            resolved_file = os.path.join(path_or_repo_id, filename)\n",
        "            if not os.path.isfile(resolved_file):\n",
        "                if _raise_exceptions_for_missing_entries and filename != os.path.join(subfolder, \"config.json\"):\n",
        "                    revision_ = \"main\" if revision is None else revision\n",
        "                else:\n",
        "                    return None\n",
        "            existing_files.append(resolved_file)\n",
        "\n",
        "    if len(existing_files) == len(full_filenames):\n",
        "        return existing_files\n",
        "\n",
        "    if cache_dir is None:\n",
        "        cache_dir = TRANSFORMERS_CACHE\n",
        "    if isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    existing_files = []\n",
        "    file_counter = 0\n",
        "    if _commit_hash is not None and not force_download:\n",
        "        for filename in full_filenames:\n",
        "            resolved_file = try_to_load_from_cache(\n",
        "                path_or_repo_id, filename, cache_dir=cache_dir, revision=_commit_hash, repo_type=repo_type\n",
        "            )\n",
        "            if resolved_file is not None:\n",
        "                if resolved_file is not _CACHED_NO_EXIST:\n",
        "                    file_counter += 1\n",
        "                    existing_files.append(resolved_file)\n",
        "                elif not _raise_exceptions_for_missing_entries:\n",
        "                    file_counter += 1\n",
        "    if file_counter == len(full_filenames):\n",
        "        return existing_files if len(existing_files) > 0 else None\n",
        "\n",
        "    user_agent = http_user_agent(user_agent)\n",
        "    if len(full_filenames) == 1:\n",
        "        hf_hub_download(\n",
        "            path_or_repo_id,\n",
        "            filenames[0],\n",
        "            subfolder=None if len(subfolder) == 0 else subfolder,\n",
        "            repo_type=repo_type,\n",
        "            revision=revision,\n",
        "            cache_dir=cache_dir,\n",
        "            user_agent=user_agent,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            resume_download=resume_download,\n",
        "            token=token,\n",
        "            local_files_only=local_files_only,\n",
        "        )\n",
        "    else:\n",
        "        snapshot_download(\n",
        "            path_or_repo_id,\n",
        "            allow_patterns=full_filenames,\n",
        "            repo_type=repo_type,\n",
        "            revision=revision,\n",
        "            cache_dir=cache_dir,\n",
        "            user_agent=user_agent,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            resume_download=resume_download,\n",
        "            token=token,\n",
        "            local_files_only=local_files_only,\n",
        "        )\n",
        "\n",
        "    resolved_files = [\n",
        "        _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n",
        "    ]\n",
        "    if any(file is None for file in resolved_files) and _raise_exceptions_for_missing_entries:\n",
        "        missing_entries = [original for original, resolved in zip(full_filenames, resolved_files) if resolved is None]\n",
        "        if len(resolved_files) == 1 and missing_entries[0] == os.path.join(subfolder, \"config.json\"):\n",
        "            return None\n",
        "        revision_ = \"main\" if revision is None else revision\n",
        "\n",
        "    resolved_files = [file for file in resolved_files if file is not None]\n",
        "    resolved_files = None if len(resolved_files) == 0 else resolved_files\n",
        "\n",
        "    return resolved_files"
      ],
      "metadata": {
        "id": "S_yQLGdq0Hup"
      },
      "execution_count": 766,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "_has_opentelemetry = True\n",
        "\n",
        "DEPRECATED_MODELS = [\n",
        "    \"bort\",\n",
        "    \"deta\",\n",
        "    \"efficientformer\",\n",
        "    \"ernie_m\",\n",
        "    \"gptsan_japanese\",\n",
        "    \"graphormer\",\n",
        "    \"jukebox\",\n",
        "    \"mctct\",\n",
        "    \"mega\",\n",
        "    \"mmbt\",\n",
        "    \"nat\",\n",
        "    \"nezha\",\n",
        "    \"open_llama\",\n",
        "    \"qdqbert\",\n",
        "    \"realm\",\n",
        "    \"retribert\",\n",
        "    \"speech_to_text_2\",\n",
        "    \"tapex\",\n",
        "    \"trajectory_transformer\",\n",
        "    \"transfo_xl\",\n",
        "    \"tvlt\",\n",
        "    \"van\",\n",
        "    \"vit_hybrid\",\n",
        "    \"xlm_prophetnet\",\n",
        "]\n",
        "\n",
        "SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict[str, str](\n",
        "    [\n",
        "        (\"openai-gpt\", \"openai\"),\n",
        "        (\"data2vec-audio\", \"data2vec\"),\n",
        "        (\"data2vec-text\", \"data2vec\"),\n",
        "        (\"data2vec-vision\", \"data2vec\"),\n",
        "        (\"donut-swin\", \"donut\"),\n",
        "        (\"kosmos-2\", \"kosmos2\"),\n",
        "        (\"maskformer-swin\", \"maskformer\"),\n",
        "        (\"xclip\", \"x_clip\"),\n",
        "        (\"clip_vision_model\", \"clip\"),\n",
        "        (\"qwen2_audio_encoder\", \"qwen2_audio\"),\n",
        "        (\"clip_text_model\", \"clip\"),\n",
        "        (\"aria_text\", \"aria\"),\n",
        "        (\"gemma3_text\", \"gemma3\"),\n",
        "        (\"gemma3n_audio\", \"gemma3n\"),\n",
        "        (\"gemma3n_text\", \"gemma3n\"),\n",
        "        (\"gemma3n_vision\", \"gemma3n\"),\n",
        "        (\"glm4v_text\", \"glm4v\"),\n",
        "        (\"idefics3_vision\", \"idefics3\"),\n",
        "        (\"siglip_vision_model\", \"siglip\"),\n",
        "        (\"aimv2_vision_model\", \"aimv2\"),\n",
        "        (\"smolvlm_vision\", \"smolvlm\"),\n",
        "        (\"chinese_clip_vision_model\", \"chinese_clip\"),\n",
        "        (\"rt_detr_resnet\", \"rt_detr\"),\n",
        "        (\"granitevision\", \"llava_next\"),\n",
        "        (\"internvl_vision\", \"internvl\"),\n",
        "        (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n",
        "        (\"qwen2_vl_text\", \"qwen2_vl\"),\n",
        "        (\"sam_vision_model\", \"sam\"),\n",
        "        (\"sam_hq_vision_model\", \"sam_hq\"),\n",
        "        (\"llama4_text\", \"llama4\"),\n",
        "        (\"blip_2_qformer\", \"blip_2\"),\n",
        "        (\"fastspeech2_conformer_with_hifigan\", \"fastspeech2_conformer\"),\n",
        "        (\"perception_encoder\", \"perception_lm\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ALL_CACHE_NAMES = [\n",
        "    \"past_key_values\",\n",
        "    \"cache_params\",\n",
        "    \"state\",\n",
        "    \"mems\",\n",
        "    \"past_buckets_states\",\n",
        "]\n",
        "\n",
        "METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n",
        "\n",
        "def model_type_to_module_name(key) -> str:\n",
        "    if key in SPECIAL_MODEL_TYPE_TO_MODULE_NAME:\n",
        "        key = SPECIAL_MODEL_TYPE_TO_MODULE_NAME[key]\n",
        "\n",
        "        if key in DEPRECATED_MODELS:\n",
        "            key = f\"deprecated.{key}\"\n",
        "        return key\n",
        "\n",
        "    key = key.replace(\"-\", \"_\")\n",
        "    if key in DEPRECATED_MODELS:\n",
        "        key = f\"deprecated.{key}\"\n",
        "\n",
        "    return key"
      ],
      "metadata": {
        "id": "oBE3rOCFvo_V"
      },
      "execution_count": 767,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerationConfig():\n",
        "    extra_output_flags = (\"output_attentions\", \"output_hidden_states\", \"output_scores\", \"output_logits\")\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.max_length = kwargs.pop(\"max_length\", 20)\n",
        "        self.max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\n",
        "        self.min_length = kwargs.pop(\"min_length\", 0)\n",
        "        self.min_new_tokens = kwargs.pop(\"min_new_tokens\", None)\n",
        "        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n",
        "        self.max_time = kwargs.pop(\"max_time\", None)\n",
        "        self.stop_strings = kwargs.pop(\"stop_strings\", None)\n",
        "\n",
        "        self.do_sample = kwargs.pop(\"do_sample\", False)\n",
        "        self.num_beams = kwargs.pop(\"num_beams\", 1)\n",
        "        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n",
        "        self.penalty_alpha = kwargs.pop(\"penalty_alpha\", None)\n",
        "        self.dola_layers = kwargs.pop(\"dola_layers\", None)\n",
        "\n",
        "        self.use_cache = kwargs.pop(\"use_cache\", True)\n",
        "        self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n",
        "        self.cache_config = kwargs.pop(\"cache_config\", None)\n",
        "        if self.cache_implementation is not None and self.cache_implementation in CACHE_CONFIG_MAPPING:\n",
        "            cache_config_class = CACHE_CONFIG_MAPPING[self.cache_implementation]\n",
        "            if isinstance(self.cache_config, dict):\n",
        "                self.cache_config = cache_config_class.from_dict(self.cache_config)\n",
        "        self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n",
        "        self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)\n",
        "\n",
        "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
        "        self.top_k = kwargs.pop(\"top_k\", 50)\n",
        "        self.top_p = kwargs.pop(\"top_p\", 1.0)\n",
        "        self.min_p = kwargs.pop(\"min_p\", None)\n",
        "        self.typical_p = kwargs.pop(\"typical_p\", 1.0)\n",
        "        self.epsilon_cutoff = kwargs.pop(\"epsilon_cutoff\", 0.0)\n",
        "        self.eta_cutoff = kwargs.pop(\"eta_cutoff\", 0.0)\n",
        "        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n",
        "        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n",
        "        self.encoder_repetition_penalty = kwargs.pop(\"encoder_repetition_penalty\", 1.0)\n",
        "        self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n",
        "        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n",
        "        self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n",
        "        self.force_words_ids = kwargs.pop(\"force_words_ids\", None)\n",
        "        self.renormalize_logits = kwargs.pop(\"renormalize_logits\", False)\n",
        "        self.constraints = kwargs.pop(\"constraints\", None)\n",
        "        self.forced_bos_token_id = kwargs.pop(\"forced_bos_token_id\", None)\n",
        "        self.forced_eos_token_id = kwargs.pop(\"forced_eos_token_id\", None)\n",
        "        self.remove_invalid_values = kwargs.pop(\"remove_invalid_values\", False)\n",
        "        self.exponential_decay_length_penalty = kwargs.pop(\"exponential_decay_length_penalty\", None)\n",
        "        self.suppress_tokens = kwargs.pop(\"suppress_tokens\", None)\n",
        "        self.begin_suppress_tokens = kwargs.pop(\"begin_suppress_tokens\", None)\n",
        "        self.sequence_bias = kwargs.pop(\"sequence_bias\", None)\n",
        "        self.token_healing = kwargs.pop(\"token_healing\", False)\n",
        "        self.guidance_scale = kwargs.pop(\"guidance_scale\", None)\n",
        "        self.low_memory = kwargs.pop(\"low_memory\", None)\n",
        "        watermarking_config = kwargs.pop(\"watermarking_config\", None)\n",
        "        if watermarking_config is None:\n",
        "            self.watermarking_config = None\n",
        "        elif isinstance(watermarking_config, BaseWatermarkingConfig):\n",
        "            self.watermarking_config = watermarking_config\n",
        "        else:\n",
        "            self.watermarking_config = WatermarkingConfig.from_dict(watermarking_config)\n",
        "\n",
        "        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n",
        "        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n",
        "        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n",
        "        self.output_scores = kwargs.pop(\"output_scores\", False)\n",
        "        self.output_logits = kwargs.pop(\"output_logits\", None)\n",
        "        self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n",
        "\n",
        "        self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n",
        "        self.bos_token_id = kwargs.pop(\"bos_token_id\", None)\n",
        "        self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n",
        "\n",
        "        self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", 0)\n",
        "        self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n",
        "\n",
        "        self.is_assistant = False\n",
        "        self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", 20)\n",
        "        self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", \"constant\")\n",
        "        self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", 0.4)\n",
        "        self.prompt_lookup_num_tokens = kwargs.pop(\"prompt_lookup_num_tokens\", None)\n",
        "        self.max_matching_ngram_size = kwargs.pop(\"max_matching_ngram_size\", None)\n",
        "        self.assistant_early_exit = kwargs.pop(\"assistant_early_exit\", None)\n",
        "        self.assistant_lookbehind = kwargs.pop(\"assistant_lookbehind\", 10)\n",
        "        self.target_lookbehind = kwargs.pop(\"target_lookbehind\", 10)\n",
        "\n",
        "        self.compile_config = kwargs.pop(\"compile_config\", None)\n",
        "        self.disable_compile = kwargs.pop(\"disable_compile\", False)\n",
        "\n",
        "        self._from_model_config = kwargs.pop(\"_from_model_config\", False)\n",
        "        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "        self.transformers_version = kwargs.pop(\"transformers_version\", __version__)\n",
        "\n",
        "        if not self._from_model_config:\n",
        "            for key, value in kwargs.items():\n",
        "                setattr(self, key, value)\n",
        "\n",
        "        self.validate()\n",
        "\n",
        "    def validate(self, strict=False):\n",
        "        minor_issues = {}\n",
        "\n",
        "        if self.cache_config is not None:\n",
        "            cache_class = CACHE_CONFIG_MAPPING.get(self.cache_implementation)\n",
        "            if not isinstance(self.cache_config, cache_class):\n",
        "                self.cache_config = cache_class.from_dict(self.cache_config)\n",
        "            self.cache_config.validate()\n",
        "        if self.watermarking_config is not None:\n",
        "            if not (\n",
        "                isinstance(self.watermarking_config, WatermarkingConfig)\n",
        "                or isinstance(self.watermarking_config, SynthIDTextWatermarkingConfig)\n",
        "            ):\n",
        "                minor_issues[\"watermarking_config\"] = (\n",
        "                    \"`watermarking_config` as a dict is deprecated and will be removed in v4.54.0. Please construct \"\n",
        "                    \"`watermarking_config` object with `WatermarkingConfig` or `SynthIDTextWatermarkingConfig` class.\"\n",
        "                )\n",
        "                self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n",
        "            self.watermarking_config.validate()\n",
        "\n",
        "        if self.do_sample is False:\n",
        "            if self.temperature is not None and self.temperature != 1.0:\n",
        "                minor_issues[\"temperature\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"temperature\", flag_value=self.temperature\n",
        "                )\n",
        "            if self.top_p is not None and self.top_p != 1.0:\n",
        "                minor_issues[\"top_p\"] = greedy_wrong_parameter_msg.format(flag_name=\"top_p\", flag_value=self.top_p)\n",
        "            if self.min_p is not None:\n",
        "                minor_issues[\"min_p\"] = greedy_wrong_parameter_msg.format(flag_name=\"min_p\", flag_value=self.min_p)\n",
        "            if self.typical_p is not None and self.typical_p != 1.0:\n",
        "                minor_issues[\"typical_p\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"typical_p\", flag_value=self.typical_p\n",
        "                )\n",
        "            if (\n",
        "                self.top_k is not None and self.top_k != 50 and self.penalty_alpha is None\n",
        "            ):\n",
        "                minor_issues[\"top_k\"] = greedy_wrong_parameter_msg.format(flag_name=\"top_k\", flag_value=self.top_k)\n",
        "            if self.epsilon_cutoff is not None and self.epsilon_cutoff != 0.0:\n",
        "                minor_issues[\"epsilon_cutoff\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"epsilon_cutoff\", flag_value=self.epsilon_cutoff\n",
        "                )\n",
        "            if self.eta_cutoff is not None and self.eta_cutoff != 0.0:\n",
        "                minor_issues[\"eta_cutoff\"] = greedy_wrong_parameter_msg.format(\n",
        "                    flag_name=\"eta_cutoff\", flag_value=self.eta_cutoff\n",
        "                )\n",
        "\n",
        "        if self.num_beams == 1:\n",
        "            if self.early_stopping is not False:\n",
        "                minor_issues[\"early_stopping\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"early_stopping\", flag_value=self.early_stopping\n",
        "                )\n",
        "            if self.num_beam_groups is not None and self.num_beam_groups != 1:\n",
        "                minor_issues[\"num_beam_groups\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"num_beam_groups\", flag_value=self.num_beam_groups\n",
        "                )\n",
        "            if self.diversity_penalty is not None and self.diversity_penalty != 0.0:\n",
        "                minor_issues[\"diversity_penalty\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"diversity_penalty\", flag_value=self.diversity_penalty\n",
        "                )\n",
        "            if self.length_penalty is not None and self.length_penalty != 1.0:\n",
        "                minor_issues[\"length_penalty\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"length_penalty\", flag_value=self.length_penalty\n",
        "                )\n",
        "            if self.constraints is not None:\n",
        "                minor_issues[\"constraints\"] = single_beam_wrong_parameter_msg.format(\n",
        "                    flag_name=\"constraints\", flag_value=self.constraints\n",
        "                )\n",
        "\n",
        "        if self.use_cache is False:\n",
        "            for arg_name in (\"cache_implementation\", \"cache_config\", \"return_legacy_cache\"):\n",
        "                if getattr(self, arg_name) is not None:\n",
        "                    minor_issues[arg_name] = no_cache_warning.format(\n",
        "                        cache_arg=arg_name, cache_arg_value=getattr(self, arg_name)\n",
        "                    )\n",
        "\n",
        "        generate_arguments = (\n",
        "            \"logits_processor\",\n",
        "            \"stopping_criteria\",\n",
        "            \"prefix_allowed_tokens_fn\",\n",
        "            \"synced_gpus\",\n",
        "            \"assistant_model\",\n",
        "            \"streamer\",\n",
        "            \"negative_prompt_ids\",\n",
        "            \"negative_prompt_attention_mask\",\n",
        "            \"use_model_defaults\",\n",
        "        )\n",
        "\n",
        "        if len(minor_issues) > 0:\n",
        "            attributes_with_issues = list(minor_issues.keys())\n",
        "    def save_pretrained(\n",
        "        self,\n",
        "        save_directory: Union[str, os.PathLike],\n",
        "        config_file_name: Optional[Union[str, os.PathLike]] = None,\n",
        "        push_to_hub: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.validate(strict=True)\n",
        "\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "\n",
        "        if use_auth_token is not None:\n",
        "            kwargs[\"token\"] = use_auth_token\n",
        "\n",
        "        config_file_name = config_file_name if config_file_name is not None else GENERATION_CONFIG_NAME\n",
        "\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "        if push_to_hub:\n",
        "            commit_message = kwargs.pop(\"commit_message\", None)\n",
        "            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n",
        "            repo_id = self._create_repo(repo_id, **kwargs)\n",
        "            files_timestamps = self._get_files_timestamps(save_directory)\n",
        "\n",
        "        output_config_file = os.path.join(save_directory, config_file_name)\n",
        "\n",
        "        self.to_json_file(output_config_file, use_diff=True)\n",
        "        if push_to_hub:\n",
        "            self._upload_modified_files(\n",
        "                save_directory,\n",
        "                repo_id,\n",
        "                files_timestamps,\n",
        "                commit_message=commit_message,\n",
        "                token=kwargs.get(\"token\"),\n",
        "            )\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        pretrained_model_name: Union[str, os.PathLike],\n",
        "        config_file_name: Optional[Union[str, os.PathLike]] = None,\n",
        "        cache_dir: Optional[Union[str, os.PathLike]] = None,\n",
        "        force_download: bool = False,\n",
        "        local_files_only: bool = False,\n",
        "        token: Optional[Union[str, bool]] = None,\n",
        "        revision: str = \"main\",\n",
        "        **kwargs,\n",
        "    ) -> \"GenerationConfig\":\n",
        "        config_file_name = config_file_name if config_file_name is not None else GENERATION_CONFIG_NAME\n",
        "\n",
        "        resume_download = kwargs.pop(\"resume_download\", None)\n",
        "        proxies = kwargs.pop(\"proxies\", None)\n",
        "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
        "        subfolder = kwargs.pop(\"subfolder\", \"\")\n",
        "        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n",
        "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
        "        commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
        "\n",
        "        if use_auth_token is not None:\n",
        "            token = use_auth_token\n",
        "\n",
        "        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n",
        "        if from_pipeline is not None:\n",
        "            user_agent[\"using_pipeline\"] = from_pipeline\n",
        "\n",
        "        config_path = os.path.join(pretrained_model_name, config_file_name)\n",
        "        config_path = str(config_path)\n",
        "\n",
        "        is_local = os.path.exists(config_path)\n",
        "        if os.path.isfile(os.path.join(subfolder, config_path)):\n",
        "            resolved_config_file = config_path\n",
        "            is_local = True\n",
        "        elif is_remote_url(config_path):\n",
        "            configuration_file = config_path\n",
        "            resolved_config_file = download_url(config_path)\n",
        "        else:\n",
        "            configuration_file = config_file_name\n",
        "            resolved_config_file = cached_file(\n",
        "                pretrained_model_name,\n",
        "                configuration_file,\n",
        "                cache_dir=cache_dir,\n",
        "                force_download=force_download,\n",
        "                proxies=proxies,\n",
        "                resume_download=resume_download,\n",
        "                local_files_only=local_files_only,\n",
        "                token=token,\n",
        "                user_agent=user_agent,\n",
        "                revision=revision,\n",
        "                subfolder=subfolder,\n",
        "                _commit_hash=commit_hash,\n",
        "            )\n",
        "            commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
        "\n",
        "        config_dict = cls._dict_from_json_file(resolved_config_file)\n",
        "        config_dict[\"_commit_hash\"] = commit_hash\n",
        "\n",
        "        if kwargs.get(\"return_unused_kwargs\") is True:\n",
        "            config, unused_kwargs = cls.from_dict(config_dict, **kwargs)\n",
        "            config._original_object_hash = hash(config)\n",
        "            return config, unused_kwargs\n",
        "        else:\n",
        "            config = cls.from_dict(config_dict, **kwargs)\n",
        "            config._original_object_hash = hash(config)\n",
        "            return config\n",
        "\n",
        "    @classmethod\n",
        "    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n",
        "        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "            text = reader.read()\n",
        "        return json.loads(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"GenerationConfig\":\n",
        "        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n",
        "        kwargs.pop(\"_from_auto\", None)\n",
        "        kwargs.pop(\"_from_pipeline\", None)\n",
        "        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n",
        "            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n",
        "\n",
        "        config = cls(**{**config_dict, **kwargs})\n",
        "        unused_kwargs = config.update(**kwargs)\n",
        "\n",
        "        if return_unused_kwargs:\n",
        "            return config, unused_kwargs\n",
        "        else:\n",
        "            return config\n",
        "\n",
        "    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n",
        "        if d.get(\"torch_dtype\", None) is not None and not isinstance(d[\"torch_dtype\"], str):\n",
        "            d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n",
        "        for value in d.values():\n",
        "            if isinstance(value, dict):\n",
        "                self.dict_torch_dtype_to_str(value)\n",
        "\n",
        "    def to_diff_dict(self) -> dict[str, Any]:\n",
        "        config_dict = self.to_dict()\n",
        "\n",
        "        default_config_dict = GenerationConfig().to_dict()\n",
        "\n",
        "        serializable_config_dict = {}\n",
        "\n",
        "        for key, value in config_dict.items():\n",
        "            if key not in default_config_dict or key == \"transformers_version\" or value != default_config_dict[key]:\n",
        "                serializable_config_dict[key] = value\n",
        "\n",
        "        self.dict_torch_dtype_to_str(serializable_config_dict)\n",
        "        return serializable_config_dict\n",
        "\n",
        "    def to_dict(self) -> dict[str, Any]:\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "\n",
        "        if \"_commit_hash\" in output:\n",
        "            del output[\"_commit_hash\"]\n",
        "        if \"_original_object_hash\" in output:\n",
        "            del output[\"_original_object_hash\"]\n",
        "        if \"compile_config\" in output:\n",
        "            del output[\"compile_config\"]\n",
        "\n",
        "        output[\"transformers_version\"] = __version__\n",
        "\n",
        "        self.dict_torch_dtype_to_str(output)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self, use_diff: bool = True, ignore_metadata: bool = False) -> str:\n",
        "        if use_diff is True:\n",
        "            config_dict = self.to_diff_dict()\n",
        "        else:\n",
        "            config_dict = self.to_dict()\n",
        "\n",
        "        if ignore_metadata:\n",
        "            for metadata_field in METADATA_FIELDS:\n",
        "                config_dict.pop(metadata_field, None)\n",
        "\n",
        "        def convert_keys_to_string(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {str(key): convert_keys_to_string(value) for key, value in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_keys_to_string(item) for item in obj]\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        def convert_dataclass_to_dict(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {key: convert_dataclass_to_dict(value) for key, value in obj.items()}\n",
        "            elif is_dataclass(obj):\n",
        "                return obj.to_dict()\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        config_dict = convert_keys_to_string(config_dict)\n",
        "        config_dict = convert_dataclass_to_dict(config_dict)\n",
        "\n",
        "        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "    def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True):\n",
        "        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n",
        "            writer.write(self.to_json_string(use_diff=use_diff))\n",
        "\n",
        "    @classmethod\n",
        "    def from_model_config(cls, model_config: PretrainedConfig) -> \"GenerationConfig\":\n",
        "        config_dict = model_config.to_dict()\n",
        "        config_dict.pop(\"_from_model_config\", None)\n",
        "\n",
        "        config_dict = {key: value for key, value in config_dict.items() if value is not None}\n",
        "\n",
        "        generation_config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)\n",
        "\n",
        "        decoder_config = model_config.get_text_config(decoder=True)\n",
        "        if decoder_config is not model_config:\n",
        "            default_generation_config = GenerationConfig()\n",
        "            decoder_config_dict = decoder_config.to_dict()\n",
        "            for attr in generation_config.to_dict().keys():\n",
        "                is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n",
        "                if attr in decoder_config_dict and is_unset:\n",
        "                    setattr(generation_config, attr, decoder_config_dict[attr])\n",
        "        if generation_config.return_dict_in_generate is False:\n",
        "            if any(\n",
        "                getattr(generation_config, extra_output_flag, False)\n",
        "                for extra_output_flag in generation_config.extra_output_flags\n",
        "            ):\n",
        "                generation_config.return_dict_in_generate = True\n",
        "\n",
        "        generation_config._original_object_hash = hash(generation_config)\n",
        "        return generation_config\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        to_remove = []\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "                to_remove.append(key)\n",
        "\n",
        "        self.validate()\n",
        "\n",
        "        unused_kwargs = {key: value for key, value in kwargs.items() if key not in to_remove}\n",
        "        return unused_kwargs"
      ],
      "metadata": {
        "id": "kNPxiQ-a2JzS"
      },
      "execution_count": 768,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 769,
      "metadata": {
        "id": "299249F6Ww9U"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class GenerateDecoderOnlyOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerateEncoderDecoderOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n",
        "    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n",
        "    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerateBeamDecoderOnlyOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    sequences_scores: Optional[torch.FloatTensor] = None\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    beam_indices: Optional[torch.LongTensor] = None\n",
        "    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerateBeamEncoderDecoderOutput(ModelOutput):\n",
        "    sequences: torch.LongTensor\n",
        "    sequences_scores: Optional[torch.FloatTensor] = None\n",
        "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
        "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
        "    beam_indices: Optional[torch.LongTensor] = None\n",
        "    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n",
        "    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n",
        "    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
        "    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n",
        "\n",
        "GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]\n",
        "GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]\n",
        "GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]\n",
        "\n",
        "class GenerationMixin():\n",
        "    def load_custom_generate(\n",
        "        self,\n",
        "        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]] = None,\n",
        "        trust_remote_code: Optional[bool] = None,\n",
        "        **kwargs,\n",
        "    ) -> Callable:\n",
        "        is_local_code = os.path.exists(pretrained_model_name_or_path)\n",
        "        has_custom_generate_folder = True\n",
        "        if is_local_code:\n",
        "            if not os.path.exists(os.path.join(pretrained_model_name_or_path, \"custom_generate/generate.py\")):\n",
        "                has_custom_generate_folder = False\n",
        "        else:\n",
        "            if not file_exists(pretrained_model_name_or_path, \"custom_generate/generate.py\"):\n",
        "                has_custom_generate_folder = False\n",
        "\n",
        "    def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n",
        "        if self.config.is_encoder_decoder:\n",
        "            for key in [\"decoder_input_ids\"]:\n",
        "                model_kwargs.pop(key, None)\n",
        "\n",
        "        unused_model_args = []\n",
        "        model_args = set(inspect.signature(self.prepare_inputs_for_generation).parameters)\n",
        "        if \"kwargs\" in model_args or \"model_kwargs\" in model_args:\n",
        "            model_args |= set(inspect.signature(self.forward).parameters)\n",
        "\n",
        "        if self.config.is_encoder_decoder:\n",
        "            base_model = getattr(self, self.base_model_prefix, None)\n",
        "\n",
        "            encoder = getattr(self, \"encoder\", None)\n",
        "            if encoder is None and base_model is not None:\n",
        "                encoder = getattr(base_model, \"encoder\", None)\n",
        "\n",
        "            if encoder is not None:\n",
        "                encoder_model_args = set(inspect.signature(encoder.forward).parameters)\n",
        "                model_args |= encoder_model_args\n",
        "\n",
        "            decoder = getattr(self, \"decoder\", None)\n",
        "            if decoder is None and base_model is not None:\n",
        "                decoder = getattr(base_model, \"decoder\", None)\n",
        "\n",
        "            if decoder is not None:\n",
        "                decoder_model_args = set(inspect.signature(decoder.forward).parameters)\n",
        "                model_args |= {f\"decoder_{x}\" for x in decoder_model_args}\n",
        "\n",
        "        for key, value in model_kwargs.items():\n",
        "            if value is not None and key not in model_args:\n",
        "                unused_model_args.append(key)\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        model_inputs = {}\n",
        "        model_inputs[\"cache_position\"] = cache_position\n",
        "\n",
        "        if past_key_values is not None:\n",
        "            model_inputs[\"past_key_values\"] = past_key_values\n",
        "            inputs_embeds, input_ids = self._cache_dependant_input_preparation(\n",
        "                input_ids, inputs_embeds, cache_position\n",
        "            )\n",
        "\n",
        "        input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
        "        if not self.config.is_encoder_decoder:\n",
        "            if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n",
        "                model_inputs[input_ids_key] = None\n",
        "                model_inputs[\"inputs_embeds\"] = inputs_embeds\n",
        "            else:\n",
        "                model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n",
        "                model_inputs[\"inputs_embeds\"] = None\n",
        "        else:\n",
        "            model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n",
        "\n",
        "        encoder_attention_mask = attention_mask if self.config.is_encoder_decoder else None\n",
        "        attention_mask = (\n",
        "            kwargs.pop(\"decoder_attention_mask\", None) if self.config.is_encoder_decoder else attention_mask\n",
        "        )\n",
        "        attention_mask_key = \"decoder_attention_mask\" if self.config.is_encoder_decoder else \"attention_mask\"\n",
        "        position_ids_key = \"decoder_position_ids\" if self.config.is_encoder_decoder else \"position_ids\"\n",
        "        if (\n",
        "            attention_mask is not None\n",
        "            and kwargs.get(position_ids_key) is None\n",
        "            and position_ids_key in set(inspect.signature(self.forward).parameters.keys())\n",
        "        ):\n",
        "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "            kwargs[position_ids_key] = position_ids\n",
        "\n",
        "        for model_input_name in [\"position_ids\", \"token_type_ids\", \"decoder_position_ids\"]:\n",
        "            model_input = kwargs.get(model_input_name)\n",
        "            if model_input is not None:\n",
        "                if past_key_values is not None:\n",
        "                    current_input_length = (\n",
        "                        model_inputs[\"inputs_embeds\"].shape[1]\n",
        "                        if model_inputs.get(\"inputs_embeds\") is not None\n",
        "                        else model_inputs[input_ids_key].shape[1]\n",
        "                    )\n",
        "                    model_input = model_input[:, -current_input_length:]\n",
        "                    model_input = model_input.clone(memory_format=torch.contiguous_format)\n",
        "                model_inputs[model_input_name] = model_input\n",
        "\n",
        "        if (\n",
        "            isinstance(past_key_values, Cache)\n",
        "            and past_key_values.is_compileable\n",
        "            and attention_mask is not None\n",
        "            and attention_mask.ndim == 2\n",
        "        ):\n",
        "            if not self.config.is_encoder_decoder and model_inputs[\"inputs_embeds\"] is not None:\n",
        "                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n",
        "            else:\n",
        "                batch_size, sequence_length = model_inputs[input_ids_key].shape[:2]\n",
        "\n",
        "            base_model = getattr(self, self.base_model_prefix, self)\n",
        "            decoder = base_model.get_decoder() if hasattr(base_model, \"get_decoder\") else None\n",
        "            causal_mask_creation_function = getattr(\n",
        "                base_model, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n",
        "            )\n",
        "            if causal_mask_creation_function is None and decoder is not None:\n",
        "                causal_mask_creation_function = getattr(\n",
        "                    decoder, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n",
        "                )\n",
        "\n",
        "            if causal_mask_creation_function is None:\n",
        "                token_type_ids = getattr(model_input, \"token_type_ids\", None)\n",
        "                position_ids = getattr(model_input, position_ids_key, None)\n",
        "                causal_mask_creation_function = getattr(self, \"create_masks_for_generate\", create_masks_for_generate)\n",
        "                attention_mask = causal_mask_creation_function(\n",
        "                    config=self.config,\n",
        "                    input_embeds=torch.empty((batch_size, sequence_length), dtype=self.dtype),\n",
        "                    attention_mask=attention_mask,\n",
        "                    cache_position=cache_position,\n",
        "                    past_key_values=past_key_values,\n",
        "                    position_ids=position_ids,\n",
        "                    token_type_ids=token_type_ids,\n",
        "                )\n",
        "            else:\n",
        "                attention_mask = causal_mask_creation_function(\n",
        "                    attention_mask,\n",
        "                    sequence_length=sequence_length,\n",
        "                    target_length=past_key_values.get_max_cache_shape(),\n",
        "                    dtype=self.dtype,\n",
        "                    cache_position=cache_position,\n",
        "                    batch_size=batch_size,\n",
        "                    config=self.config,\n",
        "                    past_key_values=past_key_values,\n",
        "                )\n",
        "        if attention_mask is not None:\n",
        "            model_inputs[attention_mask_key] = attention_mask\n",
        "\n",
        "        if encoder_attention_mask is not None:\n",
        "            model_inputs[\"attention_mask\"] = encoder_attention_mask\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            if key not in model_inputs:\n",
        "                model_inputs[key] = value\n",
        "\n",
        "        model_inputs.pop(\"labels\", None)\n",
        "        return model_inputs\n",
        "\n",
        "    def _validate_assistant(self, assistant_model, tokenizer, assistant_tokenizer):\n",
        "        if assistant_model is None:\n",
        "            return\n",
        "\n",
        "        if self.config.is_encoder_decoder and not assistant_model.config.is_encoder_decoder:\n",
        "            attributes_to_check = [\"encoder_attention_heads\", \"encoder_ffn_dim\", \"encoder_layers\"]\n",
        "            attributes_to_check = [attr for attr in dir(assistant_model.config) if attr in attributes_to_check]\n",
        "            are_equal = all(\n",
        "                getattr(self.config, attr) == getattr(assistant_model.config, attr) for attr in attributes_to_check\n",
        "            )\n",
        "\n",
        "    def _prepare_model_inputs(\n",
        "        self,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        bos_token_id: Optional[torch.Tensor] = None,\n",
        "        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n",
        "    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n",
        "        if (\n",
        "            self.config.is_encoder_decoder\n",
        "            and hasattr(self, \"encoder\")\n",
        "            and self.encoder.main_input_name != self.main_input_name\n",
        "        ):\n",
        "            input_name = self.encoder.main_input_name\n",
        "        else:\n",
        "            input_name = self.main_input_name\n",
        "\n",
        "        model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name}\n",
        "\n",
        "        inputs_kwarg = model_kwargs.pop(input_name, None)\n",
        "        if inputs_kwarg is not None:\n",
        "            inputs = inputs_kwarg\n",
        "\n",
        "        if input_name == \"input_ids\" and \"inputs_embeds\" in model_kwargs:\n",
        "            if model_kwargs[\"inputs_embeds\"] is None:\n",
        "                model_kwargs.pop(\"inputs_embeds\")\n",
        "            elif not self.config.is_encoder_decoder:\n",
        "                has_inputs_embeds_forwarding = \"inputs_embeds\" in set(\n",
        "                    inspect.signature(self.prepare_inputs_for_generation).parameters.keys()\n",
        "                )\n",
        "\n",
        "                model_kwargs[\"input_ids\"] = self._maybe_initialize_input_ids_for_generation(\n",
        "                    inputs, bos_token_id, model_kwargs=model_kwargs\n",
        "                )\n",
        "                inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\"\n",
        "            else:\n",
        "                inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\"\n",
        "\n",
        "        inputs = self._maybe_initialize_input_ids_for_generation(inputs, bos_token_id, model_kwargs)\n",
        "        return inputs, input_name, model_kwargs\n",
        "\n",
        "    def _maybe_initialize_input_ids_for_generation(\n",
        "        self,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        bos_token_id: Optional[torch.Tensor] = None,\n",
        "        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n",
        "    ) -> torch.LongTensor:\n",
        "        if inputs is not None:\n",
        "            return inputs\n",
        "\n",
        "        encoder_outputs = model_kwargs.get(\"encoder_outputs\")\n",
        "        if self.config.is_encoder_decoder and encoder_outputs is not None:\n",
        "            shape = encoder_outputs.last_hidden_state.size()[:-1]\n",
        "            return torch.ones(shape, dtype=torch.long, device=self.device) * -100\n",
        "\n",
        "        batch_size = 1\n",
        "        for value in model_kwargs.values():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                batch_size = value.shape[0]\n",
        "                break\n",
        "\n",
        "        if \"inputs_embeds\" in model_kwargs:\n",
        "            return torch.ones((batch_size, 0), dtype=torch.long, device=self.device)\n",
        "\n",
        "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * bos_token_id\n",
        "\n",
        "    def _prepare_special_tokens(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        kwargs_has_attention_mask: Optional[bool] = None,\n",
        "        device: Optional[Union[torch.device, str]] = None,\n",
        "    ):\n",
        "        def _tensor_or_none(token, device=None):\n",
        "            if token is None:\n",
        "                return token\n",
        "\n",
        "            device = device if device is not None else self.device\n",
        "            if isinstance(token, torch.Tensor):\n",
        "                return token.to(device)\n",
        "            return torch.tensor(token, device=device, dtype=torch.long)\n",
        "\n",
        "        bos_token_tensor = _tensor_or_none(generation_config.bos_token_id, device=device)\n",
        "        eos_token_tensor = _tensor_or_none(generation_config.eos_token_id, device=device)\n",
        "        pad_token_tensor = _tensor_or_none(generation_config.pad_token_id, device=device)\n",
        "        decoder_start_token_tensor = _tensor_or_none(generation_config.decoder_start_token_id, device=device)\n",
        "\n",
        "        if self.config.is_encoder_decoder:\n",
        "            decoder_start_token_tensor = (\n",
        "                decoder_start_token_tensor if decoder_start_token_tensor is not None else bos_token_tensor\n",
        "            )\n",
        "\n",
        "        if eos_token_tensor is not None and eos_token_tensor.ndim == 0:\n",
        "            eos_token_tensor = eos_token_tensor.unsqueeze(0)\n",
        "\n",
        "        if pad_token_tensor is None and eos_token_tensor is not None:\n",
        "            pad_token_tensor = eos_token_tensor[0]\n",
        "\n",
        "        generation_config._bos_token_tensor = bos_token_tensor\n",
        "        generation_config._eos_token_tensor = eos_token_tensor\n",
        "        generation_config._pad_token_tensor = pad_token_tensor\n",
        "        generation_config._decoder_start_token_tensor = decoder_start_token_tensor\n",
        "\n",
        "    def _prepare_generated_length(\n",
        "        self,\n",
        "        generation_config,\n",
        "        has_default_max_length,\n",
        "        has_default_min_length,\n",
        "        model_input_name,\n",
        "        input_ids_length,\n",
        "        inputs_tensor,\n",
        "    ):\n",
        "\n",
        "        if generation_config.max_new_tokens is not None:\n",
        "            generation_config.max_length = generation_config.max_new_tokens + input_ids_length\n",
        "\n",
        "        elif (\n",
        "            model_input_name == \"inputs_embeds\"\n",
        "            and input_ids_length != inputs_tensor.shape[1]\n",
        "            and not self.config.is_encoder_decoder\n",
        "        ):\n",
        "            generation_config.max_length -= inputs_tensor.shape[1]\n",
        "        elif has_default_max_length:\n",
        "            if generation_config.max_length == GenerationConfig().max_length:\n",
        "                generation_config.max_length = generation_config.max_length + input_ids_length\n",
        "                max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n",
        "                if max_position_embeddings is not None:\n",
        "                    generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n",
        "\n",
        "        if generation_config.min_new_tokens is not None:\n",
        "            generation_config.min_length = generation_config.min_new_tokens + input_ids_length\n",
        "\n",
        "        elif (\n",
        "            model_input_name == \"inputs_embeds\"\n",
        "            and input_ids_length != inputs_tensor.shape[1]\n",
        "            and not self.config.is_encoder_decoder\n",
        "        ):\n",
        "            generation_config.min_length = max(generation_config.min_length - inputs_tensor.shape[1], 0)\n",
        "\n",
        "        return generation_config\n",
        "\n",
        "    def _supports_logits_to_keep(self) -> bool:\n",
        "        return \"logits_to_keep\" in set(inspect.signature(self.forward).parameters.keys())\n",
        "\n",
        "    def _validate_generated_length(self, generation_config, input_ids_length, has_default_max_length):\n",
        "        if input_ids_length >= generation_config.max_length:\n",
        "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
        "\n",
        "        if generation_config.min_new_tokens is not None:\n",
        "            min_length = generation_config.min_new_tokens + input_ids_length\n",
        "\n",
        "    def _prepare_cache_for_generation(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        model_kwargs: dict,\n",
        "        assistant_model: \"PreTrainedModel\",\n",
        "        batch_size: int,\n",
        "        max_cache_length: int,\n",
        "        device: torch.device,\n",
        "    ) -> bool:\n",
        "        is_hybrid_cache = any(class_name in self.__class__.__name__.lower() for class_name in [\"mamba\", \"falconh1\"])\n",
        "        cache_name = \"past_key_values\" if not is_hybrid_cache else \"cache_params\"\n",
        "\n",
        "        requires_cross_attention_cache = (\n",
        "            self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n",
        "        )\n",
        "\n",
        "        user_defined_cache = model_kwargs.get(cache_name)\n",
        "        if user_defined_cache is not None:\n",
        "            if isinstance(user_defined_cache, tuple) and self._supports_default_dynamic_cache():\n",
        "                model_kwargs[cache_name] = (\n",
        "                    DynamicCache.from_legacy_cache(user_defined_cache)\n",
        "                    if not requires_cross_attention_cache\n",
        "                    else EncoderDecoderCache.from_legacy_cache(user_defined_cache)\n",
        "                )\n",
        "            return\n",
        "\n",
        "        if generation_config.use_cache is False:\n",
        "            return\n",
        "\n",
        "        if not self._supports_default_dynamic_cache():\n",
        "            return\n",
        "\n",
        "        if assistant_model is not None and generation_config.cache_implementation is not None:\n",
        "            generation_config.cache_implementation = None\n",
        "\n",
        "        generation_config.cache_implementation = generation_config.cache_implementation or getattr(\n",
        "            self.config.get_text_config(), \"cache_implementation\", None\n",
        "        )\n",
        "        if generation_config.cache_implementation is not None:\n",
        "            if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n",
        "                model_kwargs[cache_name] = self._get_cache(\n",
        "                    cache_implementation=generation_config.cache_implementation,\n",
        "                    batch_size=max(generation_config.num_beams, generation_config.num_return_sequences) * batch_size,\n",
        "                    max_cache_len=max_cache_length,\n",
        "                    device=device,\n",
        "                    model_kwargs=model_kwargs,\n",
        "                )\n",
        "            elif generation_config.cache_implementation == \"quantized\":\n",
        "                cache_config = (\n",
        "                    generation_config.cache_config\n",
        "                    if generation_config.cache_config is not None\n",
        "                    else QuantizedCacheConfig()\n",
        "                )\n",
        "                cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]\n",
        "\n",
        "                model_kwargs[cache_name] = cache_class(cache_config)\n",
        "            elif generation_config.cache_implementation == \"offloaded\":\n",
        "                model_kwargs[cache_name] = OffloadedCache()\n",
        "            elif generation_config.cache_implementation == \"dynamic\":\n",
        "                model_kwargs[cache_name] = DynamicCache()\n",
        "\n",
        "        else:\n",
        "            model_kwargs[cache_name] = (\n",
        "                DynamicCache()\n",
        "                if not requires_cross_attention_cache\n",
        "                else EncoderDecoderCache(DynamicCache(), DynamicCache())\n",
        "            )\n",
        "\n",
        "    def _get_logits_processor(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        input_ids_seq_length: Optional[int] = None,\n",
        "        encoder_input_ids: torch.LongTensor = None,\n",
        "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n",
        "        logits_processor: Optional[LogitsProcessorList] = None,\n",
        "        device: Optional[str] = None,\n",
        "        model_kwargs: Optional[dict[str, Any]] = None,\n",
        "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
        "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> LogitsProcessorList:\n",
        "        processors = LogitsProcessorList()\n",
        "        if logits_processor is None:\n",
        "            logits_processor = []\n",
        "\n",
        "        if generation_config.guidance_scale is not None and generation_config.guidance_scale != 1:\n",
        "            processors.append(\n",
        "                UnbatchedClassifierFreeGuidanceLogitsProcessor(\n",
        "                    generation_config.guidance_scale,\n",
        "                    self,\n",
        "                    unconditional_ids=negative_prompt_ids,\n",
        "                    unconditional_attention_mask=negative_prompt_attention_mask,\n",
        "                    use_cache=generation_config.use_cache,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.sequence_bias is not None:\n",
        "            processors.append(SequenceBiasLogitsProcessor(sequence_bias=generation_config.sequence_bias))\n",
        "\n",
        "        if generation_config.diversity_penalty is not None and generation_config.diversity_penalty > 0.0:\n",
        "            processors.append(\n",
        "                HammingDiversityLogitsProcessor(\n",
        "                    diversity_penalty=generation_config.diversity_penalty,\n",
        "                    num_beams=generation_config.num_beams,\n",
        "                    num_beam_groups=generation_config.num_beam_groups,\n",
        "                )\n",
        "            )\n",
        "        if (\n",
        "            generation_config.encoder_repetition_penalty is not None\n",
        "            and generation_config.encoder_repetition_penalty != 1.0\n",
        "        ):\n",
        "            if len(encoder_input_ids.shape) == 2:\n",
        "                processors.append(\n",
        "                    EncoderRepetitionPenaltyLogitsProcessor(\n",
        "                        penalty=generation_config.encoder_repetition_penalty,\n",
        "                        encoder_input_ids=encoder_input_ids,\n",
        "                    )\n",
        "                )\n",
        "        if generation_config.repetition_penalty is not None and generation_config.repetition_penalty != 1.0:\n",
        "            processors.append(RepetitionPenaltyLogitsProcessor(penalty=generation_config.repetition_penalty))\n",
        "        if generation_config.no_repeat_ngram_size is not None and generation_config.no_repeat_ngram_size > 0:\n",
        "            processors.append(NoRepeatNGramLogitsProcessor(generation_config.no_repeat_ngram_size))\n",
        "        if (\n",
        "            generation_config.encoder_no_repeat_ngram_size is not None\n",
        "            and generation_config.encoder_no_repeat_ngram_size > 0\n",
        "        ):\n",
        "            if len(encoder_input_ids.shape) == 2:\n",
        "                processors.append(\n",
        "                    EncoderNoRepeatNGramLogitsProcessor(\n",
        "                        generation_config.encoder_no_repeat_ngram_size,\n",
        "                        encoder_input_ids,\n",
        "                    )\n",
        "                )\n",
        "        if generation_config.bad_words_ids is not None:\n",
        "            processors.append(\n",
        "                NoBadWordsLogitsProcessor(\n",
        "                    generation_config.bad_words_ids,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                )\n",
        "            )\n",
        "        if (\n",
        "            generation_config.min_length is not None\n",
        "            and getattr(generation_config, \"_eos_token_tensor\", None) is not None\n",
        "            and generation_config.min_length > 0\n",
        "        ):\n",
        "            processors.append(\n",
        "                MinLengthLogitsProcessor(\n",
        "                    generation_config.min_length,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if (\n",
        "            generation_config.min_new_tokens is not None\n",
        "            and getattr(generation_config, \"_eos_token_tensor\", None) is not None\n",
        "            and generation_config.min_new_tokens > 0\n",
        "        ):\n",
        "            processors.append(\n",
        "                MinNewTokensLengthLogitsProcessor(\n",
        "                    input_ids_seq_length,\n",
        "                    generation_config.min_new_tokens,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if prefix_allowed_tokens_fn is not None:\n",
        "            processors.append(\n",
        "                PrefixConstrainedLogitsProcessor(\n",
        "                    prefix_allowed_tokens_fn,\n",
        "                    generation_config.num_beams // generation_config.num_beam_groups,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.forced_bos_token_id is not None:\n",
        "            processors.append(\n",
        "                ForcedBOSTokenLogitsProcessor(\n",
        "                    generation_config.forced_bos_token_id,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.forced_eos_token_id is not None:\n",
        "            processors.append(\n",
        "                ForcedEOSTokenLogitsProcessor(\n",
        "                    generation_config.max_length,\n",
        "                    generation_config.forced_eos_token_id,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.remove_invalid_values is True:\n",
        "            processors.append(InfNanRemoveLogitsProcessor())\n",
        "        if generation_config.exponential_decay_length_penalty is not None:\n",
        "            processors.append(\n",
        "                ExponentialDecayLengthPenalty(\n",
        "                    generation_config.exponential_decay_length_penalty,\n",
        "                    generation_config._eos_token_tensor,\n",
        "                    input_ids_seq_length,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.suppress_tokens is not None:\n",
        "            processors.append(\n",
        "                SuppressTokensLogitsProcessor(\n",
        "                    generation_config.suppress_tokens,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.begin_suppress_tokens is not None:\n",
        "            begin_index = input_ids_seq_length\n",
        "            begin_index = (\n",
        "                begin_index\n",
        "                if (input_ids_seq_length > 1 or generation_config.forced_bos_token_id is None)\n",
        "                else begin_index + 1\n",
        "            )\n",
        "            processors.append(\n",
        "                SuppressTokensAtBeginLogitsProcessor(\n",
        "                    generation_config.begin_suppress_tokens,\n",
        "                    begin_index,\n",
        "                    device=device,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        processors = self._merge_criteria_processor_list(processors, logits_processor)\n",
        "\n",
        "        if generation_config.do_sample:\n",
        "            if generation_config.num_beams > 1:\n",
        "                if isinstance(generation_config._eos_token_tensor, list):\n",
        "                    min_tokens_to_keep = len(generation_config._eos_token_tensor) + 1\n",
        "                elif isinstance(generation_config._eos_token_tensor, torch.Tensor):\n",
        "                    min_tokens_to_keep = generation_config._eos_token_tensor.shape[0] + 1\n",
        "                else:\n",
        "                    min_tokens_to_keep = 2\n",
        "            else:\n",
        "                min_tokens_to_keep = 1\n",
        "\n",
        "            if generation_config.temperature is not None and generation_config.temperature != 1.0:\n",
        "                processors.append(TemperatureLogitsWarper(generation_config.temperature))\n",
        "            if generation_config.top_k is not None and generation_config.top_k != 0:\n",
        "                processors.append(\n",
        "                    TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.top_p is not None and generation_config.top_p < 1.0:\n",
        "                processors.append(\n",
        "                    TopPLogitsWarper(top_p=generation_config.top_p, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.min_p is not None:\n",
        "                processors.append(\n",
        "                    MinPLogitsWarper(min_p=generation_config.min_p, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.typical_p is not None and generation_config.typical_p < 1.0:\n",
        "                processors.append(\n",
        "                    TypicalLogitsWarper(mass=generation_config.typical_p, min_tokens_to_keep=min_tokens_to_keep)\n",
        "                )\n",
        "            if generation_config.epsilon_cutoff is not None and 0.0 < generation_config.epsilon_cutoff < 1.0:\n",
        "                processors.append(\n",
        "                    EpsilonLogitsWarper(\n",
        "                        epsilon=generation_config.epsilon_cutoff, min_tokens_to_keep=min_tokens_to_keep\n",
        "                    )\n",
        "                )\n",
        "            if generation_config.eta_cutoff is not None and 0.0 < generation_config.eta_cutoff < 1.0:\n",
        "                processors.append(\n",
        "                    EtaLogitsWarper(\n",
        "                        epsilon=generation_config.eta_cutoff, min_tokens_to_keep=min_tokens_to_keep, device=device\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        if generation_config.watermarking_config is not None:\n",
        "            processors.append(\n",
        "                generation_config.watermarking_config.construct_processor(\n",
        "                    self.config.get_text_config().vocab_size, device\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if generation_config.renormalize_logits is True:\n",
        "            processors.append(LogitNormalization())\n",
        "        return processors\n",
        "\n",
        "    def _merge_criteria_processor_list(\n",
        "        self,\n",
        "        default_list: Union[LogitsProcessorList, StoppingCriteriaList],\n",
        "        custom_list: Union[LogitsProcessorList, StoppingCriteriaList],\n",
        "    ) -> Union[LogitsProcessorList, StoppingCriteriaList]:\n",
        "        if len(custom_list) == 0:\n",
        "            return default_list\n",
        "\n",
        "        final_list = type(default_list)()\n",
        "        for default in default_list:\n",
        "            using_custom = False\n",
        "            for custom in custom_list:\n",
        "                if type(custom) is type(default):\n",
        "                    object_type = \"stopping criteria\" if isinstance(custom, ABC) else \"logits processor\"\n",
        "                    final_list.append(custom)\n",
        "                    using_custom = True\n",
        "                    break\n",
        "            if not using_custom:\n",
        "                final_list.append(default)\n",
        "\n",
        "        for custom in custom_list:\n",
        "            if custom not in final_list:\n",
        "                final_list.append(custom)\n",
        "        return final_list\n",
        "\n",
        "    def _get_stopping_criteria(\n",
        "        self,\n",
        "        generation_config: GenerationConfig,\n",
        "        stopping_criteria: Optional[StoppingCriteriaList],\n",
        "        tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n",
        "        **kwargs,\n",
        "    ) -> StoppingCriteriaList:\n",
        "        criteria = StoppingCriteriaList()\n",
        "        if generation_config.max_length is not None:\n",
        "            max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n",
        "            criteria.append(\n",
        "                MaxLengthCriteria(\n",
        "                    max_length=generation_config.max_length,\n",
        "                    max_position_embeddings=max_position_embeddings,\n",
        "                )\n",
        "            )\n",
        "        if generation_config.max_time is not None:\n",
        "            criteria.append(MaxTimeCriteria(max_time=generation_config.max_time))\n",
        "        if generation_config.stop_strings is not None:\n",
        "            criteria.append(StopStringCriteria(stop_strings=generation_config.stop_strings, tokenizer=tokenizer))\n",
        "        if generation_config._eos_token_tensor is not None:\n",
        "            criteria.append(EosTokenCriteria(eos_token_id=generation_config._eos_token_tensor))\n",
        "        if (\n",
        "            generation_config.is_assistant\n",
        "            and generation_config.assistant_confidence_threshold is not None\n",
        "            and generation_config.assistant_confidence_threshold > 0\n",
        "        ):\n",
        "            criteria.append(\n",
        "                ConfidenceCriteria(assistant_confidence_threshold=generation_config.assistant_confidence_threshold)\n",
        "            )\n",
        "        criteria = self._merge_criteria_processor_list(criteria, stopping_criteria)\n",
        "        return criteria\n",
        "\n",
        "    @staticmethod\n",
        "    def _expand_inputs_for_generation(\n",
        "        expand_size: int = 1,\n",
        "        is_encoder_decoder: bool = False,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        **model_kwargs,\n",
        "    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n",
        "        if expand_size == 1:\n",
        "            return input_ids, model_kwargs\n",
        "\n",
        "        def _expand_dict_for_generation(dict_to_expand):\n",
        "            for key in dict_to_expand:\n",
        "                if (\n",
        "                    key != \"cache_position\"\n",
        "                    and dict_to_expand[key] is not None\n",
        "                    and isinstance(dict_to_expand[key], torch.Tensor)\n",
        "                ):\n",
        "                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n",
        "            return dict_to_expand\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
        "\n",
        "        model_kwargs = _expand_dict_for_generation(model_kwargs)\n",
        "\n",
        "        if is_encoder_decoder:\n",
        "            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n",
        "\n",
        "        return input_ids, model_kwargs\n",
        "\n",
        "    def _sample(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        logits_processor: LogitsProcessorList,\n",
        "        stopping_criteria: StoppingCriteriaList,\n",
        "        generation_config: GenerationConfig,\n",
        "        synced_gpus: bool,\n",
        "        streamer: Optional[\"BaseStreamer\"],\n",
        "        **model_kwargs,\n",
        "    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n",
        "        pad_token_id = generation_config._pad_token_tensor\n",
        "        output_attentions = generation_config.output_attentions\n",
        "        output_hidden_states = generation_config.output_hidden_states\n",
        "        output_scores = generation_config.output_scores\n",
        "        output_logits = generation_config.output_logits\n",
        "        return_dict_in_generate = generation_config.return_dict_in_generate\n",
        "        has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)\n",
        "        do_sample = generation_config.do_sample\n",
        "\n",
        "        scores = () if (return_dict_in_generate and output_scores) else None\n",
        "        raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
        "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
        "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
        "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
        "\n",
        "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
        "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
        "            encoder_hidden_states = (\n",
        "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
        "            )\n",
        "\n",
        "        batch_size, cur_len = input_ids.shape[:2]\n",
        "        this_peer_finished = False\n",
        "        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
        "        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n",
        "\n",
        "        model_forward = self.__call__\n",
        "        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n",
        "        if compile_forward:\n",
        "            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
        "            if self.config._attn_implementation == \"flash_attention_2\" and getattr(\n",
        "                model_kwargs.get(\"past_key_values\"), \"is_compileable\", False\n",
        "            ):\n",
        "                if generation_config.compile_config is None:\n",
        "                    generation_config.compile_config = CompileConfig(fullgraph=False)\n",
        "                elif generation_config.compile_config.fullgraph:\n",
        "                    generation_config.compile_config.fullgraph = False\n",
        "            model_forward = self.get_compiled_call(generation_config.compile_config)\n",
        "\n",
        "        if generation_config.prefill_chunk_size is not None:\n",
        "            model_kwargs = self._prefill_chunking(input_ids, generation_config, **model_kwargs)\n",
        "            is_prefill = False\n",
        "        else:\n",
        "            is_prefill = True\n",
        "\n",
        "        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "\n",
        "            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n",
        "            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
        "\n",
        "            if is_prefill:\n",
        "                outputs = self(**model_inputs, return_dict=True)\n",
        "                is_prefill = False\n",
        "            else:\n",
        "                outputs = model_forward(**model_inputs, return_dict=True)\n",
        "\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(\n",
        "                outputs,\n",
        "                model_kwargs,\n",
        "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "            )\n",
        "            if synced_gpus and this_peer_finished:\n",
        "                continue\n",
        "\n",
        "            next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)\n",
        "\n",
        "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
        "\n",
        "            if return_dict_in_generate:\n",
        "                if output_scores:\n",
        "                    scores += (next_token_scores,)\n",
        "                if output_logits:\n",
        "                    raw_logits += (next_token_logits,)\n",
        "                if output_attentions:\n",
        "                    decoder_attentions += (\n",
        "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
        "                    )\n",
        "                    if self.config.is_encoder_decoder:\n",
        "                        cross_attentions += (outputs.cross_attentions,)\n",
        "                if output_hidden_states:\n",
        "                    decoder_hidden_states += (\n",
        "                        (outputs.decoder_hidden_states,)\n",
        "                        if self.config.is_encoder_decoder\n",
        "                        else (outputs.hidden_states,)\n",
        "                    )\n",
        "\n",
        "            if do_sample:\n",
        "                probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
        "                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "            else:\n",
        "                next_tokens = torch.argmax(next_token_scores, dim=-1)\n",
        "\n",
        "            if has_eos_stopping_criteria:\n",
        "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "            if streamer is not None:\n",
        "                streamer.put(next_tokens.cpu())\n",
        "\n",
        "            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
        "            this_peer_finished = unfinished_sequences.max() == 0\n",
        "            cur_len += 1\n",
        "\n",
        "            del outputs\n",
        "\n",
        "        if streamer is not None:\n",
        "            streamer.end()\n",
        "\n",
        "        if return_dict_in_generate:\n",
        "            if self.config.is_encoder_decoder:\n",
        "                return GenerateEncoderDecoderOutput(\n",
        "                    sequences=input_ids,\n",
        "                    scores=scores,\n",
        "                    logits=raw_logits,\n",
        "                    encoder_attentions=encoder_attentions,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    decoder_attentions=decoder_attentions,\n",
        "                    cross_attentions=cross_attentions,\n",
        "                    decoder_hidden_states=decoder_hidden_states,\n",
        "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
        "                )\n",
        "            else:\n",
        "                return GenerateDecoderOnlyOutput(\n",
        "                    sequences=input_ids,\n",
        "                    scores=scores,\n",
        "                    logits=raw_logits,\n",
        "                    attentions=decoder_attentions,\n",
        "                    hidden_states=decoder_hidden_states,\n",
        "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
        "                )\n",
        "        else:\n",
        "            return input_ids\n",
        "\n",
        "    def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n",
        "        if \"cache_position\" in model_kwargs and model_kwargs[\"cache_position\"]:\n",
        "            return model_kwargs\n",
        "        if \"inputs_embeds\" in model_kwargs and not self.config.is_encoder_decoder:\n",
        "            cache_position = torch.ones_like(model_kwargs[\"inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n",
        "        elif \"decoder_inputs_embeds\" in model_kwargs and self.config.is_encoder_decoder:\n",
        "            cache_position = (\n",
        "                torch.ones_like(model_kwargs[\"decoder_inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n",
        "            )\n",
        "        else:\n",
        "            cache_position = torch.ones(seq_length, dtype=torch.int64, device=device).cumsum(0) - 1\n",
        "\n",
        "        past_length = 0\n",
        "        if model_kwargs.get(\"past_key_values\") is not None:\n",
        "            cache = model_kwargs[\"past_key_values\"]\n",
        "            past_length = 0\n",
        "            if not isinstance(cache, Cache):\n",
        "                past_length = cache[0][0].shape[2]\n",
        "            elif hasattr(cache, \"get_seq_length\") and cache.get_seq_length() is not None:\n",
        "                past_length = cache.get_seq_length()\n",
        "\n",
        "            cache_position = cache_position[past_length:]\n",
        "\n",
        "        model_kwargs[\"cache_position\"] = cache_position\n",
        "        return model_kwargs\n",
        "\n",
        "    def _valid_auto_compile_criteria(self, model_kwargs: dict, generation_config: GenerationConfig) -> bool:\n",
        "        if generation_config.disable_compile:\n",
        "            return False\n",
        "\n",
        "        valid_hardware = self.device.type == \"cuda\" or bool(\n",
        "            generation_config.compile_config is not None and generation_config.compile_config._compile_all_devices\n",
        "        )\n",
        "        using_compilable_cache = (\n",
        "            isinstance(model_kwargs.get(\"past_key_values\"), Cache) and model_kwargs[\"past_key_values\"].is_compileable\n",
        "        )\n",
        "        can_compile = valid_hardware and using_compilable_cache and self._supports_static_cache\n",
        "\n",
        "        if getattr(self, \"hf_quantizer\", None) is not None:\n",
        "            can_compile &= self.hf_quantizer.is_compileable\n",
        "\n",
        "        if hasattr(self, \"hf_device_map\"):\n",
        "            all_model_devices = set(self.hf_device_map.values())\n",
        "            has_cpu_offload = \"cpu\" in all_model_devices and len(all_model_devices) > 1\n",
        "            can_compile &= not has_cpu_offload\n",
        "\n",
        "            has_disk_offload = \"disk\" in all_model_devices\n",
        "            can_compile &= not has_disk_offload\n",
        "\n",
        "        return can_compile\n",
        "\n",
        "    def _has_unfinished_sequences(self, this_peer_finished: bool, synced_gpus: bool, device: torch.device) -> bool:\n",
        "        if synced_gpus:\n",
        "            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0, device=device)\n",
        "            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
        "            if this_peer_finished_flag.item() == 0.0:\n",
        "                return False\n",
        "        elif this_peer_finished:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _update_model_kwargs_for_generation(\n",
        "        self,\n",
        "        outputs: ModelOutput,\n",
        "        model_kwargs: dict[str, Any],\n",
        "        is_encoder_decoder: bool = False,\n",
        "        num_new_tokens: int = 1,\n",
        "    ) -> dict[str, Any]:\n",
        "        for possible_cache_name in ALL_CACHE_NAMES:\n",
        "            if possible_cache_name in outputs:\n",
        "                if possible_cache_name in (\"past_buckets_states\", \"mems\"):\n",
        "                    cache_name = \"past_key_values\"\n",
        "                else:\n",
        "                    cache_name = possible_cache_name\n",
        "                model_kwargs[cache_name] = getattr(outputs, possible_cache_name)\n",
        "                break\n",
        "\n",
        "        if \"token_type_ids\" in model_kwargs:\n",
        "            token_type_ids = model_kwargs[\"token_type_ids\"]\n",
        "            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        if not is_encoder_decoder:\n",
        "            if \"attention_mask\" in model_kwargs:\n",
        "                attention_mask = model_kwargs[\"attention_mask\"]\n",
        "                model_kwargs[\"attention_mask\"] = torch.cat(\n",
        "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
        "                )\n",
        "        else:\n",
        "            if \"decoder_attention_mask\" in model_kwargs:\n",
        "                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n",
        "                model_kwargs[\"decoder_attention_mask\"] = torch.cat(\n",
        "                    [decoder_attention_mask, decoder_attention_mask.new_ones((decoder_attention_mask.shape[0], 1))],\n",
        "                    dim=-1,\n",
        "                )\n",
        "\n",
        "        if model_kwargs.get(\"use_cache\", True):\n",
        "            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n",
        "        else:\n",
        "            past_positions = model_kwargs.pop(\"cache_position\")\n",
        "            new_positions = torch.arange(\n",
        "                past_positions[-1] + 1, past_positions[-1] + num_new_tokens + 1, dtype=past_positions.dtype\n",
        "            ).to(past_positions.device)\n",
        "            model_kwargs[\"cache_position\"] = torch.cat((past_positions, new_positions))\n",
        "        return model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "pqrV4C7BkgxJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 770,
      "metadata": {
        "id": "xzvgfrXiAjc8"
      },
      "outputs": [],
      "source": [
        "tokens_per_layer = [[] for _ in range(28)]\n",
        "\n",
        "class DirectionalMaskedLinear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None, threshold=0.3):\n",
        "        super().__init__(in_features, out_features, bias=bias, device=device, dtype=dtype)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        output = input @ self.weight.T\n",
        "        if self.bias is not None:\n",
        "            output += self.bias\n",
        "        return output\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
        "    cos = cos.unsqueeze(unsqueeze_dim)\n",
        "    sin = sin.unsqueeze(unsqueeze_dim)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "class Qwen2Config(PretrainedConfig):\n",
        "    model_type = \"qwen2\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "\n",
        "    base_model_tp_plan = {\n",
        "        \"layers.*.self_attn.q_proj\": \"colwise\",\n",
        "        \"layers.*.self_attn.k_proj\": \"colwise\",\n",
        "        \"layers.*.self_attn.v_proj\": \"colwise\",\n",
        "        \"layers.*.self_attn.o_proj\": \"rowwise\",\n",
        "        \"layers.*.mlp.gate_proj\": \"colwise\",\n",
        "        \"layers.*.mlp.up_proj\": \"colwise\",\n",
        "        \"layers.*.mlp.down_proj\": \"rowwise\",\n",
        "    }\n",
        "    base_model_pp_plan = {\n",
        "        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n",
        "        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n",
        "        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n",
        "    }\n",
        "\n",
        "def sdpa_attention_forward(\n",
        "    module: torch.nn.Module,\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    dropout: float = 0.0,\n",
        "    scaling: Optional[float] = None,\n",
        "    is_causal: Optional[bool] = None,\n",
        "    **kwargs,\n",
        ") -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "\n",
        "    if hasattr(module, \"num_key_value_groups\"):\n",
        "        key = repeat_kv(key, module.num_key_value_groups)\n",
        "        value = repeat_kv(value, module.num_key_value_groups)\n",
        "\n",
        "    if attention_mask is not None and attention_mask.ndim == 4:\n",
        "        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
        "\n",
        "    query = query.contiguous()\n",
        "    key = key.contiguous()\n",
        "    value = value.contiguous()\n",
        "\n",
        "    if is_causal is None:\n",
        "        is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n",
        "\n",
        "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
        "        is_causal = is_causal.item()\n",
        "\n",
        "    attn_output = scaled_dot_product_attention(\n",
        "        query,\n",
        "        key,\n",
        "        value,\n",
        "        attn_mask=attention_mask,\n",
        "        dropout_p=dropout,\n",
        "        scale=scaling,\n",
        "        is_causal=is_causal,\n",
        "    )\n",
        "\n",
        "    attn_weights = None\n",
        "\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "    return attn_output, attn_weights\n",
        "\n",
        "class CustomQwen2Attention(nn.Module):\n",
        "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
        "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "        self.attention_dropout = config.attention_dropout\n",
        "        self.is_causal = True\n",
        "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n",
        "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n",
        "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n",
        "        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n",
        "        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
        "        attention_mask: Optional[torch.Tensor],\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n",
        "        input_shape = hidden_states.shape[:-1]\n",
        "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
        "\n",
        "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
        "\n",
        "        cos, sin = position_embeddings\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
        "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "        attn_output, attn_weights = sdpa_attention_forward(\n",
        "            self,\n",
        "            query_states,\n",
        "            key_states,\n",
        "            value_states,\n",
        "            attention_mask,\n",
        "            dropout=0.0 if not self.training else self.attention_dropout,\n",
        "            scaling=self.scaling,\n",
        "            sliding_window=self.sliding_window,\n",
        "            is_causal=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "class CustomQwen2RMSNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "        return self.weight * hidden_states.to(input_dtype)\n",
        "\n",
        "class CustomQwen2MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
        "        self.act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, x):\n",
        "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "        return down_proj\n",
        "\n",
        "class Qwen2DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        self.self_attn = CustomQwen2Attention(config=config, layer_idx=layer_idx)\n",
        "\n",
        "        self.mlp = CustomQwen2MLP(config)\n",
        "        self.input_layernorm = CustomQwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.post_attention_layernorm = CustomQwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.attention_type = config.layer_types[layer_idx]\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n",
        "        **kwargs,\n",
        "    ) -> tuple[torch.Tensor]:\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.input_layernorm(hidden_states)\n",
        "        hidden_states, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_value=past_key_value,\n",
        "            use_cache=use_cache,\n",
        "            cache_position=cache_position,\n",
        "            position_embeddings=position_embeddings,\n",
        "            **kwargs,\n",
        "        )\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        hidden_states = residual + hidden_states\n",
        "        return hidden_states\n",
        "\n",
        "class Qwen2PreTrainedModel(PreTrainedModel):\n",
        "    config_class = Qwen2Config\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [\"Qwen2DecoderLayer\"]\n",
        "    _skip_keys_device_placement = [\"past_key_values\"]\n",
        "    _supports_flash_attn_2 = True\n",
        "    _supports_flash_attn_3 = True\n",
        "    _supports_sdpa = True\n",
        "    _supports_flex_attn = True\n",
        "    _supports_cache_class = True\n",
        "    _supports_quantized_cache = True\n",
        "    _supports_static_cache = True\n",
        "    _supports_attention_backend = True\n",
        "    _can_record_outputs = {\n",
        "        \"hidden_states\": Qwen2DecoderLayer,\n",
        "        \"attentions\": CustomQwen2Attention,\n",
        "    }\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.initializer_range\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, CustomQwen2RMSNorm):\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "class Qwen2RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, config: Qwen2Config, device=None):\n",
        "        super().__init__()\n",
        "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
        "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
        "        else:\n",
        "            self.rope_type = \"default\"\n",
        "        self.max_seq_len_cached = config.max_position_embeddings\n",
        "        self.original_max_seq_len = config.max_position_embeddings\n",
        "\n",
        "        self.config = config\n",
        "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
        "\n",
        "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        self.original_inv_freq = self.inv_freq\n",
        "\n",
        "    def forward(self, x, position_ids):\n",
        "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
        "        position_ids_expanded = position_ids[:, None, :].float()\n",
        "\n",
        "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
        "        with torch.autocast(device_type=device_type, enabled=False):\n",
        "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1)\n",
        "            cos = emb.cos() * self.attention_scaling\n",
        "            sin = emb.sin() * self.attention_scaling\n",
        "\n",
        "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
        "\n",
        "\n",
        "class CustomQwen2Model(Qwen2PreTrainedModel):\n",
        "    def __init__(self, config: Qwen2Config):\n",
        "        super().__init__(config)\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "        )\n",
        "        self.norm = CustomQwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.rotary_emb = Qwen2RotaryEmbedding(config=config)\n",
        "        self.gradient_checkpointing = False\n",
        "        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> BaseModelOutputWithPast:\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        if use_cache and past_key_values is None:\n",
        "            past_key_values = DynamicCache()\n",
        "\n",
        "        if cache_position is None:\n",
        "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
        "            cache_position = torch.arange(\n",
        "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
        "            )\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = cache_position.unsqueeze(0)\n",
        "\n",
        "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
        "\n",
        "            mask_kwargs = {\n",
        "                \"config\": self.config,\n",
        "                \"input_embeds\": inputs_embeds,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"cache_position\": cache_position,\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"position_ids\": position_ids,\n",
        "            }\n",
        "\n",
        "            causal_mask_mapping = {\n",
        "                \"full_attention\": create_causal_mask(**mask_kwargs),\n",
        "            }\n",
        "\n",
        "            if self.has_sliding_layers:\n",
        "                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n",
        "\n",
        "        hidden_states = inputs_embeds\n",
        "\n",
        "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
        "\n",
        "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
        "            hidden_states = decoder_layer(\n",
        "                hidden_states,\n",
        "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
        "                position_ids=position_ids,\n",
        "                past_key_value=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                cache_position=cache_position,\n",
        "                position_embeddings=position_embeddings,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=past_key_values if use_cache else None,\n",
        "        )\n",
        "\n",
        "\n",
        "class CustomQwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
        "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = CustomQwen2Model(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model\n",
        "\n",
        "    def _prepare_generation_config(\n",
        "        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: dict\n",
        "    ) -> tuple[GenerationConfig, dict]:\n",
        "        using_model_generation_config = False\n",
        "        if generation_config is None:\n",
        "            if (\n",
        "                self.generation_config._from_model_config\n",
        "                and self.generation_config._original_object_hash == hash(self.generation_config)\n",
        "                and len(self.config._get_non_default_generation_parameters()) > 0\n",
        "            ):\n",
        "                new_generation_config = GenerationConfig.from_model_config(self.config)\n",
        "                if new_generation_config != self.generation_config:\n",
        "                    self.generation_config = new_generation_config\n",
        "\n",
        "            generation_config = self.generation_config\n",
        "            using_model_generation_config = True\n",
        "\n",
        "        generation_config = copy.deepcopy(generation_config)\n",
        "\n",
        "        if not using_model_generation_config:\n",
        "            model_base_version = version.parse(version.parse(self.generation_config.transformers_version).base_version)\n",
        "            if use_model_defaults is True or (\n",
        "                use_model_defaults is None and model_base_version >= version.parse(\"4.50.0\")\n",
        "            ):\n",
        "                modified_values = {}\n",
        "                global_default_generation_config = GenerationConfig()\n",
        "                model_generation_config = self.generation_config\n",
        "                for key, model_gen_config_value in model_generation_config.__dict__.items():\n",
        "                    if key.startswith(\"_\") or key == \"transformers_version\":\n",
        "                        continue\n",
        "                    global_default_value = getattr(global_default_generation_config, key, None)\n",
        "                    custom_gen_config_value = getattr(generation_config, key, None)\n",
        "                    if (\n",
        "                        custom_gen_config_value == global_default_value\n",
        "                        and model_gen_config_value != global_default_value\n",
        "                    ):\n",
        "                        modified_values[key] = model_gen_config_value\n",
        "                        setattr(generation_config, key, model_gen_config_value)\n",
        "                if generation_config.temperature == 0.0:\n",
        "                    generation_config.do_sample = False\n",
        "            else:\n",
        "                if generation_config.bos_token_id is None:\n",
        "                    generation_config.bos_token_id = self.generation_config.bos_token_id\n",
        "                if generation_config.eos_token_id is None:\n",
        "                    generation_config.eos_token_id = self.generation_config.eos_token_id\n",
        "                if generation_config.pad_token_id is None:\n",
        "                    generation_config.pad_token_id = self.generation_config.pad_token_id\n",
        "                if generation_config.decoder_start_token_id is None:\n",
        "                    generation_config.decoder_start_token_id = self.generation_config.decoder_start_token_id\n",
        "\n",
        "        model_kwargs = generation_config.update(**kwargs)\n",
        "\n",
        "        return generation_config, model_kwargs\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Cache] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
        "        **kwargs,\n",
        "    ) -> CausalLMOutputWithPast:\n",
        "        outputs: BaseModelOutputWithPast = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            cache_position=cache_position,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
        "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        generation_config: Optional[GenerationConfig] = None,\n",
        "        logits_processor: Optional[LogitsProcessorList] = None,\n",
        "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
        "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n",
        "        synced_gpus: Optional[bool] = None,\n",
        "        assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
        "        streamer: Optional[\"BaseStreamer\"] = None,\n",
        "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
        "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
        "        use_model_defaults: Optional[bool] = None,\n",
        "        custom_generate: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
        "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
        "        if custom_generate is not None:\n",
        "            global_keys_to_exclude = {\n",
        "                \"self\",\n",
        "                \"kwargs\",\n",
        "                \"global_keys_to_exclude\",\n",
        "                \"trust_remote_code\",\n",
        "                \"custom_generate\",\n",
        "            }\n",
        "            generate_arguments = {key: value for key, value in locals().items() if key not in global_keys_to_exclude}\n",
        "            generate_arguments.update(kwargs)\n",
        "\n",
        "            custom_generate_function = self.load_custom_generate(\n",
        "                custom_generate, trust_remote_code=trust_remote_code, **kwargs\n",
        "            )\n",
        "            return custom_generate_function(model=self, **generate_arguments)\n",
        "\n",
        "        tokenizer = kwargs.pop(\"tokenizer\", None)\n",
        "        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)\n",
        "\n",
        "        generation_config, model_kwargs = self._prepare_generation_config(\n",
        "            generation_config, use_model_defaults, **kwargs\n",
        "        )\n",
        "        self._validate_model_kwargs(model_kwargs.copy())\n",
        "        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
        "\n",
        "        if synced_gpus is None:\n",
        "            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n",
        "\n",
        "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
        "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
        "\n",
        "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
        "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
        "        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n",
        "\n",
        "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n",
        "            inputs, generation_config.bos_token_id, model_kwargs\n",
        "        )\n",
        "        batch_size = inputs_tensor.shape[0]\n",
        "\n",
        "        device = inputs_tensor.device\n",
        "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
        "\n",
        "        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n",
        "            generation_config.use_cache = True\n",
        "\n",
        "        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:\n",
        "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
        "                inputs_tensor, generation_config, model_kwargs\n",
        "            )\n",
        "\n",
        "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
        "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
        "                inputs_tensor, model_kwargs, model_input_name, generation_config\n",
        "            )\n",
        "\n",
        "        if self.config.is_encoder_decoder:\n",
        "            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n",
        "                batch_size=batch_size,\n",
        "                model_input_name=model_input_name,\n",
        "                model_kwargs=model_kwargs,\n",
        "                decoder_start_token_id=generation_config._decoder_start_token_tensor,\n",
        "                device=inputs_tensor.device,\n",
        "            )\n",
        "        else:\n",
        "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
        "\n",
        "        if generation_config.token_healing:\n",
        "            input_ids = self.heal_tokens(input_ids, tokenizer)\n",
        "\n",
        "        if streamer is not None:\n",
        "            streamer.put(input_ids.cpu())\n",
        "\n",
        "        input_ids_length = input_ids.shape[1]\n",
        "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
        "        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n",
        "        generation_config = self._prepare_generated_length(\n",
        "            generation_config=generation_config,\n",
        "            has_default_max_length=has_default_max_length,\n",
        "            has_default_min_length=has_default_min_length,\n",
        "            model_input_name=model_input_name,\n",
        "            inputs_tensor=inputs_tensor,\n",
        "            input_ids_length=input_ids_length,\n",
        "        )\n",
        "\n",
        "        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n",
        "            model_kwargs[\"logits_to_keep\"] = 1\n",
        "\n",
        "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
        "\n",
        "        max_cache_length = generation_config.max_length - 1\n",
        "        if (\n",
        "            inputs_tensor.shape[1] != input_ids_length\n",
        "            and model_input_name == \"inputs_embeds\"\n",
        "            and not self.config.is_encoder_decoder\n",
        "        ):\n",
        "            max_cache_length += inputs_tensor.shape[1]\n",
        "        self._prepare_cache_for_generation(\n",
        "            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n",
        "        )\n",
        "\n",
        "        generation_mode = \"sample\"\n",
        "\n",
        "        prepared_logits_processor = self._get_logits_processor(\n",
        "            generation_config=generation_config,\n",
        "            input_ids_seq_length=input_ids_length,\n",
        "            encoder_input_ids=inputs_tensor,\n",
        "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "            logits_processor=logits_processor,\n",
        "            device=inputs_tensor.device,\n",
        "            model_kwargs=model_kwargs,\n",
        "            negative_prompt_ids=negative_prompt_ids,\n",
        "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
        "        )\n",
        "        prepared_stopping_criteria = self._get_stopping_criteria(\n",
        "            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n",
        "        )\n",
        "\n",
        "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
        "\n",
        "        input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
        "            input_ids=input_ids,\n",
        "            expand_size=generation_config.num_return_sequences,\n",
        "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "        result = self._sample(\n",
        "            input_ids,\n",
        "            logits_processor=prepared_logits_processor,\n",
        "            stopping_criteria=prepared_stopping_criteria,\n",
        "            generation_config=generation_config,\n",
        "            synced_gpus=synced_gpus,\n",
        "            streamer=streamer,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "        if (\n",
        "            generation_config.return_legacy_cache is True\n",
        "            and hasattr(result, \"past_key_values\")\n",
        "            and getattr(result.past_key_values, \"to_legacy_cache\") is not None\n",
        "        ):\n",
        "            result.past_key_values = result.past_key_values.to_legacy_cache()\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AttentionInterface(GeneralInterface):\n",
        "    _global_mapping = {\n",
        "        \"sdpa\": sdpa_attention_forward,\n",
        "    }\n",
        "ALL_ATTENTION_FUNCTIONS: AttentionInterface = AttentionInterface()\n",
        "\n",
        "@contextmanager\n",
        "def no_init_weights():\n",
        "    global _init_weights\n",
        "    old_init_weights = _init_weights\n",
        "\n",
        "    _init_weights = False\n",
        "\n",
        "    def _skip_init(*args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    for name, init_func in TORCH_INIT_FUNCTIONS.items():\n",
        "        setattr(torch.nn.init, name, _skip_init)\n",
        "\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        _init_weights = old_init_weights\n",
        "        for name, init_func in TORCH_INIT_FUNCTIONS.items():\n",
        "            setattr(torch.nn.init, name, init_func)\n",
        "\n",
        "def _get_torch_dtype(\n",
        "    cls,\n",
        "    torch_dtype: Optional[Union[str, torch.dtype, dict]],\n",
        "    checkpoint_files: Optional[list[str]],\n",
        "    config: PretrainedConfig,\n",
        "    sharded_metadata: Optional[dict],\n",
        "    state_dict: Optional[dict],\n",
        "    weights_only: bool,\n",
        ") -> tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n",
        "    dtype_orig = None\n",
        "    is_sharded = sharded_metadata is not None\n",
        "\n",
        "    if torch_dtype is not None:\n",
        "        if isinstance(torch_dtype, str):\n",
        "            if torch_dtype == \"auto\":\n",
        "                if hasattr(config, \"torch_dtype\") and config.torch_dtype is not None:\n",
        "                    torch_dtype = config.torch_dtype\n",
        "                else:\n",
        "                    if is_sharded and \"dtype\" in sharded_metadata:\n",
        "                        torch_dtype = sharded_metadata[\"dtype\"]\n",
        "                    elif state_dict is not None:\n",
        "                        torch_dtype = get_state_dict_dtype(state_dict)\n",
        "                    else:\n",
        "                        state_dict = load_state_dict(\n",
        "                            checkpoint_files[0], map_location=\"meta\", weights_only=weights_only\n",
        "                        )\n",
        "                        torch_dtype = get_state_dict_dtype(state_dict)\n",
        "            elif hasattr(torch, torch_dtype):\n",
        "                torch_dtype = getattr(torch, torch_dtype)\n",
        "                config.torch_dtype = torch_dtype\n",
        "                for sub_config_key in config.sub_configs.keys():\n",
        "                    sub_config = getattr(config, sub_config_key)\n",
        "                    sub_config.torch_dtype = torch_dtype\n",
        "        elif isinstance(torch_dtype, torch.dtype):\n",
        "            config.torch_dtype = torch_dtype\n",
        "            for sub_config_key in config.sub_configs.keys():\n",
        "                sub_config = getattr(config, sub_config_key)\n",
        "                sub_config.torch_dtype = torch_dtype\n",
        "        elif isinstance(torch_dtype, dict):\n",
        "            for key, curr_dtype in torch_dtype.items():\n",
        "                if hasattr(config, key):\n",
        "                    value = getattr(config, key)\n",
        "                    curr_dtype = curr_dtype if not isinstance(curr_dtype, str) else getattr(torch, curr_dtype)\n",
        "                    value.torch_dtype = curr_dtype\n",
        "            torch_dtype = torch_dtype.get(\"\")\n",
        "            torch_dtype = torch_dtype if not isinstance(torch_dtype, str) else getattr(torch, torch_dtype)\n",
        "            config.torch_dtype = torch_dtype\n",
        "            if torch_dtype is None:\n",
        "                torch_dtype = torch.float32\n",
        "\n",
        "        dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n",
        "    else:\n",
        "        default_dtype = torch.get_default_dtype()\n",
        "        config.torch_dtype = default_dtype\n",
        "        for key in config.sub_configs.keys():\n",
        "            value = getattr(config, key)\n",
        "            value.torch_dtype = default_dtype\n",
        "\n",
        "    return config, torch_dtype, dtype_orig\n",
        "\n",
        "\n",
        "def _get_resolved_checkpoint_files(\n",
        "    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n",
        "    subfolder: str,\n",
        "    variant: Optional[str],\n",
        "    gguf_file: Optional[str],\n",
        "    from_tf: bool,\n",
        "    from_flax: bool,\n",
        "    use_safetensors: bool,\n",
        "    cache_dir: str,\n",
        "    force_download: bool,\n",
        "    proxies: Optional[dict[str, str]],\n",
        "    local_files_only: bool,\n",
        "    token: Optional[Union[str, bool]],\n",
        "    user_agent: dict,\n",
        "    revision: str,\n",
        "    commit_hash: Optional[str],\n",
        "    is_remote_code: bool,\n",
        "    transformers_explicit_filename: Optional[str] = None,\n",
        ") -> tuple[Optional[list[str]], Optional[dict]]:\n",
        "    is_sharded = False\n",
        "\n",
        "    if pretrained_model_name_or_path is not None and gguf_file is None:\n",
        "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
        "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
        "        if is_local:\n",
        "            if transformers_explicit_filename is not None:\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, transformers_explicit_filename)\n",
        "                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n",
        "            elif from_tf and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n",
        "            ):\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n",
        "            elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)):\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)\n",
        "            elif from_flax and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n",
        "            ):\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, FLAX_WEIGHTS_NAME)\n",
        "            elif use_safetensors is not False and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant))\n",
        "            ):\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant)\n",
        "                )\n",
        "            elif use_safetensors is not False and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant))\n",
        "            ):\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant)\n",
        "                )\n",
        "                is_sharded = True\n",
        "            elif not use_safetensors and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant))\n",
        "            ):\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant)\n",
        "                )\n",
        "            elif not use_safetensors and os.path.isfile(\n",
        "                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant))\n",
        "            ):\n",
        "                archive_file = os.path.join(\n",
        "                    pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant)\n",
        "                )\n",
        "                is_sharded = True\n",
        "        elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n",
        "            archive_file = pretrained_model_name_or_path\n",
        "            is_local = True\n",
        "        elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path + \".index\")):\n",
        "            archive_file = os.path.join(subfolder, pretrained_model_name_or_path + \".index\")\n",
        "            is_local = True\n",
        "        elif is_remote_url(pretrained_model_name_or_path):\n",
        "            filename = pretrained_model_name_or_path\n",
        "            resolved_archive_file = download_url(pretrained_model_name_or_path)\n",
        "        else:\n",
        "            if transformers_explicit_filename is not None:\n",
        "                filename = transformers_explicit_filename\n",
        "                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n",
        "            elif from_tf:\n",
        "                filename = TF2_WEIGHTS_NAME\n",
        "            elif from_flax:\n",
        "                filename = FLAX_WEIGHTS_NAME\n",
        "            elif use_safetensors is not False:\n",
        "                filename = _add_variant(SAFE_WEIGHTS_NAME, variant)\n",
        "            else:\n",
        "                filename = _add_variant(WEIGHTS_NAME, variant)\n",
        "\n",
        "            try:\n",
        "                cached_file_kwargs = {\n",
        "                    \"cache_dir\": cache_dir,\n",
        "                    \"force_download\": force_download,\n",
        "                    \"proxies\": proxies,\n",
        "                    \"local_files_only\": local_files_only,\n",
        "                    \"token\": token,\n",
        "                    \"user_agent\": user_agent,\n",
        "                    \"revision\": revision,\n",
        "                    \"subfolder\": subfolder,\n",
        "                    \"_raise_exceptions_for_gated_repo\": False,\n",
        "                    \"_raise_exceptions_for_missing_entries\": False,\n",
        "                    \"_commit_hash\": commit_hash,\n",
        "                }\n",
        "                resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
        "\n",
        "                if resolved_archive_file is None and filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n",
        "                    resolved_archive_file = cached_file(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant),\n",
        "                        **cached_file_kwargs,\n",
        "                    )\n",
        "                    if resolved_archive_file is not None:\n",
        "                        is_sharded = True\n",
        "                    elif use_safetensors:\n",
        "                        if revision == \"main\":\n",
        "                            resolved_archive_file, revision, is_sharded = auto_conversion(\n",
        "                                pretrained_model_name_or_path, **cached_file_kwargs\n",
        "                            )\n",
        "                        cached_file_kwargs[\"revision\"] = revision\n",
        "                    else:\n",
        "                        filename = _add_variant(WEIGHTS_NAME, variant)\n",
        "                        resolved_archive_file = cached_file(\n",
        "                            pretrained_model_name_or_path, filename, **cached_file_kwargs\n",
        "                        )\n",
        "                if resolved_archive_file is None and filename == _add_variant(WEIGHTS_NAME, variant):\n",
        "                    resolved_archive_file = cached_file(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        _add_variant(WEIGHTS_INDEX_NAME, variant),\n",
        "                        **cached_file_kwargs,\n",
        "                    )\n",
        "                    if resolved_archive_file is not None:\n",
        "                        is_sharded = True\n",
        "                if not local_files_only and not is_offline_mode():\n",
        "                    if resolved_archive_file is not None:\n",
        "                        if filename in [WEIGHTS_NAME, WEIGHTS_INDEX_NAME]:\n",
        "                            safe_weights_name = SAFE_WEIGHTS_INDEX_NAME if is_sharded else SAFE_WEIGHTS_NAME\n",
        "                            has_file_kwargs = {\n",
        "                                \"revision\": revision,\n",
        "                                \"proxies\": proxies,\n",
        "                                \"token\": token,\n",
        "                                \"cache_dir\": cache_dir,\n",
        "                                \"local_files_only\": local_files_only,\n",
        "                            }\n",
        "                            cached_file_kwargs = {\n",
        "                                \"cache_dir\": cache_dir,\n",
        "                                \"force_download\": force_download,\n",
        "                                \"local_files_only\": local_files_only,\n",
        "                                \"user_agent\": user_agent,\n",
        "                                \"subfolder\": subfolder,\n",
        "                                \"_raise_exceptions_for_gated_repo\": False,\n",
        "                                \"_raise_exceptions_for_missing_entries\": False,\n",
        "                                \"_commit_hash\": commit_hash,\n",
        "                                **has_file_kwargs,\n",
        "                            }\n",
        "                            if (\n",
        "                                not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs)\n",
        "                                and not is_remote_code\n",
        "                            ):\n",
        "                                Thread(\n",
        "                                    target=auto_conversion,\n",
        "                                    args=(pretrained_model_name_or_path,),\n",
        "                                    kwargs={\"ignore_errors_during_conversion\": True, **cached_file_kwargs},\n",
        "                                    name=\"Thread-auto_conversion\",\n",
        "                                ).start()\n",
        "                    else:\n",
        "                        has_file_kwargs = {\n",
        "                            \"revision\": revision,\n",
        "                            \"proxies\": proxies,\n",
        "                            \"token\": token,\n",
        "                            \"cache_dir\": cache_dir,\n",
        "                            \"local_files_only\": local_files_only,\n",
        "                        }\n",
        "\n",
        "            except Exception as e:\n",
        "                raise\n",
        "\n",
        "        if is_local:\n",
        "            resolved_archive_file = archive_file\n",
        "    elif gguf_file:\n",
        "        if os.path.isfile(gguf_file):\n",
        "            resolved_archive_file = gguf_file\n",
        "        else:\n",
        "            cached_file_kwargs = {\n",
        "                \"cache_dir\": cache_dir,\n",
        "                \"force_download\": force_download,\n",
        "                \"proxies\": proxies,\n",
        "                \"local_files_only\": local_files_only,\n",
        "                \"token\": token,\n",
        "                \"user_agent\": user_agent,\n",
        "                \"revision\": revision,\n",
        "                \"subfolder\": subfolder,\n",
        "                \"_raise_exceptions_for_gated_repo\": False,\n",
        "                \"_raise_exceptions_for_missing_entries\": False,\n",
        "                \"_commit_hash\": commit_hash,\n",
        "            }\n",
        "\n",
        "            resolved_archive_file = cached_file(pretrained_model_name_or_path, gguf_file, **cached_file_kwargs)\n",
        "\n",
        "    sharded_metadata = None\n",
        "    if is_sharded:\n",
        "        checkpoint_files, sharded_metadata = get_checkpoint_shard_files(\n",
        "            pretrained_model_name_or_path,\n",
        "            resolved_archive_file,\n",
        "            cache_dir=cache_dir,\n",
        "            force_download=force_download,\n",
        "            proxies=proxies,\n",
        "            local_files_only=local_files_only,\n",
        "            token=token,\n",
        "            user_agent=user_agent,\n",
        "            revision=revision,\n",
        "            subfolder=subfolder,\n",
        "            _commit_hash=commit_hash,\n",
        "        )\n",
        "    else:\n",
        "        checkpoint_files = [resolved_archive_file] if pretrained_model_name_or_path is not None else None\n",
        "\n",
        "    return checkpoint_files, sharded_metadata"
      ],
      "metadata": {
        "id": "i1OFblecjuny"
      },
      "execution_count": 771,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# メイン"
      ],
      "metadata": {
        "id": "5Ax-_brflSzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 772,
      "metadata": {
        "id": "PYIZ9EuIW1Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afabfdd5-caf2-4f50-b21e-3a19d9de0b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected torch version: 2.6.0+cu124\n",
            "今からでも遅\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 独自モデルとして読み込む\n",
        "config = Qwen2Config.from_pretrained(\"/content/drive/MyDrive/reconstructed_model\")\n",
        "config.use_flash_attn = True\n",
        "model = CustomQwen2ForCausalLM.from_pretrained(\n",
        "    \"/content/drive/MyDrive/reconstructed_model\",\n",
        "    config=config,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ").eval()\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "except Exception as e:\n",
        "    print(\"compile失敗:\", e)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/reconstructed_model\")\n",
        "\n",
        "inputs = tokenizer(\"今から\", return_tensors=\"pt\")\n",
        "inputs = {\n",
        "    k: (v.to(model.device).half() if k != \"input_ids\" else v.to(model.device))\n",
        "    for k, v in inputs.items()\n",
        "}\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=3,\n",
        "        #do_sample=False,          # サンプリングせずgreedy → 高速\n",
        "        use_cache=False,           # キャッシュ活用 → 重要\n",
        "        #num_beams=1,              # Beam search無効 → 高速\n",
        "        #early_stopping=True,      # 終了判定を早める\n",
        "        #pad_token_id=tokenizer.eos_token_id  # 明示的にパディングトークン指定\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jchku-EAuPvmq1MgTLSy37yI2Iwe2K9p",
      "authorship_tag": "ABX9TyOXROwChVrcxaG81nTzMyTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}